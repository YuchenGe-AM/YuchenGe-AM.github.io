{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Sep 16] Basis of Machine Learning II\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716c726",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef270289",
   "metadata": {},
   "source": [
    "Content\n",
    "\n",
    "1. [Kernel Method](#Kernel-Method)\n",
    "2. [SVM Method](#SVM-Method)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446c9c8",
   "metadata": {},
   "source": [
    "### 1. Kernel Method <a id='Kernel-Method'></a>\n",
    "\n",
    "In practice, **linear separation** is often not possible. One way to define such a non-linear decision boundary is to use a non-linear mapping $\\Phi$ from the input space $\\mathcal{X}$ to a higher-dimensional space $\\mathbb{H}$ (which is called a **feature space**), where linear separation is possible.\n",
    "\n",
    "> A **kernel over $\\mathcal{X}$** is a function $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "The idea is to define a kernel such that $K(x, x^{\\prime})=\\langle\\Phi(x), \\Phi(x^{\\prime})\\rangle$. However, **this is not true for general kernels**. Thereby, we define\n",
    "\n",
    "> A **positive definite symmetric (PDS) kernel** is a kernel $K$ such that for any $\\{x_1,...,x_m\n",
    "\\} \\subset \\mathcal{X}$, the kernel (Gram) matrix matrix $[K(x_i,x_j)]_{ij} \\in \\mathbb{R}_{m \\times m}$ is symmetric positive semidefinite (SPSD).\n",
    "\n",
    "For example, we have\n",
    "\n",
    "> **Polynomial kernels:** $\\quad K\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\left(\\mathbf{x} \\cdot \\mathbf{x}^{\\prime}+c\\right)^{d}$.\n",
    ">\n",
    "> **Gaussian kernels:** $\\quad K\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}^{\\prime}-\\mathbf{x}\\right\\|^{2}}{2 \\sigma^{2}}\\right)$.\n",
    ">\n",
    "> **Sigmoid kernels:** $K\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\tanh \\left(a\\left(\\mathbf{x} \\cdot \\mathbf{x}^{\\prime}\\right)+b\\right)$.\n",
    "\n",
    "\n",
    "The following is **the main result** of this note.\n",
    "\n",
    "Seeing that \n",
    "\n",
    "> $K\\left(x, x^{\\prime}\\right)^{2} \\leq K(x, x) K\\left(x^{\\prime}, x^{\\prime}\\right)$ \n",
    "\n",
    "since the matrix \\begin{pmatrix}\n",
    "K(x, x) & K\\left(x, x^{\\prime}\\right) \\\\\n",
    "K\\left(x^{\\prime}, x\\right) & K\\left(x^{\\prime}, x^{\\prime}\\right)\n",
    "\\end{pmatrix}\n",
    "is SPDS, then we immediately arrive at \n",
    "\n",
    "> If $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is a PDS kernel, then there exist $\\Phi$ s.t. $K\\left(x, x^{\\prime}\\right)=\\left\\langle\\Phi(x), \\Phi\\left(x^{\\prime}\\right)\\right\\rangle$ \n",
    "> and \n",
    "\n",
    "**Proof.** Define $\\Phi(x)\\left(x^{\\prime}\\right)=K\\left(x, x^{\\prime}\\right)$ and \n",
    "$ \\mathbb{H}_{0}=\\left\\{\\sum_{i \\in I} a_{i} \\Phi\\left(x_{i}\\right): a_{i} \\in \\mathbb{R}, x_{i} \\in X,|I|<\\infty\\right\\}$. Next, we introduce an operation on $\\mathbb{H}_{0}$, which is \n",
    "\n",
    "$$\\langle f, g\\rangle=\\sum_{i \\in I, j \\in J} a_{i} b_{j} K\\left(x_{i}, x_{j}^{\\prime}\\right) = \\sum_{j \\in J} b_{j} f (x_{j}') = \\sum_{i \\in I} a_{i} g\\left(x_{i}\\right)$$\n",
    "with $f=\\sum_{i \\in I} a_{i} \\Phi\\left(x_{i}\\right)$ and $g=\\sum_{j \\in J} b_{j} \\Phi\\left(x_{j}^{\\prime}\\right)$.  The last two equations show that the operation is well-defined. Then, it's routine to show that $\\langle f, g\\rangle$ defines a **PDS kernel** on $\\mathbb{H}_{0}$. The only **untrivial** thing is to show $\\langle f, f\\rangle=0$ iff $f=0$. Seeing \n",
    "\n",
    "$$ f(x)^2=\\bigg(\\sum_{i \\in I} a_{i} K\\left(x_{i}, x\\right)\\bigg)^2 = \\langle f, \\Phi(x)\\rangle^{2} \\leq\\langle f, f\\rangle\\langle\\Phi(x), \\Phi(x)\\rangle. $$\n",
    "\n",
    "The last inequality is true since \n",
    "\n",
    "$$ \\bigg( \\sum a_i \\langle x_i, x\\rangle\\bigg) ^{2} = \\sum a_i^2 \\langle x_i, x\\rangle^{2} +  \\sum_{ij} a_i a_j \\langle x_i, x\\rangle \\langle x_j, x\\rangle \\leq  \\sum a_i^2 \\langle x_i, x_i\\rangle \\langle x, x\\rangle +  \\sum_{ij} a_i a_j \\langle x_i, x_i\\rangle^{1/2} \\langle x_j, x_j\\rangle^{1/2} \\langle x,x\\rangle\n",
    "$$\n",
    "\n",
    "where the last inequality is from the lemma.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e72e84",
   "metadata": {},
   "source": [
    "### 2. SVM Method <a id='SVM-Method'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03a43a",
   "metadata": {},
   "source": [
    "We consider a binary classifier problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a054394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c17ed94a",
   "metadata": {},
   "source": [
    "$$\\begin{array}{ccccccccc}   \n",
    "0 & \\xrightarrow{i} & A & \\xrightarrow{f} & B & \\xrightarrow{q} & C & \\xrightarrow{d} & 0 \\\\\n",
    "\n",
    "\\downarrow & \\searrow & \\downarrow & \\nearrow & \\downarrow & \\searrow & \\downarrow & \\nearrow & \\downarrow \\\\\n",
    "\n",
    "0 & \\xrightarrow{j} & D & \\xrightarrow{g} & E & \\xrightarrow{r} & F & \\xrightarrow{e} & 0  \n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60960491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03f5331e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Gallager, Robert G. Discrete Stochastic Processes. Kluwer Acad. Publ., 1999. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.0",
   "language": "sage",
   "name": "sagemath-10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
