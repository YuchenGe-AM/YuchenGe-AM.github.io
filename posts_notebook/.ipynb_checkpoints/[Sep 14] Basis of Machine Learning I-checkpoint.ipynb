{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0980918",
   "metadata": {},
   "source": [
    "## [Sep 14] Basis of Machine Learning I\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b1255",
   "metadata": {},
   "source": [
    "### Content \n",
    "\n",
    "1. [Basic Definitions](#Basic-Definitions)\n",
    "2. [More General Definitions](#More-General-Definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1255a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a069aa",
   "metadata": {},
   "source": [
    "### 1. Basic Definitions <a id='Basic-Definitions'></a>\n",
    "\n",
    "Some definitions is needed to understand the whole framework. $\\mathcal{X}$ is the set of all possible examples (instances)， and $\\mathcal{Y}$ is the set of all possible labels (target values). For simplicity, $\\mathcal{Y}=\\{ 0,1 \\}$.\n",
    "\n",
    "> **Def 1.**  A **concept** is a mapping $c: \\mathcal{X} \\rightarrow \\mathcal{Y}$.\n",
    "\n",
    "A concept class is a set of concepts we may wish to learn, which is denoted by $\\mathcal{C}$. All concepts that we consider form a hypothesis set, which is denoted by $\\mathcal{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642983e",
   "metadata": {},
   "source": [
    "When we learn some $c \\in \\mathcal{C}$, we receive a sample $S=\\left(x_{1}, \\ldots, x_{m}\\right)$ drawn i.i.d. according to $\\mathcal{I}$.\n",
    "\n",
    "The ultimate goal is to minimize \n",
    "\n",
    "> **Def 2.** (Generalization error) The generalization error (risk) of  $h\\in\\mathcal{H}$  is defined by\n",
    ">\n",
    ">$$R(h)=\\underset{x \\sim \\mathcal{D}}{\\mathbb{P}}[h(x) \\neq c(x)]=\\underset{x \\sim \\mathcal{D}}{\\mathbb{E}}\\left[1_{h(x) \\neq c(x)}\\right].$$\n",
    "\n",
    "Also, we may have the emprical error \n",
    "\n",
    "$$\\widehat{R}_{S}(h)=\\frac{1}{m} \\sum_{i=1}^{m} 1_{h\\left(x_{i}\\right) \\neq c\\left(x_{i}\\right)}.$$\n",
    "\n",
    "We will see in the following a number of guarantees relating these two quantities with high probability, under some general assumptions.\n",
    "\n",
    "> **Remark:** this reminds me of the definition of a sourcr code $C$ in information theory, which is a mapping from $\\mathcal{X}$, the range of a random variable $X$, to $D^{∗}$, the set of finite-length strings of symbols from a $D$-ary alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f48685",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906cbe2b",
   "metadata": {},
   "source": [
    "The following introduces the Probably Approximately Correct (PAC) learning framework, where the concept $h_S$ is selected based on the algorithm $\\mathcal{A}$ and the sample $S$.\n",
    "\n",
    "> **Def 3.** (PAC-learning)  $\\mathcal{C}$  is PAC-learnable if there exists an algorithm  $\\mathcal{A}$  and a polynomial function poly  $(\\cdot, \\cdot, \\cdot, \\cdot)$  such that for any  $\\epsilon>0$  and  $\\delta>0$, the following holds for any distribution $\\mathcal{D}$, the goal $c\\in \\mathcal{C}$, and the sample size  $m \\geq \\operatorname{poly}(1 / \\epsilon, 1 / \\delta, n , size  (c))$:\n",
    ">\n",
    ">$$\\underset{S \\sim \\mathcal{D}^{m}}{\\mathbb{P}}\\left[R\\left(h_{S}\\right) \\leq \\epsilon\\right] \\geq 1-\\delta.$$\n",
    "\n",
    "When $\\# \\mathcal{H}<\\infty$, we know that \n",
    "\n",
    "> **Thm 1.** ( $\\# \\mathcal{H}<\\infty$, consistent )  When the algorithm  $\\mathcal{A}$ is s.t. for any goal $c\\in\\mathcal{H}$, $\\widehat{R}_{S}\\left(h_{S}\\right)=0$. Then  for any $\\epsilon, \\delta>0$, the inequality\n",
    ">\n",
    "> $$\\underset{S \\sim \\mathcal{D}^{m}}{\\mathbb{P}}\\left[R\\left(h_{S}\\right) \\leq \\epsilon\\right] \\geq 1-\\delta$$  holds if\n",
    "> $$ m \\geq \\frac{1}{\\epsilon}\\left(\\log \\#\\mathcal{H}+\\log \\frac{1}{\\delta}\\right).$$\n",
    "\n",
    "**Proof.** Define $\\mathcal{H}_{\\epsilon}=\\{h \\in \\mathcal{H}: R(h)>\\epsilon\\}$. Then $\n",
    "\\mathbb{P}[\\widehat{R}_{S}(h)=0] \\leq(1-\\epsilon)^{m}$ for $h\\in\\mathcal{H}_{\\epsilon}$, since $R(h)=\\underset{x \\sim \\mathcal{D}}{\\mathbb{P}}[h(x) \\neq c(x)]>\\epsilon$. Thus, by the union bound, the following holds:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}\\left[\\exists h \\in \\mathcal{H}_{\\epsilon}: \\widehat{R}_{S}(h)=0\\right] & =\\mathbb{P}\\left[\\widehat{R}_{S}\\left(h_{1}\\right)=0 \\vee \\cdots \\vee \\widehat{R}_{S}\\left(h_{\\#\\mathcal{H}_{\\epsilon}}\\right)=0\\right]\\\\\n",
    "& \\leq \\sum_{h \\in \\mathcal{H}_{\\epsilon}} \\mathbb{P}\\left[\\widehat{R}_{S}(h)=0\\right] \\\\\n",
    "& \\leq \\sum_{h \\in \\mathcal{H}_{\\epsilon}}(1-\\epsilon)^{m} \\leq|\\mathcal{H}|(1-\\epsilon)^{m} \\leq|\\mathcal{H}| e^{-m \\epsilon} .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4be1ac",
   "metadata": {},
   "source": [
    "Equivalently, in the language of generalization bound, with probability at least $1-\\delta$, $R(h_{S}) \\leq \\frac{1}{m}(\\log \\#\\mathcal{H}+\\log \\frac{1}{\\delta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef5914",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Lemma 1.** (Hoeffding's inequality) Let  $X_{1}, \\ldots, X_{m}$  be independent random variables with  $X_{i}\\in \\left[a_{i}, b_{i}\\right]$  for all  $i \\in[m]$. Then, for any  $\\epsilon>0$,\n",
    ">\n",
    ">$$\\begin{aligned}\n",
    "& \\mathbb{P}\\left[S_{m}-\\mathbb{E}\\left[S_{m}\\right] \\geq \\epsilon\\right] \\leq e^{-2 \\epsilon^{2} / \\sum_{i=1}^{m}\\left(b_{i}-a_{i}\\right)^{2}}, \\\\\n",
    "& \\mathbb{P}\\left[S_{m}-\\mathbb{E}\\left[S_{m}\\right] \\leq-\\epsilon\\right] \\leq e^{-2 \\epsilon^{2} / \\sum_{i=1}^{m}\\left(b_{i}-a_{i}\\right)^{2}}. \\end{aligned} $$\n",
    "\n",
    "Through **lemma 1**, we can derive\n",
    "\n",
    "> **Thm 1.** ( $\\# \\mathcal{H}<\\infty$, inconsistent )  For any $h\\in \\mathcal{H}$, with probability at least $1-\\delta$, \n",
    ">\n",
    ">$$ R(h) \\leq \\widehat{R}_{S}(h)+\\sqrt{\\frac{\\log |\\mathcal{H}|+\\log \\frac{2}{\\delta}}{2 m}}.$$\n",
    "\n",
    "Before proof, this can be viewed as an instance of the so-called **Occam’s Razor principle**: All other things being equal, a simpler (smaller) hypothesis set is better.\n",
    "\n",
    "**Proof.** Seeing that \n",
    "$$\\begin{aligned}\n",
    "& \\mathbb{P}\\left[\\exists h \\in \\mathcal{H}\\left|\\widehat{R}_{S}(h)-R(h)\\right|>\\epsilon\\right] \\\\\n",
    "= \\text{ } & \\mathbb{P}\\left[\\left(\\left|\\widehat{R}_{S}\\left(h_{1}\\right) R\\left(h_{1}\\right)\\right|>\\epsilon\\right) \\vee \\ldots \\vee\\left(\\left|\\widehat{R}_{S}\\left(h_{|\\mathcal{H}|}\\right)-R\\left(h_{|\\mathcal{H}|}\\right)\\right|>\\epsilon\\right)\\right] \\\\\n",
    "\\leq \\text{ } & \\sum_{h \\in \\mathcal{H}} \\mathbb{P}\\left[\\left|\\widehat{R}_{S}(h)-R(h)\\right|>\\epsilon\\right] \\\\\n",
    "\\leq \\text{ } & 2|\\mathcal{H}| \\exp \\left(-2 m \\epsilon^{2}\\right) .\n",
    "\\end{aligned}$$\n",
    "\n",
    "where the last inequality applies **lemma 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680bcc3",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d134d",
   "metadata": {},
   "source": [
    "### 2. More General Definitions <a id='More-General-Definitions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72655628",
   "metadata": {},
   "source": [
    "In the most general scenario of supervised learning, we consider a distribution $\\mathcal{D}$ over $\\mathcal{X} \\times \\mathcal{Y}$, i.e. the stochastic scenario. Then, we define \n",
    "\n",
    "> $$R(h)=\\underset{(x, y) \\sim \\mathcal{D}}{\\mathbb{P}}[h(x) \\neq y]=\\underset{(x, y) \\sim \\mathcal{D}}{\\mathbb{E}}\\left[1_{h(x) \\neq y}\\right]$$\n",
    "\n",
    "and the Bayesian error\n",
    "\n",
    "> $$R^{\\star}=\\inf _{h \\text { measurable }} R(h).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88125350",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "1. Mohri, M., Rostamizadeh, A., &amp; Talwalkar, A. (2018). Foundations of Machine Learning. The MIT Press. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
