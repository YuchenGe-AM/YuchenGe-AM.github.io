{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Dec 17] Markov Decision Process IV\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a5e95-999a-4029-bb66-5fe6047f0c9c",
   "metadata": {},
   "source": [
    "### 1. MDP Formulation\n",
    "\n",
    "A ( **Infinite-Horizon** ) MDP $M$ includes \n",
    "\n",
    "> **transition** $P: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ with $P(\\cdot \\mid s, a)$, **reward** $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow[0,1]$, and an **initial state distribution** $\\mu \\in \\Delta(\\mathcal{S})$.\n",
    "\n",
    "Consider $\\tau_{t}=\\left(s_{0}, a_{0}, r_{0}, s_{1}, \\ldots, s_{t}, a_{t}, r_{t}\\right)$ as a trajectory, then \n",
    "\n",
    "> a policy is $\\pi: \\mathcal{H} \\rightarrow \\Delta(\\mathcal{A})$ where $\\mathcal{H}$ is the set of all possible trajectories (of all lengths). Specially, we have **stationary policy** $\\pi: \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$, i.e.  $a_{t} \\sim \\pi\\left(\\cdot \\mid s_{t}\\right)$, and **deterministic policy** $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$.\n",
    ">\n",
    "> Set $V^{\\pi}(s)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r\\left(s_{t}, a_{t}\\right) \\mid \\pi, s_{0}=s\\right]$ and $Q^{\\pi}(s, a)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r\\left(s_{t}, a_{t}\\right) \\mid \\pi, s_{0}=s, a_{0}=a\\right]$ where $\\gamma$ is a discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47970f27-e5f3-4891-8c6e-a470d2ade0dc",
   "metadata": {},
   "source": [
    "The goal of MDP is to solve $\\max _{\\pi} V^{\\pi}(s)$. A stationary policy $\\pi$ induces $P_{(s, a),\\left(s^{\\prime}, a^{\\prime}\\right)}^{\\pi}:=P\\left(s^{\\prime} \\mid s, a\\right) \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right)$.\n",
    "\n",
    "> For a stationary $\\pi$,\n",
    "> \n",
    "> $$ \\begin{aligned} V^{\\pi}(s) & = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} Q^{\\pi}(s, a) . \\\\ Q^{\\pi}(s, a) & =r(s, a)+\\gamma \\mathbb{E}_{s^{\\prime} \\sim P(\\cdot \\mid s, a)}\\left[V^{\\pi}\\left(s^{\\prime}\\right)\\right] \\end{aligned} \\implies Q^{\\pi} = r+\\gamma P V^{\\pi} = r+\\gamma P^{\\pi} Q^{\\pi}.$$\n",
    "> \n",
    "> Here we view  $V^{\\pi}$  as vector of length  $|\\mathcal{S}|$, $Q^{\\pi}$  and  $r$  as vectors of length  $|\\mathcal{S}| \\cdot|\\mathcal{A}|$, $P$  as a matrix of size  $(|\\mathcal{S}| \\cdot|\\mathcal{A}|) \\times|\\mathcal{S}|$, $P^\\pi$ as a matrix as size $(|\\mathcal{S}| \\cdot|\\mathcal{A}|) \\times(|\\mathcal{S}| \\cdot|\\mathcal{A}|)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28345b3-59ea-48cd-bfd2-209e4e1a0572",
   "metadata": {},
   "source": [
    "If we can show $I-\\gamma P^{\\pi}$ is invertible, then $Q^{\\pi}=\\left(I-\\gamma P^{\\pi}\\right)^{-1} r$. This is clear since for $\\gamma<1$, $ 0 \\neq x  \\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left\\|\\left(I-\\gamma P^{\\pi}\\right) x\\right\\|_{\\infty} & =\\left\\|x-\\gamma P^{\\pi} x\\right\\|_{\\infty} \\\\\n",
    "& \\geq\\|x\\|_{\\infty}-\\gamma\\left\\|P^{\\pi} x\\right\\|_{\\infty} \\\\\n",
    "&  \\geq\\|x\\|_{\\infty}-\\gamma\\|x\\|_{\\infty} & \\text { (each element of } P^{\\pi} x \\text { is an average of } x \\text { ) } \\\\\n",
    "& =(1-\\gamma)\\|x\\|_{\\infty}>0 & (\\gamma<1, x \\neq 0)\n",
    "\\end{aligned}$$\n",
    "\n",
    "which implies  $I-\\gamma P^{\\pi}$  is full rank.\n",
    "\n",
    "> Define  $\\Pi = \\{ \\text{ non-stationary and randomized policies } \\}$, $V^{\\star}(s) :=\\sup _{\\pi \\in \\Pi} V^{\\pi}(s)$ and $Q^{\\star}(s, a) :=\\sup _{\\pi \\in \\Pi} Q^{\\pi}(s, a)$. Then there exists a stationary and deterministic policy  $\\pi$  s.t. $V^{\\pi}(s) = V^{\\star}(s)$ and $Q^{\\pi}(s, a) =Q^{\\star}(s, a)$.\n",
    "\n",
    "**Proof.** Simple computation shows that \n",
    "\n",
    "$$\\begin{aligned}\n",
    "V^{\\star}\\left(s_{0}\\right) & = \\sup _{\\pi \\in \\Pi} \\mathbb{E}\\left[r\\left(s_{0}, a_{0}\\right)+\\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t} r\\left(s_{t}, a_{t}\\right) \\mid \\pi,\\left(S_{0}, A_{0}, R_{0}, S_{1}\\right)=\\left(s_{0}, a_{0}, r_{0}, s_{1}\\right)\\right]\\right] \\\\\n",
    "& \\leq \\sup _{\\pi \\in \\Pi} \\mathbb{E}\\left[r\\left(s_{0}, a_{0}\\right)+\\sup _{\\pi^{\\prime} \\in \\Pi} \\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t} r\\left(s_{t}, a_{t}\\right) \\mid \\pi^{\\prime},\\left(S_{0}, A_{0}, R_{0}, S_{1}\\right)=\\left(s_{0}, a_{0}, r_{0}, s_{1}\\right)\\right]\\right] \\\\\n",
    "& = \\sup _{\\pi \\in \\Pi} \\mathbb{E}\\left[r\\left(s_{0}, a_{0}\\right)+\\gamma V^{\\star}\\left(s_{1}\\right)\\right] \\\\\n",
    "& =\\sup _{a_{0} \\in \\mathcal{A}} \\mathbb{E}\\left[r\\left(s_{0}, a_{0}\\right)+\\gamma V^{\\star}\\left(s_{1}\\right)\\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "where the first equality uses the law of iterated expectations. Define $$\\pi^{\\star}\n",
    "(s)=\\operatorname{argmax}_{a \\in \\mathcal{A}}\\mathbb{E}\\left[r(s, a)+\\gamma V^{\\star}\\left(s_{1}\\right) \\mid\\left(S_{0}, A_{0}\\right)=(s, a)\\right]$$ \n",
    "\n",
    "i.e. $\\pi^{\\star}=\\pi_{Q^{\\star}}$ where $\\pi_{Q}(s):=\\operatorname{argmax}_{a \\in \\mathcal{A}} Q(s, a)$. And we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659f53e",
   "metadata": {},
   "source": [
    "$$V^{\\star}\\left(s_{0}\\right) \\leq \\mathbb{E}\\left[r\\left(s_{0}, a_{0}\\right)+\\gamma V^{\\star}\\left(s_{1}\\right) \\mid \\pi^{\\star}\\right] \\leq \\mathbb{E}\\left[r\\left(s_{0}, a_{0}\\right)+\\gamma r\\left(s_{1}, a_{1}\\right)+\\gamma^{2} V^{\\star}\\left(s_{2}\\right) \\mid \\pi^{\\star}\\right] \\leq \\ldots \\leq V^{\\pi^{\\star}}\\left(s_{0}\\right)\n",
    "$$\n",
    "which finishes the proof. We refer to such  a $\\pi^{\\star}$  as an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2256b9e",
   "metadata": {},
   "source": [
    "This shows that we may restrict ourselves to using stationary and deterministic policies without any loss in performance. \n",
    "\n",
    "> Having defined $V_{Q}(s):=\\max _{a \\in \\mathcal{A}} Q(s, a)$, the Bellman optimality operator $\\mathcal{T}_{M}: \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|} \\rightarrow \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$ is defined as:\n",
    ">\n",
    "> $$\\mathcal{T} Q:=r+\\gamma P V_{Q}.$$ \n",
    "\n",
    "Then we have the famous\n",
    "\n",
    "> ( **Bellman optimality equations** ) $Q=Q^{\\star}$ iff $Q=\\mathcal{T}Q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caab3a9-95d9-42de-bc31-e39307d33683",
   "metadata": {},
   "source": [
    "**Proof.** Sufficiency is easily shown via observing that $V^{\\star}(s)=\\max _{a} Q^{\\star}(s, a)$, which follows easily that \n",
    "\n",
    "$$\\begin{aligned}\n",
    "Q^{\\star}(s, a) & =\\max _{\\pi} Q^{\\pi}(s, a)=r(s, a)+\\gamma \\max _{\\pi} \\mathbb{E}_{s^{\\prime} \\sim P(\\cdot \\mid s, a)}\\left[V^{\\pi}\\left(s^{\\prime}\\right)\\right] \\\\\n",
    "& =r(s, a)+\\gamma \\mathbb{E}_{s^{\\prime} \\sim P(\\cdot \\mid s, a)}\\left[V^{\\star}\\left(s^{\\prime}\\right)\\right] \\\\\n",
    "& =r(s, a)+\\gamma \\mathbb{E}_{s^{\\prime} \\sim P(\\cdot \\mid s, a)}\\left[\\max _{a^{\\prime}} Q^{\\star}\\left(s^{\\prime}, a^{\\prime}\\right)\\right].\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e27b1",
   "metadata": {},
   "source": [
    "For necessity, suppose $Q=\\mathcal{T} Q$. We now show that $Q=Q^{\\star}$. Let $\\pi=\\pi_{Q}$. Then we have \n",
    "\n",
    "$$  Q = \\mathcal{T} Q = r+\\gamma P^{\\pi} Q \\implies Q=\\left(I-\\gamma P^{\\pi}\\right)^{-1} r=Q^{\\pi},$$\n",
    "\n",
    "which follows that $Q$ is the action value of the policy $\\pi_Q$. Therefore, \n",
    "\n",
    "$$\\begin{aligned}\n",
    "Q-Q^{\\pi^{\\prime}} & =Q^{\\pi}-Q^{\\pi^{\\prime}} =Q^{\\pi}-\\left(I-\\gamma P^{\\pi^{\\prime}}\\right)^{-1} r \\\\\n",
    "& =\\left(I-\\gamma P^{\\pi^{\\prime}}\\right)^{-1}\\left(\\left(I-\\gamma P^{\\pi^{\\prime}}\\right)-\\left(I-\\gamma P^{\\pi}\\right)\\right) Q^{\\pi} \\\\\n",
    "& =\\gamma\\left(I-\\gamma P^{\\pi^{\\prime}}\\right)^{-1}\\left(P^{\\pi}-P^{\\pi^{\\prime}}\\right) Q^{\\pi} \\geq 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "where we have used \n",
    "\n",
    "> $\\left[\\left(I-\\gamma P^{\\pi}\\right)^{-1}\\right]_{(s, a),\\left(s^{\\prime}, a^{\\prime}\\right)}= \\sum_{t=0}^{\\infty} \\gamma^{t} \\mathbb{P}^{\\pi}\\left(s_{t}=s^{\\prime}, a_{t}=a^{\\prime} \\mid s_{0}=s, a_{0}=a\\right)$;\n",
    ">\n",
    "> $\\left[\\left(P^{\\pi}-P^{\\pi^{\\prime}}\\right) Q^{\\pi}\\right]_{s, a}=\\mathbb{E}_{s^{\\prime} \\sim P(\\cdot \\mid s, a)}\\left[Q^{\\pi}\\left(s^{\\prime}, \\pi\\left(s^{\\prime}\\right)\\right)-Q^{\\pi}\\left(s^{\\prime}, \\pi^{\\prime}\\left(s^{\\prime}\\right)\\right)\\right] \\geq 0$.\n",
    "\n",
    "Thus,  $Q^{\\pi}=Q \\geq Q^{\\pi^{\\prime}}$  for all deterministic and stationary  $\\pi^{\\prime}$, which shows that  $\\pi$  is an optimal policy. This completes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ee49d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d568390",
   "metadata": {},
   "source": [
    "### 2. Finite-horizon MDP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6cf57-0510-4318-b594-f775a8f6be49",
   "metadata": {},
   "source": [
    "A **finite-horizon ( and time-dependent )** MDP $M$ differs from the tradtional one only by $P$ and $r$ with $P_{h}$ and $r_h$, $h \\in\\{0, \\ldots H-1\\}$. \n",
    "\n",
    ">  $V_{h}^{\\pi}(s)=\\mathbb{E}\\left[\\sum_{t=h}^{H-1} r_{h}\\left(s_{t}, a_{t}\\right) \\mid \\pi, s_{h}=s\\right]$;\n",
    ">\n",
    "> $Q_{h}^{\\pi}(s, a)=\\mathbb{E}\\left[\\sum_{t=h}^{H-1} r_{h}\\left(s_{t}, a_{t}\\right) \\mid \\pi, s_{h}=s, a_{h}=a\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b24a1-6719-4dda-ad5c-e5ffbca4991d",
   "metadata": {},
   "source": [
    "And correspondingly we have ( proof is via defining $s_t, a_t$ as trivial ones, $t>T$ )\n",
    "\n",
    "> ( **Bellman optimality equations** ) Define $Q_{h}^{\\star}(s, a)=\\sup _{\\pi \\in \\Pi} Q_{h}^{\\pi}(s, a)$ where the sup is over all non-stationary and randomized policies. Suppose that  $Q_{H}=0$. We have that  $Q_{h}=Q_{h}^{\\star}$  for all  $h \\in[H]$  iff for all  $h \\in[H]$,\n",
    ">\n",
    "> $$Q_{h}(s, a)=r_{h}(s, a)+\\mathbb{E}_{s^{\\prime} \\sim P_{h}(\\cdot \\mid s, a)}\\left[\\max _{a^{\\prime} \\in \\mathcal{A}} Q_{h+1}\\left(s^{\\prime}, a^{\\prime}\\right)\\right].$$\n",
    "\n",
    "\n",
    "Furthermore,  $\\pi(s, h)=\\operatorname{argmax}_{a \\in \\mathcal{A}} Q_{h}^{\\star}(s, a)$  is an optimal policy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ab663-2a9e-48d2-995f-d4c81b015435",
   "metadata": {},
   "source": [
    "### 3. Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628becb3",
   "metadata": {},
   "source": [
    "Define\n",
    "\n",
    "> $L(P, r, \\gamma)$  denote the total bit-size required to specify  $M$ where $(P, r, \\gamma)$  in $M $ is specified with rational entries. \n",
    ">\n",
    "> Also we assume that basic arithmetic operations, $+, -, \\times, \\div$  take unit time. \n",
    "\n",
    "Then we study the value iteration method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b6f46-bb64-4c9b-b705-783a2d61c5ac",
   "metadata": {},
   "source": [
    "> **Lemma I.** $ \\left\\|\\mathcal{T} Q-\\mathcal{T} Q^{\\prime}\\right\\|_{\\infty} \\leq \\gamma\\left\\|Q-Q^{\\prime}\\right\\|_{\\infty}. $\n",
    ">\n",
    "> **Lemma II.** For any $Q \\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$, $V^{\\pi_{Q}} \\geq V^{\\star}-\\frac{2\\left\\|Q-Q^{\\star}\\right\\|_{\\infty}}{1-\\gamma} \\mathbb{1}.$\n",
    "\n",
    "**Proof.** For the first, simple computation shows that \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left\\|\\mathcal{T} Q-\\mathcal{T} Q^{\\prime}\\right\\|_{\\infty} & =\\gamma\\left\\|P V_{Q}-P V_{Q^{\\prime}}\\right\\|_{\\infty} =\\gamma\\left\\|P\\left(V_{Q}-V_{Q^{\\prime}}\\right)\\right\\|_{\\infty} \\leq \\gamma\\left\\|V_{Q}-V_{Q^{\\prime}}\\right\\|_{\\infty} \\\\\n",
    "& = \\gamma \\max _{s}\\left|V_{Q}(s)-V_{Q^{\\prime}}(s)\\right| \\leq \\gamma \\max _{s} \\max _{a}\\left|Q(s, a)-Q^{\\prime}(s, a)\\right| =\\gamma\\left\\|Q-Q^{\\prime}\\right\\|_{\\infty}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "For the second, by letting $a=\\pi_{Q}(s)$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "V^{\\star}(s)-V^{\\pi_{Q}}(s)= & Q^{\\star}\\left(s, \\pi^{\\star}(s)\\right)-Q^{\\pi_{Q}}(s, a) \\\\\n",
    "= & Q^{\\star}\\left(s, \\pi^{\\star}(s)\\right)-Q^{\\star}(s, a)+Q^{\\star}(s, a)-Q^{\\pi_{Q}}(s, a) \\\\\n",
    "= & Q^{\\star}\\left(s, \\pi^{\\star}(s)\\right)-Q^{\\star}(s, a)+\\gamma \\mathbb{E}_{s^{\\prime} \\sim P(\\cdot \\mid s, a)}\\left[V^{\\star}\\left(s^{\\prime}\\right)-V^{\\pi_{Q}}\\left(s^{\\prime}\\right)\\right] \\\\\n",
    "\\leq & Q^{\\star}\\left(s, \\pi^{\\star}(s)\\right)-Q\\left(s, \\pi^{\\star}(s)\\right)+Q(s, a)-Q^{\\star}(s, a) +\\gamma \\mathbb{E}_{s^{\\prime} \\sim P(s, a)}\\left[V^{\\star}\\left(s^{\\prime}\\right)-V^{\\pi_{Q}}\\left(s^{\\prime}\\right)\\right] \\\\\n",
    "\\leq & 2\\left\\|Q-Q^{\\star}\\right\\|_{\\infty}+\\gamma\\left\\|V^{\\star}-V^{\\pi_{Q}}\\right\\|_{\\infty} .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9422f",
   "metadata": {},
   "source": [
    "Therefore, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea231d",
   "metadata": {},
   "source": [
    "> ( **Q-value iteration convergence** ) Set  $Q^{(0)}=0$. Suppose $Q^{(k+1)}=\\mathcal{T} Q^{(k)}$ and $\\pi^{(k)}=\\pi_{Q^{(k)}}$. Then for  sufficiently large $k$,\n",
    ">\n",
    "> $$V^{\\pi^{(k)}} \\geq V^{\\star}-\\epsilon \\mathbb{1}$$\n",
    "\n",
    "**Proof** Since  $\\left\\|Q^{\\star}\\right\\|_{\\infty} \\leq 1 /(1-\\gamma)$, $Q^{(k)}=\\mathcal{T}^{k} Q^{(0)}$  and $Q^{\\star}=\\mathcal{T} Q^{\\star}$, lemma I give\n",
    "\n",
    "$$\\left\\|Q^{(k)}-Q^{\\star}\\right\\|_{\\infty}=\\left\\|\\mathcal{T}^{k} Q^{(0)}-\\mathcal{T}^{k} Q^{\\star}\\right\\|_{\\infty} \\leq \\gamma^{k}\\left\\|Q^{(0)}-Q^{\\star}\\right\\|_{\\infty}=(1-(1-\\gamma))^{k}\\left\\|Q^{\\star}\\right\\|_{\\infty} \\leq \\frac{\\exp (-(1-\\gamma) k)}{1-\\gamma},$$\n",
    "\n",
    "which immediately follows the conclusion with lemma II. Here $k \\geq \\frac{\\log \\frac{2}{(1-\\gamma)^{2} \\epsilon}}{1-\\gamma}$ will satisfy the needs. Iteration complexity for an exact solution. Then w.r.t. computing an exact optimal policy, when the gap between the current objective value and the optimal objective value is smaller than  $2^{-L(P, r, \\gamma)}$, then the greedy policy will be optimal, which follows that \n",
    "\n",
    "> The value iteration method has computational complexity $|\\mathcal{S}|^{2}|\\mathcal{A}| \\frac{L(P, r, \\gamma) \\log \\frac{1}{1-\\gamma}}{1-\\gamma}$. ( universal constants are dropped )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661548f",
   "metadata": {},
   "source": [
    "We shall also consider the following policy iteration algorithm.\n",
    "\n",
    "> 1. **Policy evaluation.** Compute  $Q^{\\pi_{k}}$; ( use $Q^{\\pi}=\\left(I-\\gamma P^{\\pi}\\right)^{-1} r$ )\n",
    "> 2. **Policy improvement.** Update the policy: $\\pi_{k+1}=\\pi_{Q^{\\pi_{k}}}$.\n",
    "\n",
    "To obtain similar conclusions, we observe that\n",
    "\n",
    "> $Q^{\\pi_{k+1}} \\geq \\mathcal{T} Q^{\\pi_{k}} \\geq Q^{\\pi_{k}} \\implies \\left\\|Q^{\\pi_{k+1}}-Q^{\\star}\\right\\|_{\\infty} \\leq \\gamma\\left\\|Q^{\\pi_{k}}-Q^{\\star}\\right\\|_{\\infty}$,\n",
    "\n",
    "which follows that \n",
    "\n",
    "> ( **Policy iteration convergence** ) Let  $\\pi_{0}$  be any initial policy. Then for $k \\geq \\frac{\\log \\frac{1}{(1-\\gamma) \\epsilon}}{1-\\gamma}$, we have the following performance bound:\n",
    ">\n",
    "> $$ Q^{\\pi_{k}} \\geq Q^{\\star}-\\epsilon \\mathbb{1} .$$\n",
    "\n",
    "Hence, w.r.t. computing an exact optimal policy, policy iteration is no worse than value iteration. However, w.r.t. computing an exact optimal policy independent of the bit complexity $L(P, r, \\gamma)$, improvements are possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90276d28",
   "metadata": {},
   "source": [
    "Another way is to consider the following optimization problem with variables  $V \\in \\mathbb{R}^{|\\mathcal{S}|}$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\min & \\sum_{s} \\mu(s) V(s) \\\\\n",
    "\\text { s.t. } & V(s) \\geq r(s, a)+\\gamma \\sum_{s^{\\prime}} P\\left(s^{\\prime} \\mid s, a\\right) V\\left(s^{\\prime}\\right) \\quad \\forall a \\in \\mathcal{A}, s \\in \\mathcal{S}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Provided that  $\\mu$  has full support, then the optimal value function  $V^{\\star}(s)$  is the unique solution to this linear program. Thus, this approach will only depend on the bit length description of the MDP, i.e. $L(P, r, \\gamma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ea21f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### 4. Sample Complexity\n",
    "\n",
    "We are interested understanding the number of samples required to find a near optimal policy, which is the sample complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331acb2",
   "metadata": {},
   "source": [
    "> $\\widehat{M}$  is the empirical MDP that is identical to the original  $M$, except that it uses  $\\widehat{P}$  instead of  P  for the transition model. \n",
    ">\n",
    "> $\\widehat{V}^{\\pi}, \\widehat{Q}^{\\pi}, \\widehat{Q}^{\\star}$, and  $\\widehat{\\pi}^{\\star}$  denote the value function, state-action value function, optimal state-action value, and optimal policy in  $\\widehat{M}$, respectively.\n",
    "\n",
    "A generative model takes as input a state action pair  $(s, a)$  and returns a sample  $s^{\\prime} \\sim P(\\cdot \\mid s, a)$  and the reward  $r(s, a)$. \n",
    "\n",
    "Let us consider the most naive approach to learning when we have access to a generative model: suppose we call our simulator  $N$  times at each state action pair. Let  $\\widehat{P}$  be our empirical model, \n",
    "\n",
    "$$\\widehat{P}\\left(s^{\\prime} \\mid s, a\\right)=\\frac{\\operatorname{count}\\left(s^{\\prime}, s, a\\right)}{N}$$\n",
    "\n",
    "where count  $\\left(s^{\\prime}, s, a\\right)$  is the number of times the state-action pair  $(s, a)$  transitions to state  $s^{\\prime}$. We can view  $\\widehat{P}$  as a matrix of size  $|\\mathcal{S}||\\mathcal{A}| \\times|\\mathcal{S}|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265574c",
   "metadata": {},
   "source": [
    "Then using McDiarmid's Inequality, we obtain the concentration bounds \n",
    "\n",
    "$$\\|P(\\cdot \\mid s, a)-\\widehat{P}(\\cdot \\mid s, a)\\|_{1} \\leq c \\sqrt{\\frac{|\\mathcal{S}| \\log (1 / \\delta)}{m}}$$\n",
    "\n",
    "with probability greater than 1 âˆ’ $\\delta$ where $m$ is the number of samples.\n",
    "\n",
    "It follows that \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left\\|Q^{\\pi}-\\widehat{Q}^{\\pi}\\right\\|_{\\infty} & =\\left\\|\\gamma\\left(I-\\gamma \\widehat{P}^{\\pi}\\right)^{-1}(P-\\widehat{P}) V^{\\pi}\\right\\|_{\\infty} \\leq \\frac{\\gamma}{1-\\gamma}\\left\\|(P-\\widehat{P}) V^{\\pi}\\right\\|_{\\infty} \\\\\n",
    "& \\leq \\frac{\\gamma}{1-\\gamma}\\left(\\max _{s, a}\\|P(\\cdot \\mid s, a)-\\widehat{P}(\\cdot \\mid s, a)\\|_{1}\\right)\\left\\|V^{\\pi}\\right\\|_{\\infty} \\leq \\frac{\\gamma}{(1-\\gamma)^{2}} \\max _{s, a}\\|P(\\cdot \\mid s, a)-\\widehat{P}(\\cdot \\mid s, a)\\|_{1}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b4503",
   "metadata": {},
   "source": [
    "Add that $\\max _{s, a}\\|P(\\cdot \\mid s, a)-\\widehat{P}(\\cdot \\mid s, a)\\|_{1}$ describes model accuracy, $\\left\\|Q^{\\pi}-\\widehat{Q}^{\\pi}\\right\\|_{\\infty}$ describes uniform value accuracy, and $\\|\\widehat{Q}^{\\star}-Q^{\\star}\\|_{\\infty}$ describes the near optimal planning. Then we know that \n",
    "\n",
    "> **Thm.** If $\\# \\{ \\text{samples from generative model} \\} =|\\mathcal{S}||\\mathcal{A}|N$ is large enough ( w.r.t. $S, A, \\delta, \\gamma$ ), then we have model accuracy, uniform value accuracy, and near optimal planning.\n",
    "\n",
    "Here the observation for near optimal planning follows from $$\\left|\\widehat{Q}^{\\star}(s, a)-Q^{\\star}(s, a)\\right|=\\left|\\sup \\widehat{Q}^{\\pi}(s, a)-\\sup Q^{\\pi}(s, a)\\right| \\leq \\sup \\left|\\widehat{Q}^{\\pi}(s, a)-Q^{\\pi}(s, a)\\right|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5331e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Alekh Agarwal. Reinforcement Learning: Theory and Algorithms.\n",
    "2. Warren B. Powell. Reinforcement Learning and Stochastic Optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
