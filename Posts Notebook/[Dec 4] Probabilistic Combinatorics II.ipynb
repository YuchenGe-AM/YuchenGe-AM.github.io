{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Dec 4] Probabilistic Combinatorics II\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716c726",
   "metadata": {},
   "source": [
    "### What you need is that your brain is open. \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6b152-e98d-4554-b80a-12d53afacbaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Derivatives\n",
    "\n",
    "We say that coordinate  $i \\in[n]$  is pivotal for  $f:\\{-1,1\\}^{n} \\rightarrow   \\{-1,1\\}$  on input  $x$  if  $f(x) \\neq f\\left(x^{\\oplus i}\\right)$. Here \n",
    "\n",
    "$$x^{\\oplus i} = \\left(x_{1}, \\ldots, x_{i-1},-x_{i}, x_{i+1}, \\ldots, x_{n}\\right).$$\n",
    "\n",
    "> The **influence** $\\operatorname{Inf}_{i}[f]$ of coordinate  $i$  on  $f:\\{-1,1\\}^{n} \\rightarrow\\{-1,1\\}$ is \n",
    "> \n",
    "> $$\\underset{x \\sim\\{-1,1\\}^{n}}{\\mathbb{P}}\\left[f(x) \\neq f\\left(x^{\\oplus i}\\right)\\right]=\\underset{x \\sim\\{-1,1\\}^{n}}{\\mathbf{E}}\\left[\\mathrm{D}_{i} f(x)^{2}\\right]$$\n",
    ">\n",
    "> where $D_i f$ is the ith (discrete) derivative operator, since \n",
    ">\n",
    "> $$\\mathrm{D}_{i} f(x)=\\left\\{\\begin{array}{ll} 0 & \\text { if coordinate } i \\text { is not pivotal for } x \\\\ \\pm 1 & \\text { if coordinate } i \\text { is pivotal for } x \\end{array}\\right. .$$\n",
    ">\n",
    "> The **total influence** of  $f:\\{-1,1\\}^{n} \\rightarrow \\mathbb{R}$  is $\\mathbf{I}[f]=\\sum_{i=1}^{n} \\operatorname{Inf}_{i}[f]=\\underset{x}{\\mathbf{E}}\\left[\\|\\nabla f(x)\\|_{2}^{2}\\right]$ where $\\nabla f(x)=\\left(\\mathrm{D}_{1} f(x), \\mathrm{D}_{2} f(x), \\ldots, \\mathrm{D}_{n} f(x)\\right)$.\n",
    ">\n",
    "\n",
    "\n",
    "Explicitly, we can write \n",
    "\n",
    "$$\\mathrm{D}_{i} f(x)=\\frac{f\\left(x^{(i \\mapsto 1)}\\right)-f\\left(x^{(i \\mapsto-1)}\\right)}{2} \\implies \\mathrm{D}_{i} x^{S}=\\left\\{\\begin{array}{ll}\n",
    "x^{S \\backslash\\{i\\}} & \\text { if } i \\in S \\\\\n",
    "0 & \\text { if } i \\notin S\n",
    "\\end{array}\\right. ,$$\n",
    "\n",
    "which follows that \n",
    "\n",
    "> **Thm.** If $\\sum_{S \\subseteq[n]} \\widehat{f}(S) x^{S}$, then \n",
    "> \n",
    "> $$\\mathrm{D}_{i} f(x)=\\sum_{\\substack{S \\subseteq[n] \\\\ S \\ni i}} \\widehat{f}(S) x^{S \\backslash\\{i\\}}.$$ \n",
    ">\n",
    "> **Cor.** $\\operatorname{Inf}_{i}[f]=\\sum_{S \\ni i} \\widehat{f}(S)^{2}$.\n",
    "\n",
    "**Proof.** $D_i$ is a linear operator.\n",
    "\n",
    "Some corollarys are as follows:\n",
    "\n",
    "> **Cor.** If $f:\\{-1,1\\}^{n} \\rightarrow\\{-1,1\\}$ is monotone, then $\\operatorname{Inf}_{i}[f]=\\widehat{f}(i)$.\n",
    "\n",
    "**Proof.** Note that $D_i f = (D_i f)^2$ is the $0$-$1$ indicator that $i$ is pivotal for $x$ if $f$ is monotone. \n",
    "\n",
    "> **Lemma.** If $f:\\{-1,1\\}^{n} \\rightarrow \\mathbb{R}$, then $\\mathbf{I}[f]=\\sum_{S \\subseteq[n]}|S| \\widehat{f}(S)^{2}=\\sum_{k=0}^{n} k \\cdot \\mathbf{W}^{k}[f]$. Specially, for  $f:\\{-1,1\\}^{n} \\rightarrow\\{-1,1\\}$, using the spectral sample, $\\mathbf{I}[f]=\\underset{\\mathbf{S} \\sim \\mathcal{S}_{f}}{\\mathbf{E}}[|\\mathbf{S}|]$.\n",
    ">\n",
    "> ( **Poincaré Inequality** ) $\\forall f:\\{-1,1\\}^{n} \\rightarrow \\mathbb{R}$, $\\operatorname{Var}[f] \\leq \\mathbf{I}[f]=\\underset{x}{\\mathbf{E}}\\left[\\|\\nabla f(x)\\|_{2}^{2}\\right]$.\n",
    "\n",
    "**Proof.** Recall that $\\operatorname{Var}[f]=\\sum_{k>0} \\mathbf{W}^{k}[f]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642efb1-303e-4430-8518-df78ce8102a5",
   "metadata": {},
   "source": [
    "$(x, y)$  is a  $\\rho$-correlated pair of random strings if independently $\\forall i \\in[n]$, the pair of random bits  $\\left(x_{i}, y_{i}\\right)$  satisfies $\\mathbf{E}\\left[x_{i}\\right]=\\mathbf{E}\\left[\\boldsymbol{y}_{i}\\right]=0$ and  $\\mathbf{E}\\left[\\boldsymbol{x}_{i} \\boldsymbol{y}_{i}\\right]=\\rho$. Then we define\n",
    "\n",
    "> **Def.** $\\mathbf{S t a b}_{\\rho}[f]=\\underset{\\substack{(\\boldsymbol{x}, \\boldsymbol{y}) \\\\ \\boldsymbol{\\mathbf { y }} \\text {-correlated }}}{\\mathbf{E}}[f(\\boldsymbol{x}) f(\\boldsymbol{y})]$. \n",
    ">\n",
    "> **Prop.** If $f\\in\\{-1,1\\}$, then we have $\\mathbf{S t a b}_{\\rho}[f]$ = $\\underset{\\substack{(\\boldsymbol{x}, \\boldsymbol{y}) \\rho \\text {-correlated }}}{\\mathbb{P}}[f(\\boldsymbol{x})$ = $f(\\boldsymbol{y})]-\\underset{\\substack{(\\boldsymbol{x}, \\boldsymbol{y}) \\rho \\text {-correlated }}}{\\mathbb{P}}[f(\\boldsymbol{x}) \\neq f(\\boldsymbol{y})]$ = $2 \\underset{\\substack{(\\boldsymbol{x}, \\boldsymbol{y}) \\rho \\text {-correlated }}}{\\mathbb{P}}[f(\\boldsymbol{x})$ = $f(\\boldsymbol{y})]-1$.\n",
    "\n",
    "When $\\rho$ is close to 1, i.e. the “noise” is small, it’s natural to consider noise sensitivity $\\mathbf{N S}_{\\delta}[f]=\\frac{1}{2}-\\frac{1}{2} \\mathbf{S t a b}_{1-2 \\delta}[f]$.\n",
    "\n",
    "Then we would know that \n",
    "\n",
    ">**Thm.** $$\\lim _{\\substack{n \\rightarrow \\infty \\\\ \\text{n odd} }} \\operatorname{Stab}_{\\rho}\\left[\\mathrm{Maj}_{n}\\right]=\\frac{2}{\\pi} \\arcsin \\rho=1-\\frac{2}{\\pi} \\arccos \\rho.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49823e8b-db38-42a6-890f-05af4fcba447",
   "metadata": {},
   "source": [
    "**Proof.** Observe that $$\\begin{aligned}\n",
    "\\operatorname{Stab}_{\\rho}\\left[\\operatorname{Maj}_{n}\\right] \n",
    "&  = \\underset{\\substack{(\\boldsymbol{x}, \\boldsymbol{y}) \\rho \\text {-correlated }}}{\\mathbf{E}}\\left[\\operatorname{Maj}_{n}(\\boldsymbol{x}) \\cdot \\operatorname{Maj}_{n}(\\boldsymbol{y})\\right] = \\underset{\\substack{(\\boldsymbol{x}, \\boldsymbol{y}) \\rho \\text {-correlated }}}{\\mathbf{E}}[\\operatorname{sgn}(\\sum_{i} \\frac{1}{\\sqrt{n}} \\boldsymbol{x}_{i}) \\cdot \\operatorname{sgn}(\\sum_{i} \\frac{1}{\\sqrt{n}} \\boldsymbol{y}_{i})] \\\\\n",
    "& =\\mathbf{E}\\left[\\operatorname{sgn}\\left(\\overrightarrow{\\boldsymbol{S}}_{1}\\right) \\cdot \\operatorname{sgn}\\left(\\overrightarrow{\\boldsymbol{S}}_{2}\\right)\\right] = \\mathbb{P}\\left[\\operatorname{sgn}\\left(\\overrightarrow{\\boldsymbol{S}}_{1}\\right)=\\operatorname{sgn}\\left(\\overrightarrow{\\boldsymbol{S}}_{2}\\right)\\right]-\\mathbb{P}\\left[\\operatorname{sgn}\\left(\\overrightarrow{\\boldsymbol{S}}_{1}\\right) \\neq \\operatorname{sgn}\\left(\\overrightarrow{\\boldsymbol{S}}_{2}\\right)\\right] \\\\\n",
    "& =2 \\mathbb{P}\\left[\\operatorname{sgn}\\left(\\overrightarrow{\\boldsymbol{S}}_{1}\\right)=\\operatorname{sgn}\\left(\\overrightarrow{\\boldsymbol{S}}_{2}\\right)\\right]-1=4 \\mathbb{P}\\left[\\overrightarrow{\\boldsymbol{S}} \\in Q_{--}\\right]-1,\n",
    "\\end{aligned} $$\n",
    "\n",
    "where $\\boldsymbol{Q}_{--}$ denotes the lower-left quadrant of $\\mathbb{R}^{2}$ and  set $\\overrightarrow{\\boldsymbol{S}}=\\sum_{i=1}^{n}\\left[\\begin{array}{c} \\frac{1}{\\sqrt{n}} \\boldsymbol{x}_{i} \\\\ \\frac{1}{\\sqrt{n}} \\boldsymbol{y}_{i} \\end{array}\\right] \\in \\mathbb{R}^{2}$ and $\\overrightarrow{\\boldsymbol{Z}} \\sim \\mathrm{N}\\left(\\left[\\begin{array}{l} 0 \\\\ 0 \\end{array}\\right],\\left[\\begin{array}{ll} 1 & \\rho \\\\ \\rho & 1 \\end{array}\\right]\\right)$. Then using the multi-dimensional CLT ( one way is to use **Invariance Principle** ), we have\n",
    "\n",
    "$$ \\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\vec{S} \\in Q_{--}\\right] = \\mathbb{P}\\left[\\vec{Z} \\in Q_{--}\\right]= \\frac{1}{2}-\\frac{1}{2} \\frac{\\arccos \\rho}{\\pi}$$\n",
    "\n",
    "which follows the conclusion. ( the last equality is the famous **Sheppard's Formula** )\n",
    "\n",
    "**Proof of Sheppard's Formula.** Since  $\\left(-\\boldsymbol{z},-\\boldsymbol{z}^{\\prime}\\right)$  has the same distribution as  $\\left(\\boldsymbol{z}, \\boldsymbol{z}^{\\prime}\\right)$, **Sheppard's Formula** is equivalent to proving\n",
    "\n",
    "$$\\operatorname{Pr}\\left[z \\leq 0, z^{\\prime} \\leq 0 \\text { or } z>0, z^{\\prime}>0\\right]=1-\\frac{\\arccos \\rho}{\\pi} \\Leftrightarrow  \\underset{\\overrightarrow{\\boldsymbol{g}} \\sim \\mathrm{N}(0,1)^{2}}{\\mathbb{P}}[\\langle\\vec{u}, \\overrightarrow{\\boldsymbol{g}}\\rangle \\leq 0 \\text{ & } \\langle\\overrightarrow{\\boldsymbol{v}}, \\overrightarrow{\\boldsymbol{g}}\\rangle>0 \\text { or vice versa }]=\\frac{\\theta}{\\pi} $$\n",
    "\n",
    "for all  $\\theta \\in[0, \\pi]$ where  $\\vec{u}, \\vec{v} \\in \\mathbb{R}^{2}$  is some fixed pair of unit vectors making an angle of  $\\theta$, and  $\\overrightarrow{\\boldsymbol{g}} \\sim \\mathrm{N}(0,1)^{2}$, which is clear by rotational symmetry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05174146-0746-4525-8c90-ffd28fbb1857",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0daf3be-8c0f-4260-a334-d553eadbbd1a",
   "metadata": {},
   "source": [
    "### 2. Decision Tree and DNF Formula\n",
    "\n",
    "> A **decision tree**  $T$  is a representation of $f: \\mathbb{F}_{2}^{n} \\rightarrow \\mathbb{R}$. It consists of a rooted binary tree in which the internal nodes are labeled by coordinates  $i \\in[n]$, the outgoing edges of each internal node are labeled $0$ and $1$, and the leaves are labeled by real numbers. \n",
    ">\n",
    "> **We insist that no coordinate  $i \\in[n]$  appears more than once on any root-to-leaf path.**\n",
    "\n",
    "The size  $s$  of $T$  is the total number of leaves. The depth  $k$  of  $T$  is the maximum length of any root-to-leaf path. \n",
    "\n",
    "> For decision trees over  $\\mathbb{F}_{2}^{n}$, we have  $k \\leq n$  and  $s \\leq 2^{k}$. \n",
    "\n",
    "Let  $T$  be computing  $f: \\mathbb{F}_{2}^{n} \\rightarrow \\mathbb{R}$  and let $P$  be one of its root-to-leaf paths. The set of inputs  $x$  that follow computation path  $P$  in  $T$  is precisely a subcube of  $\\mathbb{F}_{2}^{n}$, call it  $C_{P}$, which follows that \n",
    "\n",
    "> **Prop.** $$ f=\\sum_{\\text {paths } P \\text { of } T} f(P) \\cdot 1_{C_{P}} .$$\n",
    "\n",
    "One of the commonest ways of representing a Boolean function  $f:\\{0,1\\}^{n} \\rightarrow   \\{0,1\\}$  is by a DNF formula:\n",
    "\n",
    "> **Def.** A  **DNF**  ( disjunctive normal form ) formula is a logical OR of terms, each of which is a logical AND of literals, which is either $x_{i} $ or $\\bar{x}_{i}$.\n",
    ">\n",
    "> **Def.** A **CNF** ( conjunctive normal form ) formulas is a logical AND of clauses, each of which is a logical OR of literals. \n",
    "\n",
    "The **size** of a DNF ( CNF ) formula is its number of terms. The **width** is the maximum width of its terms. If we take a CNF computing $f$ and switch its ANDs and ORs, the result is a DNF computing the dual function $-f(-x)$. Since $f$ and $-f(-x)$ have essentially the same Fourier expansion, there isn’t much difference between CNFs and DNFs when it comes to Fourier analysis.\n",
    "\n",
    "> **Prop.** Let $f: \\mathbb{F}_{2}^{n} \\rightarrow \\{0,1\\}$ be computable by a decision tree $\\boldsymbol{T}$ of size  $s$  and depth  $k$. Then  $f$  is computable by a **DNF** ( **CNF** ) of size at most  $s$  and width at most  $k$.\n",
    "\n",
    "Proof. Take each path in  $T$  from the root to a leaf labeled $1$ and form the logical AND of the literals describing the path. These are the terms of the required DNF. ( For the CNF clauses, take paths to label $0$ and negate all literals describing the path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77348a58-5ad2-4717-9839-4355496cd93a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87057339-c15c-409d-8cb3-8e8fb95b32d0",
   "metadata": {},
   "source": [
    "### 3. Learning and Crypotography\n",
    "\n",
    "A **randomized algorithm** is one that adds a degree of randomness to its logic to produce the final output. A learning algorithm  $A$  is a randomized algorithm which has limited access to an unknown target function  $f \\in \\mathscr{C}$. The two access models, in increasing order of strength, are: \n",
    "- random examples, meaning  $A$  can draw pairs  $(x, f(x))$  where  $x \\in\\{-1,1\\}^{n}$  is uniformly random;\n",
    "- queries, meaning  $A $ can request the value  $f(x)$  for any  $x \\in\\{-1,1\\}^{n}$  of its choice.\n",
    "\n",
    "\n",
    "> Let  $\\mathscr{F} \\subset \\mathbf{P}([n])$, the Fourier spectrum of  $f:\\{-1,1\\}^{n} \\rightarrow \\mathbb{R}$  is  **$\\epsilon$-concentrated** on  $\\mathscr{F} $ if \n",
    ">\n",
    "> $$\\sum_{\\substack{S \\subseteq[n] \\\\ S \\notin \\mathscr{F}}} \\widehat{f}(S)^{2} \\leq \\epsilon.$$\n",
    ">\n",
    "> **Lemma I.** Given access to random examples from  $f:\\{-1,1\\}^{n} \\rightarrow\\{-1,1\\}$, $\\exists$ randomized algorithm which takes as input  $S \\subseteq[n]$, $0<\\delta, \\epsilon \\leq 1 / 2$, and outputs an estimate  $\\widetilde{f}(S)$  for  $\\widehat{f}(S)$  that satisfies\n",
    ">\n",
    "> $$|\\widetilde{f}(S)-\\widehat{f}(S)| \\leq \\epsilon$$\n",
    ">\n",
    "> except with probability at most  $\\delta$. The running time is poly$(n, 1 / \\epsilon) \\cdot \\log (1 / \\delta)$.\n",
    ">\n",
    "> **Lemma II.** Suppose $f:\\{-1,1\\}^{n} \\rightarrow\\{-1,1\\}$  and  $g:\\{-1,1\\}^{n} \\rightarrow \\mathbb{R}$  satisfy  $\\|f-g\\|_{2}^{2} \\leq \\epsilon$. Then  $\\operatorname{dist}(f, h) \\leq \\epsilon$ where $h(x)=\\operatorname{sgn}(g(x))$.\n",
    "\n",
    "Proof. For the first, we have  $\\widehat{f}(\\boldsymbol{S})=\\mathbf{E}_{\\boldsymbol{x}}\\left[f(\\boldsymbol{x}) \\chi_{S}(\\boldsymbol{x})\\right]$. Given random examples  $(\\boldsymbol{x}, f(\\boldsymbol{x}))$, the algorithm can compute  $f(x) \\chi_{S}(x) \\in\\{-1,1\\}$  and therefore empirically estimate  $\\mathbf{E}_{x}\\left[f(\\boldsymbol{x}) \\chi_{S}(\\boldsymbol{x})\\right]$. **Hoeffding's inequality** implies that  $O\\left(\\log (1 / \\delta) / \\epsilon^{2}\\right)$  examples are sufficient to obtain an estimate within  $\\pm \\epsilon$  with probability at least  $1-\\delta$. For the second, since  $|f(x)-g(x)|^{2} \\geq 1$  whenever  $f(x) \\neq \\operatorname{sgn}(g(x))$, we conclude  \n",
    "\n",
    "$$\\operatorname{dist}(f, h)=\\underset{x}{\\mathbf{P r}}[f(x) \\neq h(x)]=\\underset{x}{\\mathbf{N}}\\left[\\mathbf{1}_{f(x) \\neq \\operatorname{sgn}(g(x))}\\right] \\leq \\underset{x}{\\mathbf{E}}\\left[|f(x)-g(x)|^{2}\\right]=\\|f-g\\|_{2}^{2}.$$ \n",
    "\n",
    "Then we may prove\n",
    "\n",
    "> Assume learning algorithm $A$ has random example access to target  $f:\\{-1,1\\}^{n} \\rightarrow\\{-1,1\\}$. Suppose that $A$ can identify a collection  $\\mathscr{F}$  of subsets on which  $f $'s Fourier spectrum is  $\\epsilon / 2$-concentrated. Then using poly$(|\\mathscr{F}|, n, 1 / \\epsilon)$  additional time, $A$ can with high probability output a hypothesis  $h$  that is  $\\epsilon$-close to  $f$.\n",
    "\n",
    "**Proof**. For each  $S \\in \\mathscr{F}$,  $A$ can produce an estimate  $\\widetilde{f}(S)$  for  $\\widehat{f}(S)$  which satisfies  $|\\widetilde{f}(S)-\\widehat{f}(S)| \\leq \\sqrt{\\epsilon} \\big/ (2 \\sqrt{|\\mathscr{F}|})$  except with probability at most  $1 /(10|\\mathscr{F}|)$. Overall this requires poly $(|\\mathscr{F}|, n, 1 / \\epsilon)$  time, and by the union bound, except with probability at most  $1 / 10$  all  $|\\mathscr{F}|$  estimates have the desired accuracy. Finally,  $A$  forms the real-valued function  $g=\\sum_{S \\in \\mathscr{F}} \\widetilde{f}(S) \\chi_{S}$  and outputs hypothesis  $h=\\operatorname{sgn}(g)$. By Lemma II, it suffices to show that  $\\|f-g\\|_{2}^{2} \\leq \\epsilon$. And indeed,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\|f-g\\|_{2}^{2} & =\\sum_{S \\subseteq[n]} \\widehat{f-g}(S)^{2} = \\sum_{S \\in \\mathscr{F}}(\\widehat{f}(S)-\\widetilde{f}(S))^{2}+\\sum_{S \\notin \\mathscr{F}} \\widehat{f}(S)^{2} \\\\\n",
    "& \\leq \\sum_{S \\in \\mathscr{F}}\\left(\\frac{\\sqrt{\\epsilon}}{2 \\sqrt{|\\mathscr{F}|}}\\right)^{2}+\\epsilon / 2 = \\epsilon / 4+\\epsilon / 2 \\quad \\leq \\epsilon, \\end{aligned}$$\n",
    "\n",
    "as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5331e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Noga Alon. The Probabilistic Method.\n",
    "2. Ryan O'Donnell. Analysis of Boolean Functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
