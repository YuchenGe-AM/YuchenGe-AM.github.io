{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Sep 15] Markov Decision Process II\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716c726",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446c9c8",
   "metadata": {},
   "source": [
    "### 1. Causal Conditioning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41da825-64b8-4f1a-a725-92fddba3a80d",
   "metadata": {},
   "source": [
    "Most basically, $H(X):=\\mathbf{E} I(x)$ and $H(X|Y):=\\mathbf{E} I(x|y)$  where $I(x):=-\\log P(x)$ and $I(x|y):=-\\log P(x|y)$. Here follows the most basic inequalities.\n",
    "\n",
    "> $H(X)-H(X \\mid Y)=I(X ; Y) \\geq 0$ and $H(X|Z) - H(X|YZ)= I(X ; Y Z)-I(X ; Z) = I(X ; Y \\mid Z) = I(Y ; X \\mid Z)\\geq 0$.\n",
    "\n",
    "Generalizing from above, the probability mass of $X^{n}$ **causally conditioned** on $Y^{n-d}$ is defined as $P\\left(x^{n}|| y^{n-d}\\right):=\\prod_{i=1}^{n} P\\left(x_{i} \\mid x^{i-1}, y^{i-d}\\right)$.  We call them the causal conditioning distribution since $ \\sum_{x^{N}} P\\left(x^{N} \\| y^{N-d}\\right)=1$. The directed information Here the term causal reflects the conditioning on past and present values of the sequence $Y^{N}$. Correspondingly, the directed information is \n",
    "\n",
    "$$I\\left(X^{n} \\rightarrow Y^{n}\\right) := \\sum_{i=1}^{n} I\\left(X^{i} ; Y_{i} \\mid Y^{i-1}\\right)\n",
    "= \\sum_{i=1}^{n} \\big( H\\left(Y^{i} | Y^{i-1} \\right) - H\\left(Y^{i} |X^i,  Y^{i-1} \\right) \\big) \n",
    "=\\mathbf{E}\\left[\\log \\frac{P\\left(Y^{N} \\| X^{N}\\right)}{P\\left(Y^{N}\\right)}\\right] =: H\\left(Y^{N}\\right)-H\\left(Y^{N} \\| X^{N}\\right) . $$\n",
    "\n",
    "> **Prop I.** $H\\left(X^{N} \\mid Y^{N}\\right) \\leq H\\left(X^{N} \\| Y^{N}\\right) \\leq H\\left(X^{N}\\right) \\implies 0 \\leq I\\left(X^{N} \\rightarrow Y^{N}\\right) \\leq I\\left(X^{N} ; Y^{N}\\right)$.\n",
    ">\n",
    "> **Prop II ( Chain Rule I )**. $I\\left(X^{N} Y^{N} \\rightarrow Z^{N}\\right) = I\\left(X^{N} \\rightarrow Z^{N}\\right)+I\\left(Y^{N} \\rightarrow Z^{N} \\| X^{N}\\right) $\n",
    ">\n",
    "> **Prop III ( Chain Rule II )**. $I\\left(X^{N} \\rightarrow Y^{N} Z^{N}\\right) = I\\left(X^{N} \\rightarrow Y^{N} \\| D Z^{N}\\right) + I\\left(X^{N} \\rightarrow Z^{N} \\| Y^{N}\\right)$ where $D Z^{N}:= (0, Z_{1}, Z_{2}, \\ldots, Z_{N-1})$.\n",
    ">\n",
    "> Here if removing $X^{N} \\rightarrow$, **Chain Rule II** remains true, i.e. $H\\left(Y^{N} Z^{N}\\right) = H\\left(Y^{N} \\| D Z^{N}\\right) + H\\left(Z^{N} \\| Y^{N}\\right)$.\n",
    "\n",
    "**Proof.** For the first:\n",
    "$$  H\\left(X^{N} \\mid Y^{N}\\right)=\\sum_{n=1}^{N} H\\left(X_{n} \\mid X^{n-1} Y^{N}\\right) \\leq  \\sum_{n=1}^{N} H\\left(X_{n} \\mid X^{n-1} Y^{n}\\right) = H\\left(X^{N} \\| Y^{N}\\right). $$\n",
    "\n",
    "For the second:\n",
    "$$ I\\left(X^{N} Y^{N} \\rightarrow Z^{N}\\right)= H\\left(Z^{N}\\right)-H\\left(Z^{N} \\| X^{N} Y^{N}\\right)\n",
    "=  {\\left[H\\left(Z^{N}\\right)-H\\left(Z^{N} \\| X^{N}\\right)\\right]+} {\\left[H\\left(Z^{N} \\| X^{N}\\right)-H\\left(Z^{N} \\| X^{N} Y^{N}\\right)\\right] . } $$\n",
    "\n",
    "\n",
    "For the third, simply observe that $ I\\left(X^{n} ; Y_{n} Z_{n} \\mid Y^{n-1} Z^{n-1}\\right)=\n",
    "I\\left(X^{n} ; Y_{n} \\mid Y^{n-1} Z^{n-1}\\right)+I\\left(X^{n} ; Z_{n} \\mid Y^{n} Z^{n-1}\\right)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52259ba-dc7d-4059-8326-0d0cd0f52e9a",
   "metadata": {},
   "source": [
    "It's also natural to consider $I\\left(X^{N} \\rightarrow Y^{N}\\right)+I\\left(Y^{N} \\rightarrow X^{N}\\right)$.\n",
    "\n",
    "> $I\\left(X^{N} ; Y^{N}\\right) = I\\left(X^{N} \\rightarrow Y^{N}\\right)+I\\left( DY^N \\rightarrow X^{N}\\right) = I\\left(X^{N} \\rightarrow Y^{N}\\right) + I\\left(Y^{N} \\rightarrow X^{N}\\right)  - I\\left( Y^N \\rightarrow X^{N}|| DY^N \\right)$.\n",
    "\n",
    "which can be seen via \n",
    "\n",
    "$$ I\\left(X^{N} ; Y^{N}\\right) = \\mathbf{E}\\left[\\log \\frac{P\\left(Y^{N}, X^{N}\\right)}{P\\left(Y^{N}\\right) P\\left(X^{N}\\right)}\\right] = \\mathbf{E}\\left[\\log \\frac{P\\left(Y^{N} \\| X^{N}\\right) P\\left(X^{N} \\| Y^{N-1}\\right)}{P\\left(Y^{N}\\right) P\\left(X^{N}\\right)}\\right] = I\\left(X^{N} \\rightarrow Y^{N}\\right)+I\\left(\\left\\{0, Y^{N-1}\\right\\} \\rightarrow X^{N}\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee1745-be7b-4941-89ae-c68cc0961f4f",
   "metadata": {},
   "source": [
    ">  Analogue to  $|I(X ; Y)-I(X ; Y \\mid S)| \\leq H(S)$, we have $\\left|I\\left(X^{N} \\rightarrow Y^{N}\\right)-I\\left(X^{N} \\rightarrow Y^{N} \\mid S\\right)\\right| \\leq H(S) \\leq \\log |\\mathcal{S}|$.\n",
    "\n",
    "**Proof.** We have \n",
    "\n",
    "$$\\big|I\\left(X^{N} \\rightarrow Y^{N}\\right)-I\\left(X^{N} \\rightarrow Y^{N}, S\\right)\\big| \n",
    "= \\big|\\sum_{i=1}^{N} I\\left(Y_{i} ; X^{i} \\mid Y^{i-1}\\right)-I\\left(Y_{i} ; X^{i} \\mid Y^{i-1}, S\\right)\\big| \n",
    "= \\big|\\sum_{i=1}^{N} I\\left(Y_{i} ; S \\mid Y^{i-1}\\right)-I\\left(Y_{i} ; S \\mid Y^{i-1}, X^{i}\\right)\\big|.\n",
    "$$\n",
    "\n",
    "Then we have \n",
    " \n",
    "$$ \\big|\\sum_{i=1}^{N} I\\left(Y_{i} ; S \\mid Y^{i-1}\\right)-I\\left(Y_{i} ; S \\mid Y^{i-1}, X^{i}\\right)\\big| \n",
    "\\leq \\max \\left(I\\left(Y^{N} ; S\\right), I\\left(Y^{N}, X_{2}^{N} ; S \\mid X_{1}\\right)\\right) \\leq  \\max (H(S), H(S)) \\leq \\log |\\mathcal{S}| .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d508858-1575-4bfc-9240-b07051c322ee",
   "metadata": {},
   "source": [
    "Observe that $P\\left(x_{i} \\mid x^{i-1}, z^{i-1}\\right)= P\\left(x^{i} \\| z^{i-1}\\right) \\big/ P\\left(x^{i-1} \\| z^{i-2}\\right)$, and we have\n",
    "\n",
    ">  $P\\left(x^{n-1} \\| z^{n-2}\\right)=\\sum_{x_{n}} P\\left(x^{n} \\| z^{n-1}\\right)$; and $P\\left(x^{n} \\| z^{n-1}\\right)$ uniquely determines $P\\left(x_{i} \\mid x^{i-1}, z^{i-1}\\right)$, $i \\leq n$.\n",
    "\n",
    "Finally, we would like to add that if there is no feedback, namely $X_{i}-X^{i-1}-Y^{i-1}$, then $I\\left(X^{n} \\rightarrow Y^{n}\\right)=I\\left(X^{n} ; Y^{n}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9868324e-e591-407c-9eb6-8f2f7e3d52b7",
   "metadata": {},
   "source": [
    "### 2. Finite State Channels\n",
    "\n",
    "Finite State Channels are described statistically by $P\\left(y_{n}, s_{n} \\mid x_{n}, s_{n-1}\\right)$ s.t. \n",
    "\n",
    "$$P\\left(y_{i}, s_{i} \\mid x^{i}, s^{i-1}, y^{i-1}, a_{e}^{i}, a_{d}^{i}, m\\right)=P\\left(y_{i}, s_{i} \\mid x_{i}, s_{i-1}\\right).$$\n",
    "\n",
    "Recall basic properties of  **Bayesian Network** ( proved via $d$-separation ):\n",
    "\n",
    "> **Decomposition**: $\\big( \\mathbf{X} \\perp \\mathbf{Y}, \\mathbf{W} \\mid \\mathbf{Z} \\big) \\Longrightarrow \\big(\\mathbf{X} \\perp \\mathbf{Y} \\mid \\mathbf{Z} \\big)$.\n",
    ">\n",
    "> **Weak union**: $\\big(\\mathbf{X} \\perp \\mathbf{Y}, \\mathbf{W} \\mid \\mathbf{Z}\\big) \\Longrightarrow \\big(\\mathbf{X} \\perp \\mathbf{Y} \\mid \\mathbf{Z}, \\mathbf{W}\\big)$.\n",
    ">\n",
    "> **Contraction**: $\\big(\\mathbf{X} \\perp \\mathbf{W} \\mid \\mathbf{Z}, \\mathbf{Y}\\big)$ & $\\big(\\mathbf{X} \\perp \\mathbf{Y} \\mid \\mathbf{Z}\\big) \\Longrightarrow \\big(\\mathbf{X} \\perp \\mathbf{Y}, \\mathbf{W} \\mid \\mathbf{Z}\\big)$.\n",
    "> \n",
    "> **Intersection**: $\\big(\\mathbf{X} \\perp \\mathbf{Y} \\mid \\mathbf{Z}, \\mathbf{W}\\big)$ & $\\big(\\mathbf{X} \\perp \\mathbf{W} \\mid \\mathbf{Z}, \\mathbf{Y}\\big) \\Longrightarrow \\big(\\mathbf{X} \\perp \\mathbf{Y}, \\mathbf{W} \\mid \\mathbf{Z}\\big)$.\n",
    "\n",
    "Then we give the following framework:\n",
    "\n",
    "> Generate encoder action using $f_{A_{e, i}}: \\mathcal{M} \\times \\mathcal{Z}^{i-1} \\rightarrow \\mathcal{A}_{e}$ where  $Z_{i} \\in \\mathcal{Z}$  is the sampled feedback component, i.e. $A_{e, i}=f_{A_{e, i}}\\left(M, Z^{i-1}\\right)$.\n",
    ">\n",
    "> Generate decoder action using $f_{A_{d, i}}: \\mathcal{Y}^{i-1} \\rightarrow \\mathcal{A}_{d}$ where  $Y_{i} \\in \\mathcal{Y}$  is the channel output.\n",
    ">\n",
    "> Generates sampled feedback $Z_{i}=f\\left(A_{e, i}, A_{d, i}, Y_{i}\\right)$ where $f$ is determinstic.\n",
    ">\n",
    "> Construct channel input $X_{i}\\left(M, Z^{i-1}\\right)$ using the encoding function $f_{e, i}: \\mathcal{M} \\times \\mathcal{Z}^{i-1} \\rightarrow \\mathcal{X}$; generate $\\hat{M}\\left(Y^{N}\\right)$ using the decoding function $f_{d}: \\mathcal{Y}^{N} \\rightarrow \\mathcal{M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9d6d7-1fc7-49d5-9a3b-e1604fd0080d",
   "metadata": {},
   "source": [
    "\n",
    "The joint probability distribution induced by a given scheme,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& P_{M, A_{e}^{N}, A_{d}^{N}, Z^{N}, X^{N}, S_{0}^{N}, Y^{N}, \\hat{M}}\\big(m, a_{e}^{N}, a_{d}^{N}, z^{N}, x^{N}, s_{0}^{N}, y^{N}, \\hat{m}\\big) \\\\\n",
    "= & \\frac{1}{|\\mathcal{M}|} P_{S}\\big(s_{0}\\big) \\prod_{i=1}^{n} \\mathbf{1}_{\\big\\{a_{d, i}=f_{A_{d, i}}(y^{i-1})\\big\\}\\big.} \\mathbf{1}_{\\big\\{a_{e, i}=f_{A_{e, i}}(m, z^{i-1})\\big\\}} \\mathbf{1}_{\\big\\{x_{i}=f_{e, i}(m, z^{i-1})\\big\\}} P\\big(y_{i}, s_{i} \\mid x_{i}, s_{i-1}\\big) \\mathbf{1}_{\\big\\{z_{i}=f(a_{e, i}, a_{d, i}, y_{i})\\big\\}} \\times \\mathbf{1}_{\\big\\{\\hat{m}=f_{d}(y^{n})\\big\\}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "> A rate $R$  is **achievable** if $\\exists$ a sequence of block codes  $\\left(N,\\left\\lceil 2^{N R}\\right\\rceil\\right)$  s.t. \n",
    ">\n",
    "> $$\\max _{m \\in\\left\\{1, \\cdots,\\left\\lceil 2^{N R}\\right\\rceil\\right\\}} P(\\hat{m} \\neq m \\mid \\text { message } m \\text { was sent })$$ \n",
    ">\n",
    "> vanishes as  $N \\rightarrow \\infty$. The capacity $C$ is the supremum of all achievable rates.\n",
    "\n",
    "Equivalently a rate $R$ is achieveable if $P_{e}^{N}\\left(s_{0}\\right) \\rightarrow 0, \\forall s_{0} \\in \\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06e66a-5c78-41f9-abc5-c06873851397",
   "metadata": {},
   "source": [
    "First we give proof of some Markov chains.\n",
    "\n",
    "Summing over  $\\left(\\hat{M}, X_{i+1}^{N}, Z_{i}^{N}, S_{i+1}^{N}, Y_{i+1}^{N}, A_{e, i+1}^{N}, A_{d, i+1}^{N}\\right)$  in the given distribution scheme,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& P_{M, A_{e}^{i}, A_{d}^{i}, Z^{i-1}, X^{i}, S_{0}^{i}, Y^{i}}\\left(m, a_{e}^{i}, a_{d}^{i}, z^{i-1}, x^{i}, s_{0}^{i}, y^{i}\\right) \\\\\n",
    "= & \\frac{1}{|\\mathcal{M}|} \\prod_{j=1}^{i} \\mathbf{1}_{\\big\\{a_{d, j}=f_{A_{d, j}}(y^{j-1})\\big\\}} \\mathbf{1}_{\\big\\{a_{e, j}=f_{A_{e, j}}(m, z^{j-1})\\big\\}} \\mathbf{1}_{\\big\\{x_{j}=f_{e, j}(m, z^{j-1})\\big\\}} \\times \\prod_{j=1}^{i-1} 1_{\\big\\{z_{i}=f(a_{e, i}, a_{d, i}, y_{i})\\big\\}} \\times \\prod_{j=1}^{i} P\\left(y_{j}, s_{j} \\mid x_{j}, s_{j-1}\\right) \\times P_{S}\\left(s_{0}\\right) \\\\\n",
    "= &  \\Phi_{1}\\left(M, A_{e}^{i}, A_{d}^{i}, Z^{i-1}, Y^{i-1}, X^{i}\\right) \\Phi_{2}\\left(Y^{i}, S_{0}^{i}, X^{i}\\right).\\end{aligned}$$\n",
    "\n",
    "It follows that \n",
    "\n",
    "> $\\left(M, A_{e}^{i}, A_{d}^{i}, Z^{i-1}\\right)-\\left(Y^{i-1}, X^{i}, S_{0}\\right)-\\left(Y_{i}, S^{i}\\right)$ is a Markov chain $\\implies$ $Y_{i}-\\left(X^{i}, Y^{i-1}, S_{0}\\right)-\\left(M, A_{e}^{i}, A_{d}^{i}\\right)$ is a Markov chain.\n",
    "\n",
    "Similarly, sum over $\\left(\\hat{M}, X_{i+1}^{N}, Z_{i}^{N}, S_{i}^{N}, Y_{i}^{N}, A_{e, i+1}^{N}, A_{d, i}^{N}\\right)$ and we have\n",
    "\n",
    "> $\\left(M, X_{i}, A_{e, i}\\right)-\\left(X^{i-1}, A_{e}^{i-1}, Z^{i-1}\\right)-\\left(Y^{i-1}, S_{0}^{i-1}, A_{d}^{i-1}\\right)$  $\\implies$ $\\left(X_{i}, A_{e, i}\\right)-\\left(X^{i-1}, A_{e}^{i-1}, Z^{i-1}\\right)-\\left(Y^{i-1}, A_{d}^{i-1}, S_{0}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64032c-055f-40d8-908f-995e1869aef2",
   "metadata": {},
   "source": [
    "Sum over $\\left(\\hat{M}, X_{i+1}^{N}, Z_{i}^{N}, S_{i}^{N}, Y_{i}^{N}, A_{e, i+1}^{N}, A_{d, i+1}^{N}\\right)$ and we have\n",
    "> $A_{d, i}-\\left(Y^{i-1}, A_{d}^{i-1}\\right)-\\left(X^{i}, A_{e}^{i}, Z^{i-1}, S_{0}^{i-1}, M\\right) \\implies A_{d, i}-\\left(Y^{i-1}, A_{d}^{i-1}\\right)-\\left(X^{i}, A_{e}^{i}, Z^{i-1}, S_{0}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2de793-8d8a-41c4-a083-64bcc0d96c07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9055d-cdc3-406a-b2e5-38ec28d59e14",
   "metadata": {},
   "source": [
    "### 3. Capacity\n",
    "\n",
    "Given a constraint $\\mathrm{E}\\left[\\Lambda\\left(A_{e}^{N}, A_{d}^{N}\\right)\\right]=\\mathrm{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\Lambda\\left(A_{e, i}, A_{d, i}\\right)\\right] \\leq \\Gamma$ where  $\\Lambda(\\cdot, \\cdot)$  is a given cost function, we define\n",
    "\n",
    "> **Def.** Set $\\underline{C}_{N}(\\Gamma) := \\frac{1}{N} \\max \\min _{s_{0}} I\\left(X^{N} \\rightarrow Y^{N} \\mid s_{0}\\right)$ and $\\bar{C}_{N}(\\Gamma) := \\frac{1}{N} \\max \\max _{s_{0}} I\\left(X^{N} \\rightarrow Y^{N} \\mid s_{0}\\right) $.\n",
    "\n",
    "Then we define $Q\\left(x^{i}, a_{e}^{i} \\mid x^{i-1}, a_{e}^{i-1}, z^{i-1}\\right)$ and $Q\\left(a_{d, i} \\mid a_{d}^{i-1}, y^{i-1}\\right)$ generating **encoder code-trees** and **decoder action code-trees** correspondingly where the Markov chains properties are used to see that these conditional probabilities are only dependent on these inputs.\n",
    "\n",
    "Specifically, \n",
    "$$ P\\left(x_{i}, a_{e, i} \\mid x^{i-1}, a_{e}^{i-1}, a_{d}^{i-1}, y^{i-1}, s_{0}^{i}\\right) = P\\left(x_{i}, a_{e, i} \\mid x^{i-1}, a_{e}^{i-1}, a_{d}^{i-1}, y^{i-1}, z^{i-1}, s_{0}^{i}\\right) = Q\\left(x_{i}, a_{e, i} \\mid x^{i-1}, a_{e}^{i-1}, z^{i-1}\\right) \\quad  (1) $$\n",
    "\n",
    "$$ P\\left(a_{d, i} \\mid a_{d}^{i-1}, y^{i-1}, x^{i}, a_{e}^{i}, s_{0}^{i}\\right)=Q\\left(a_{d, i} \\mid a_{d}^{i-1}, y^{i-1}\\right) \\quad (2) $$\n",
    "\n",
    "where we used $z_{i}=f\\left(a_{e, i}, a_{d, i}, y_{i}\\right)$ and some existing markov chains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f5f84-b373-4f70-bf7d-12dd211dfbe9",
   "metadata": {},
   "source": [
    "Note that\n",
    "\n",
    "> $P\\left(s_{0}, x^{N}, a_{e}^{N}, a_{d}^{N}, y^{N}\\right)=P\\left(s_{0}\\right) Q\\left(x^{N}, a_{e}^{N} \\| z^{N-1}\\right) Q\\left(a_{d}^{N} \\| y^{N-1}\\right) P\\left(y^{N} \\| x^{N}, s_{0}\\right)$.\n",
    "\n",
    "It can be seen via\n",
    "\n",
    "$$ \\begin{aligned} \n",
    "P\\left(s^N, x^{N}, a_{e}^{N}, a_{d}^{N}, y^{N}\\right) & = P\\left(s_{0}\\right) Q\\left(x^{N}, a_{e}^{N}, a_{d}^{N} \\| y^{N-1}, s_{0}\\right) P\\left(y^{N}, s^{N} \\| x^{N}, a_{e}^{N}, a_{d}^{N}, s_{0}\\right) \\\\\n",
    "& = P\\left(s_{0}\\right) Q\\left(x^{N}, a_{e}^{N} \\| z^{N-1}\\right) Q\\left(a_{d}^{N} \\| y^{N-1}\\right) \\prod_{i=1}^{N} P\\left(y_{i}, s_{i} \\mid x_{i}, s_{i-1}\\right)\n",
    "\\end{aligned} $$\n",
    "\n",
    "It follows that \n",
    "\n",
    "$$ P\\left(s_{0}^{N}, x^{N}, a_{e}^{N}, a_{d}^{N}, y^{N}\\right)=P\\left(s_{0}\\right) Q\\left(x^{N}, a_{e}^{N} \\| z^{N-1}\\right) Q\\left(a_{d}^{N} \\| y^{N-1}\\right) P\\left(y^{N} \\| x^{N}, s_{0}\\right). $$\n",
    "\n",
    "where (1), (2), and the following equations are used.\n",
    "\n",
    "> $P\\left(y^{N}, s_1^N \\| x^{N}, a_{e}^{N}, a_{d}^{N}, s_{0}\\right)=P\\left(y^{N}, s_1^N \\| x^{N}, s_{0}\\right)$ and $P\\left(y^{N}, s_1^N \\| x^{N}, a_{e}^{N}, a_{d}^{N}\\right)=P\\left(y^{N}, s_1^N \\| x^{N}\\right)$.\n",
    ">\n",
    "> $ P\\left(y^{N}, s_{0}^{N}, x^{N}\\right)=P\\left(s_{0}\\right) Q\\left(x^{N}|| z^{N-1}\\right) \\prod_{i=1}^{N} P\\left(y_{i}, s_{i} \\mid x_{i}, s_{i-1}\\right) \\implies P\\left(y^{N} \\| x^{N}, s_{0}\\right) = \\sum_{s^{N}} \\prod_{i=1}^{N} P\\left(y_{i}, s_{i} \\mid x_{i}, s_{i-1}\\right). $ \n",
    ">\n",
    "> $ P\\left(y^{N}, s_{0}^{N}, x^{N}\\right)=P\\left(s_{0}\\right) Q\\left(x^{N}|| z^{N-1}\\right) \\prod_{i=1}^{N} P\\left(y_{i}, s_{i} \\mid x_{i}, s_{i-1}\\right) \\implies P\\left(y^{N} \\| x^{N}\\right) = \\sum_{s_{0}^{N}}\\left(\\prod_{i=1}^{N} P\\left(y_{i}, s_{i} \\mid x_{i}, s_{i-1}\\right)\\right) P\\left(s_{0}\\right) $.\n",
    "\n",
    "Specially, we have \n",
    "\n",
    "> $ P\\left(y^{N} \\| x^{N}\\right)  = \\sum_{s_{0}} P\\left(y^{N} \\| x^{N}, s_{0}\\right) P\\left(s_{0}\\right) $.\n",
    ">\n",
    "> $  P\\left(x^{N}, y^{N}\\right)=Q\\left(x^{N} \\| z^{N-1}\\right) P\\left(y^{N} \\| x^{N}\\right) $. \n",
    "\n",
    "For decoding, we assume the message  $m$  The decoder performs ML decoding, i.e.  $P\\left(y^{N} \\mid m\\right)$  is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b701b1-fb1e-4e3f-9ea8-43a00c14bfc7",
   "metadata": {},
   "source": [
    "Since \n",
    "\n",
    "$$ P\\left(y^{N} \\mid m\\right) = \\prod_{i=1}^{N} P\\left(y_{i} \\mid y^{i-1}, m\\right) = \\prod_{i=1}^{N} P\\left(y_{i} \\mid y^{i-1}, a_{d}^{i}\\left(y^{i-1}\\right), m, x^{i}\\left(m, z^{i-1}\\right), a_{e}^{i}\\left(m . z^{i-1}\\right)\\right) = P\\left(y^{N} \\| x^{N}, a_{e}^{N}, a_{d}^{N}\\right) = P\\left(y^{N} \\| x^{N}\\right),\n",
    "$$\n",
    "\n",
    "where the second equality follows from the fact that knowing $m$ and $y^{i-1}$ would deduce that we know $\\left(x^{i}, a_{e}^{i}, a_{d}^{i}\\right)$, \n",
    "\n",
    "> $\\hat{m}=\\underset{m}{\\operatorname{argmax}} P\\left(y^{N} \\mid m\\right)$ = the $m$ corresponding to $\\underset{x^{N}}{\\operatorname{argmax}} P\\left(y^{N} \\| x^{N}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0716bf-b3b4-4105-a52c-f481c30c9ab3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a0182-5713-4518-a700-b592d3069ffa",
   "metadata": {},
   "source": [
    "> ( **Fano's Inequality** )  $ H(X \\mid Y) \\leq 1 + P_e \\log (|\\mathcal{X}|-1) $.\n",
    ">\n",
    "> **Lemma I.** $\\forall  \\epsilon_{N}>0$, $\\exists N_{0}$,  s.t. $\\forall N>N_{0}$  we have $R \\leq \\bar{C}_{N}(\\Gamma)+\\epsilon_{N}$.\n",
    "\n",
    "Proof: Let a message  $m $ is chosen uniformly with probability  $2^{-N R}$. Observe that \n",
    "\n",
    "$$\\begin{aligned}\n",
    "N R & = H(M) = H\\left(M \\mid S_{0}\\right) \\leq I\\left(M ; Y^{N} \\mid S_{0}\\right)+ H\\left(M \\mid Y^{N}\\right) \\leq  I\\left(M ; Y^{N} \\mid S_{0}\\right)+1+P_{e}^{(N)} N R \\\\\n",
    "& = \\sum_{i=1}^{N} H\\left(Y_{i} \\mid Y^{i-1}, S_{0}\\right)-H\\left(Y_{i} \\mid Y^{i-1}, X^{i}, A_{e}^{i}, A_{d}^{i}, M, S_{0}\\right)+1+P_{e}^{(N)} N R \\\\\n",
    "& = \\sum_{i=1}^{N} H\\left(Y_{i} \\mid Y^{i-1}, S_{0}\\right)-H\\left(Y_{i} \\mid Y^{i-1}, X^{i}, S_{0}\\right)+1+P_{e}^{(N)} N R = \\sum_{i=1}^{N} I\\left(X^{i} ; Y_{i} \\mid Y^{i-1}, S_{0}\\right)+1+P_{e}^{(N)} N R \\\\\n",
    "& =I\\left(X^{N} \\rightarrow Y^{N} \\mid S_{0}\\right)+1+P_{e}^{(N)} N R \\leq \\max _{s_{0}} I\\left(X^{N} \\rightarrow Y^{N} \\mid s_{0}\\right)+1+P_{e}^{(N)} N R \\leq \\max \\max _{s_{0}} I\\left(X^{N} \\rightarrow Y^{N} \\mid s_{0}\\right)+1+P_{e}^{(N)} N R \\\\\n",
    "& =N \\bar{C}_{N}(\\Gamma)+1+P_{e}^{(N)} N R. \\end{aligned}$$\n",
    "\n",
    "\n",
    "> **Lemma II.** The capacity $ C = \\underline{C}_{N}$.\n",
    "\n",
    "**Proof.** Let a message  $m $ is chosen uniformly with probability  $2^{-N R}$. Observe that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "N R & = H(M) =  H\\left(M \\mid s_{0}\\right) = I\\left(M ; Y^{N} \\mid s_{0}\\right)+H\\left(M \\mid Y^{N}, s_{0}\\right) \\leq I\\left(M ; Y^{N} \\mid s_{0}\\right)+1+P_{e}^{(N)}\\left(s_{0}\\right) N R \\\\\n",
    "& = \\sum_{i=1}^{N} H\\left(Y_{i} \\mid Y^{i-1}, s_{0}\\right)-H\\left(Y_{i} \\mid Y^{i-1}, X^{i}, A_{e}^{i}, A_{d}^{i}, M, s_{0}\\right)+1+P_{e}^{(N)}\\left(s_{0}\\right) N R \\\\\n",
    "& = \\sum_{i=1}^{N} H\\left(Y_{i} \\mid Y^{i-1}, s_{0}\\right)-H\\left(Y_{i} \\mid Y^{i-1}, X^{i}, s_{0}\\right)+1+P_{e}^{(N)}\\left(s_{0}\\right) N R \\\\\n",
    "& =\\sum_{i=1}^{N} I\\left(X^{i} ; Y_{i} \\mid Y^{i-1}, s_{0}\\right)+1+P_{e}^{(N)}\\left(s_{0}\\right) N R = I\\left(X^{N} \\rightarrow Y^{N} \\mid s_{0}\\right)+1+P_{e}^{(N)} N R \\leq \\min _{s_{0}}\\left[I\\left(X^{N} \\rightarrow Y^{N} \\mid s_{0}\\right)+1+P_{e}^{(N)}\\left(s_{0}\\right) N R\\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "Since $P_{e}^{N}\\left(s_{0}\\right) \\rightarrow 0, \\forall s_{0} \\in \\mathcal{S}$, we arrive at the conclusion if we can show $\\underline{C}_{N}$ can be achieved.\n",
    "\n",
    "> The distribution $Q\\left(x^{N}, a_{e}^{N} \\| z^{N-1}\\right) Q\\left(a_{d}^{N} \\| y^{N-1}\\right)$ can achieve $\\underline{C}_{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3068476-0678-450b-b788-4699734cdbc3",
   "metadata": {},
   "source": [
    "It then suffices to show that\n",
    "\n",
    "> $$\\lim _{N \\rightarrow \\infty} \\underline{C}_{N}(\\Gamma)=\\sup _{N}\\left[\\underline{C}_{N}(\\Gamma)-\\frac{\\log |\\mathcal{S}|}{N}\\right] .$$\n",
    "\n",
    "Let  $N=n+l, n, l \\in \\mathbf{Z}^{+}$ and assume that  $\\underline{C}_{n}(\\Gamma)$  and  $\\underline{C}_{l}(\\Gamma)$  are achieved by  $Q\\left(x^{n}, a_{e}^{n} \\| z^{n-1}\\right) Q\\left(a_{d}^{n} \\| y^{n-1}\\right)$  and  $Q\\left(x^{l}, a_{e}^{l} \\| z^{l-1}\\right) Q\\left(a_{d}^{l} \\| y^{l-1}\\right)$  respectively. FOr $N$, we consider the probability assignment \n",
    "\n",
    "$$Q\\left(x^{n}, a_{e}^{n} \\| z^{n-1}\\right) Q\\left(a_{d}^{n} \\| y^{n-1}\\right) \\cdot Q\\left(x^{l}, a_{e}^{l} \\| z^{l-1}\\right) Q\\left(a_{d}^{l} \\| y^{l-1}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134600aa-0bb6-45b1-8fd2-b3f57400db15",
   "metadata": {},
   "source": [
    "Then we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "N \\underline{C}_{N}  & \\geq \\min _{s_{0}} I\\left(X^{N} \\rightarrow Y^{N} \\mid s_{0}\\right) \\geq \\min _{s_{0}} \\sum_{i=1}^{n} I\\left(Y_{i} ; X^{i} \\mid Y^{i-1}, s_{0}\\right)+\\min _{s_{0}} \\sum_{j=n+1}^{n+l} I\\left(Y_{j} ; X^{j} \\mid Y^{j-1}, s_{0}\\right) \\\\\n",
    "& \\geq n \\underline{C}_{n}+\\min _{s_{0}} \\sum_{j=n+1}^{n+l} I\\left(Y_{j} ; X_{n+1}^{j} \\mid Y^{j-1}, s_{0}\\right) \\\\\n",
    "& \\geq n \\underline{C}_{n}+\\min _{s_{0}} \\sum_{j=n+1}^{n+l} I\\left(Y_{j} ; X_{n+1}^{j} \\mid Y^{j-1}, S_{n}, s_{0}\\right)-\\log |\\mathcal{S}| \\\\\n",
    "& \\geq n \\underline{C}_{n}+\\min _{s_{0}} \\min _{s_{n}} \\sum_{j=n+1}^{n+l} I\\left(Y_{j} ; X_{n+1}^{j} \\mid Y^{j-1}, s_{n}, s_{0}\\right)-\\log |\\mathcal{S}| \\\\\n",
    "& = n \\underline{C}_{n}+\\min _{s_{n}} \\sum_{j=n+1}^{n+l} I\\left(Y_{j} ; X_{n+1}^{j} \\mid Y_{n+1}^{j-1}, s_{n}\\right)-\\log |\\mathcal{S}| = n \\underline{C}_{n}+l \\underline{C}_{l}-\\log |\\mathcal{S}| .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f979f91-3aeb-41dc-91bc-acc22dd831ea",
   "metadata": {},
   "source": [
    "By using the convergence of a super additive sequence, the theorem is proved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5331e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Robert G. Gallager. Discrete Stochastic Processes.\n",
    "2. Robert G. Gallager. Information Theory and Reliable Communication.\n",
    "3. Gerhard Kramer. Directed Information for Channels with Feedback.\n",
    "3. Haim Henry Permuter. Finite State Channels With Time-Invariant Deterministic Feedback.\n",
    "4. Haim Henry Permuter. To Feed or Not to Feed Back.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
