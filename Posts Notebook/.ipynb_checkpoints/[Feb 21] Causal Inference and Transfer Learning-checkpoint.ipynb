{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Feb 21] Causal Inference and Transfer Learning\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c723a98-16e1-4224-997f-2b80431b4d02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.  Causal Inference Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc02ff-28a1-42e6-8cb4-55691668c09b",
   "metadata": {},
   "source": [
    "For randomised experiments, the basic postulates follow.\n",
    "\n",
    "> The assignment mechanism for $n$ units is $\\mathbb{P}\\left(\\boldsymbol{A}_{[n]}=\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{X}_{[n]}=\\boldsymbol{x}_{[n]}\\right)=\\pi\\left(\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{x}_{[n]}\\right)$ with treatments conditional on covariates. Here follows some examples.\n",
    ">\n",
    "> ( **Bernoulli trial with covariate** ) $\\pi\\left(\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{x}_{[n]}\\right)=\\prod_{i=1}^{n} \\pi\\left(\\boldsymbol{x}_{i}\\right)^{a_{i}}\\left\\{1-\\pi\\left(\\boldsymbol{x}_{i}\\right)\\right\\}^{1-a_{i}}$\n",
    ">\n",
    "> ( **Sample without replacement** ) $\\pi\\left(\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{x}_{[n]}\\right)=\\left\\{\\begin{array}{ll}\n",
    "\\left(\\begin{array}{c}\n",
    "n \\\\\n",
    "n_{1}\n",
    "\\end{array}\\right)^{-1}, & \\text { if } \\sum_{i=1}^{n} a_{i}=n_{1} \\\\\n",
    "0, & \\text { otherwise. }\n",
    "\\end{array}\\right.$\n",
    ">\n",
    "> The **PO** model introduces: an observed factual (outcome) is linked with **counterfactuals (potential outcomes)** via $Y_{i}=Y_{i}\\left(\\boldsymbol{A}_{[n]}\\right) = \\sum \\mathbb{1}(A_n=a_n) Y_i(a_n)$, together with **assumption of no interference**,  i.e. $Y_{i}\\left(\\boldsymbol{a}_{[n]}\\right)=Y_{i}\\left(a_{i}\\right) \\text { for all } i \\in[n] \\text { and } \\boldsymbol{a}_{[n]} \\in \\mathcal{A}^{n}$. ( note the abuse of notation and therefore, $Y_{i} \\neq Y_i(A_i) $ generally )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d7d31-419e-4946-9219-ca4163cbde2b",
   "metadata": {},
   "source": [
    "The potential outcome framework allows as to view causal inference as a missing data\n",
    "problem, which consider two populations: $\\mathrm{SATE}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}(1)-Y_{i}(0)$ and $\\mathrm{PATE}=\\mathbb{E}\\left[Y_{i}(1)-Y_{i}(0)\\right]$. ( The latter implicitly assumes that the $n$ units are sampled from a superpopulation )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83068b-b640-4eee-82cb-f79530d43730",
   "metadata": {},
   "source": [
    "> **Assumption of randomisation.** $\\boldsymbol{A}_{[n]} \\perp \\boldsymbol{Y}_{[n]}\\left(\\boldsymbol{a}_{[n]}\\right) \\mid \\boldsymbol{X}_{[n]} \\text { for } \\boldsymbol{a}_{[n]} \\in \\mathcal{A}^{n}$.\n",
    "\n",
    "Note that **assumption of randomisation** is different from  $\\boldsymbol{A}_{[n]} \\perp \\boldsymbol{Y}_{[n]} \\mid \\boldsymbol{X}_{[n]}$, as  $Y_{i}=Y_{i}\\left(A_{i}\\right)$  generally depends on  $A_{i}$. \n",
    "\n",
    "> We are using  $\\boldsymbol{X}$, $A$, $Y$, and $Y(a)$  to refer to a generic  $\\boldsymbol{X}_{i}$, $A_{i}$, $Y_{i}$, and $Y_{i}(a)$ when they are iid.\n",
    ">\n",
    "> **Thm.** ( **Causal identification in randomised experiments** ) Consider any assignment mechanism where  $\\left\\{\\boldsymbol{X}_{i}, A_{i}, Y_{i}(a), a \\in \\mathcal{A}\\right\\}$  are iid. Suppose the above assumptions are given, then\n",
    ">\n",
    "> $$\\begin{aligned} \\mathbb{P}(A=a \\mid \\boldsymbol{X}=\\boldsymbol{x})>0 & \\implies  (Y(a) \\mid \\boldsymbol{X}=\\boldsymbol{x}) \\stackrel{d}{=}(Y \\mid A=a, \\boldsymbol{X}=\\boldsymbol{x}) \\\\\n",
    "& \\implies  A T E=\\mathbb{E}[Y(1)-Y(0)]=\\mathbb{E}\\{\\mathbb{E}[Y \\mid A=1, \\boldsymbol{X}]-\\mathbb{E}[Y \\mid A=0, \\boldsymbol{X}]\\}. \\end{aligned} $$\n",
    "\n",
    "**Proof.** For the first implication, computation shows that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}(Y(a) \\leq y \\mid \\boldsymbol{X}=\\boldsymbol{x}) & =\\mathbb{P}\\left(Y_{i}(a) \\leq y \\mid \\boldsymbol{X}=\\boldsymbol{x}, A=a\\right), \\\\\n",
    "& =\\mathbb{P}(Y_i(A) \\leq y \\mid \\boldsymbol{X}=\\boldsymbol{x}, A=a), \\\\\n",
    "& =\\mathbb{P}(Y \\leq y \\mid \\boldsymbol{X}=\\boldsymbol{x}, A=a),\n",
    "\\end{aligned}$$\n",
    "\n",
    "where the first equality uses assumption of randomisation.\n",
    "\n",
    "> As a special case, if $\\mathbb{P}(A=1 \\mid \\boldsymbol{X})$  does not depend on  $\\boldsymbol{X}$  ( i.e. $A \\perp \\boldsymbol{X}$ ), then $PATE=\\mathbb{E}[Y \\mid A=1]-\\mathbb{E}[Y \\mid A=0]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99ee8a-4389-4cb6-83cf-44fc79c84830",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Neyman considered the following difference-in-means estimator:\n",
    "\n",
    "$$\\hat{\\beta}=\\bar{Y}_{1}-\\bar{Y}_{0}, \\text { where } \\bar{Y}_{1}=\\frac{\\sum_{i=1}^{n} A_{i} Y_{i}}{\\sum_{i=1}^{n} A_{i}}, \\bar{Y}_{0}=\\frac{\\sum_{i=1}^{n}\\left(1-A_{i}\\right) Y_{i}}{\\sum_{i=1}^{n} 1-A_{i}} \\text {. }$$\n",
    "\n",
    "Denote  $\\boldsymbol{Y}(a)=\\left(Y_{1}(a), Y_{2}(a), \\ldots, Y_{n}(a)\\right)^{T}$. \n",
    "\n",
    "> Neyman studied the conditional distribution of  $\\hat{\\beta}$  given the potential outcomes  $\\boldsymbol{Y}(0), \\boldsymbol{Y}(1)$. We may refer to this as the **randomization distribution**, because the only randomness left in  $\\hat{\\beta}$  comes from the randomization of $\\boldsymbol{A}_{[n]}$. \n",
    "\n",
    "Set $\\bar{Y}(a)=\\sum_{i=1}^{n} Y_{i}(a) / n$. Suppose the treatment assignments $A_i$ are sampled without replacement, by using  $\\mathbb{E}\\left[A_{i}\\right]=n_{1} / n$, ( For simplicity of exposition, we omit the conditioning on  $\\boldsymbol{Y}(a)$ )\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}[\\hat{\\beta}] & =\\mathbb{E}\\left[\\frac{1}{n_{1}} \\sum_{i=1}^{n} A_{i} Y_{i}-\\frac{1}{n_{0}} \\sum_{i=1}^{n}\\left(1-A_{i}\\right) Y_{i}\\right] =\\mathbb{E}\\left[\\frac{1}{n_{1}} \\sum_{i=1}^{n} A_{i} Y_{i}(1)-\\frac{1}{n_{0}} \\sum_{i=1}^{n}\\left(1-A_{i}\\right) Y_{i}(0)\\right] \\\\\n",
    "& =\\frac{1}{n_{1}} \\sum_{i=1}^{n} \\frac{n_{1}}{n} Y_{i}(1)-\\frac{1}{n_{0}} \\sum_{i=1}^{n} \\frac{n_{0}}{n} Y_{i}(0) =\\bar{Y}(1)-\\bar{Y}(0) .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "> Suppose the treatment assignments $A_i$ are sampled without replacement, $\\mathbb{E}[\\hat{\\beta} \\mid \\boldsymbol{Y}(0), \\boldsymbol{Y}(1)]=S A T E$ and \n",
    "> $$\\operatorname{Var}(\\hat{\\beta} \\mid \\boldsymbol{Y}(0), \\boldsymbol{Y}(1))=\\frac{1}{n_{0}} S_{0}^{2}+\\frac{1}{n_{1}} S_{1}^{2}-\\frac{S_{01}^{2}}{n},$$\n",
    ">\n",
    "> where  $n_{0}=n-n_{1}$, $S_{a}^{2}=\\sum_{i=1}^{n}\\left(Y_{i}(a)-\\bar{Y}(a)\\right)^{2} /(n-1)$, and  $S_{01}^{2}=\\sum_{i=1}^{n}\\left(Y_{i}(1)-Y_{i}(0)-S A T E\\right)^{2} /(n-1)$.\n",
    ">\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d2b15-4f12-4e61-836e-9643e709f554",
   "metadata": {},
   "source": [
    "\n",
    "For the variance, we can show that $$\\operatorname{Var}(\\hat{\\beta} \\mid \\boldsymbol{Y}(0), \\boldsymbol{Y}(1))=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n} \\frac{A_{i}}{n_{1}} Y_{i}^{*}(1)-\\frac{1-A_{i}}{n_{0}} Y_{i}^{*}(0)\\right)^{2}\\right].$$\n",
    "\n",
    "where $Y_{i}^{*}(a)=Y_{i}(a)-\\bar{Y}(a)$. Expand the sum of squares and use\n",
    "\n",
    "$$\\mathbb{E}\\left[A_{i} A_{i^{\\prime}}\\right]=\\frac{n_{1}}{n} \\frac{n_{1}-1}{n-1}, \\mathbb{E}\\left[A_{i} (1-A_{i^{\\prime}})\\right]=\\frac{n_{1}}{n} \\frac{n_{0}}{n-1}, i \\neq i^{\\prime} \\text { and } \\sum_{i=1}^{n} Y_{i}^{*}(a)=0,$$\n",
    "\n",
    "then we arrives at the conclusion. One drawback of Neyman’s randomisation inference is that it is difficult to extend it to settings with covariates unless the covariates are discrete. The main obstacle is that the randomisation distribution necessarily depends on unobserved potential outcomes.\n",
    "\n",
    "> It is common to estimate the variance  by  $\\hat{S}_{0}^{2} / n_{0}+\\hat{S}_{1}^{2} / n_{1}$, where \n",
    ">\n",
    "> $$\\hat{S}_{1}^{2}=\\frac{1}{n_{1}-1} \\sum_{i=1}^{n} A_{i}\\left(Y_{i}-\\bar{Y}_{1}\\right)^{2}, \\hat{S}_{0}^{2}=\\frac{1}{n_{0}-1} \\sum_{i=1}^{n}\\left(1-A_{i}\\right)\\left(Y_{i}-\\bar{Y}_{0}\\right)^{2}.$$\n",
    ">\n",
    "> This is an unbiased estimator of $S_{0}^{2} / n_{0}+S_{1}^{2} / n_{1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409742a4-169f-4f14-bce6-24d7122a1fdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Fisher is the first to grasp fully the importance of randomisation. Consider $H_{0}: Y_{i}(1)-Y_{i}(0)=\\beta, \\forall i \\in[n]$. Using the consistency assumption, the hypothesis allow us to impute the potential outcomes as\n",
    "\n",
    "$$Y_{i}(a)=\\left\\{\\begin{array}{ll}\n",
    "Y_{i}, & \\text { if } a=A_{i} \\\\\n",
    "Y_{i}+\\beta, & \\text { if } a>A_{i} \\\\\n",
    "Y_{i}-\\beta, & \\text { if } a<A_{i}\n",
    "\\end{array}\\right.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f3ddf-2fe2-4a5b-a7f3-329b24b0204e",
   "metadata": {},
   "source": [
    "A more compact form is $\\boldsymbol{Y}_{[n]}\\left(\\boldsymbol{a}_{[n]}\\right)=\\boldsymbol{Y}_{[n]}+\\beta\\left(\\boldsymbol{a}_{[n]}-\\boldsymbol{A}_{[n]}\\right)$. The key step is to derive the **randomisation distribution** of $T = T\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}\\right)$. There are two ways to do this, which is shown below. $\\big($**In both cases, the randomness comes from the randomisation of $\\boldsymbol{A}_{[n]}$**$\\big)$\n",
    "\n",
    "> Consider the distribution of  $T_{1}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right)$  given  $\\boldsymbol{X}_{[n]}$  and  $\\boldsymbol{Y}_{[n]}(0)$;\n",
    ">\n",
    "> Consider the distribution of  $T_{2}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}\\left(\\boldsymbol{A}_{[n]}\\right)\\right)$  given  $\\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)$, and  $\\boldsymbol{Y}_{[n]}(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43a02b-8d76-4c0b-9685-984ed125180e",
   "metadata": {},
   "source": [
    "Let  $\\mathcal{F}=\\left(\\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0), \\boldsymbol{Y}_{[n]}(1)\\right)$. The randomisation distributions in the two approaches above are given by\n",
    "\n",
    "$$F_{1}(t)=\\mathbb{P}\\left(T_{1}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right) \\leq t \\mid \\mathcal{F}\\right) \\quad \\text{and} \\quad F_{2}(t)=\\mathbb{P}\\left(T_{2}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}\\left(\\boldsymbol{A}_{[n]}\\right)\\right) \\leq t \\mid \\mathcal{F}\\right). $$\n",
    "\n",
    "The observed test statistics are\n",
    "\n",
    "$$T_{1}=T_{1}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}-\\beta \\boldsymbol{A}_{[n]}\\right), T_{2}=T_{2}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}\\right) .$$\n",
    "\n",
    "The one-sided  $p$-value is the probability of observing the same or a more extreme test statistic than the observed statistic $T$, which is denoted by $P_{m}=F_{m}\\left(T_{m}\\right)$. An equivalent and perhaps more informative representation is\n",
    "\n",
    "$$P_{1}=\\mathbb{P}^{*}\\left(T_{1}\\left(\\boldsymbol{A}_{[n]}^{*}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right) \\leq T_{1} \\mid \\mathcal{F}\\right),$$\n",
    "\n",
    "where  $\\boldsymbol{A}_{[n]}^{*}$  is an independent copy of  $\\boldsymbol{A}$, so  $\\boldsymbol{A}_{[n]}^{*} \\mid \\boldsymbol{X}_{[n]} \\sim \\pi$  but  $\\boldsymbol{A}^{*} \\perp \\boldsymbol{A}$, and  $\\mathbb{P}^{*}$ is w.r.t.  $\\boldsymbol{A}^{*}$. The other  $p$-value  $P_{2}$  can be similarly defined. \n",
    "\n",
    "> A level- $\\alpha$  randomisation test then rejects  $H_{0}$  if  $P_{m} \\leq \\alpha$ .\n",
    "\n",
    "**Proof.** We know that \n",
    "\n",
    ">  If $F(t)$ is the distribution function of a random variable $T$, then $\\mathbb{P}(F(T) \\leq \\alpha)=\\mathbb{P}\\left(T<F^{-1}(\\alpha)\\right)=\\lim _{t \\uparrow F^{-1}(\\alpha)} \\mathbb{P}(T \\leq t) \\leq \\alpha$. Here $F^{-1}(\\alpha)=\\sup \\{t \\mid F(t) \\leq \\alpha\\}$. \n",
    "\n",
    "This shows that under assumption of randomness and $ H_{0}$, \n",
    "\n",
    "> $$\\mathbb{P}\\left(P_{m} \\leq \\alpha\\right) \\leq   \\alpha, \\forall 0<\\alpha<1, m=1,2.$$\n",
    "\n",
    "which enables us to do the level-$\\alpha$  randomisation test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad733f3-b857-4855-aae1-35f0f6b2673b",
   "metadata": {},
   "source": [
    "The randomisation assumption make the $p$-values possible to compute. To see this, by definition we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "F_{1}(t) & =\\mathbb{P}\\left(T_{1}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right) \\leq t \\mid \\mathcal{F}\\right) \\\\\n",
    "& =\\sum_{\\boldsymbol{a}_{[n]} \\in \\mathcal{A}^{n}} \\mathbb{P}\\left(\\boldsymbol{A}_{[n]}=\\boldsymbol{a}_{[n]} \\mid \\mathcal{F}\\right) \\cdot I\\left(T\\left(\\boldsymbol{a}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right) \\leq t\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Assumption of randomness and  $H_{0}$  allow us to replace the first term by\n",
    "\n",
    "$$\\mathbb{P}\\left(\\boldsymbol{A}_{[n]}=\\boldsymbol{a}_{[n]} \\mid \\mathcal{F}\\right)=\\mathbb{P}\\left(\\boldsymbol{A}_{[n]}=\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{X}_{[n]}\\right)=\\pi\\left(\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{X}_{[n]}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938cca83-0419-46c3-83bd-75600d2c1e0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8edb4-ba32-41d8-b7b4-144500ab5a82",
   "metadata": {},
   "source": [
    "Next we consider a different inference paradigm where the potential outcomes are drawn\n",
    "from a “super-population”.\n",
    "\n",
    "> Asymptotic super-population inference will be discusses by considering the **simple** Bernoulli trial (Example 2.1), so  $A_{i} \\perp \\boldsymbol{X}_{i}$. \n",
    ">\n",
    "> Further, suppose  $\\left(A_{i}, \\boldsymbol{X}_{i}, Y_{i}(0), Y_{i}(1)\\right)$  are i.i.d. and $\\mathbb{E}[\\boldsymbol{X}]=\\mathbf{0}$.\n",
    "\n",
    "Denote  $\\pi=\\mathbb{P}(A=1)$, $\\boldsymbol{\\Sigma}=\\mathbb{E}\\left[\\boldsymbol{X} \\boldsymbol{X}^{T}\\right]$, and  $\\beta=\\mathbb{E}[Y \\mid A=1]-\\mathbb{E}[Y \\mid A=0]$.\n",
    "\n",
    "> In a randomised experiment, $\\beta =$ PATE.\n",
    "\n",
    "We shall consider three regression estimators of  $\\beta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left(\\hat{\\alpha}_{1}, \\hat{\\beta}_{1}\\right) & =\\underset{\\alpha, \\beta}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\alpha-\\beta A_{i}\\right)^{2}, \\\\\n",
    "\\left(\\hat{\\alpha}_{2}, \\hat{\\beta}_{2}, \\hat{\\gamma}_{2}\\right) & =\\underset{(\\alpha, \\beta, \\boldsymbol{\\gamma})}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\alpha-\\beta A_{i}-\\boldsymbol{\\gamma}^{T} \\boldsymbol{X}_{i}\\right)^{2}, \\\\\n",
    "\\left(\\hat{\\alpha}_{3}, \\hat{\\beta}_{3}, \\hat{\\gamma}_{3}, \\hat{\\boldsymbol{\\delta}}_{3}\\right) & =\\underset{(\\alpha, \\beta, \\boldsymbol{\\gamma}, \\boldsymbol{\\delta})}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\alpha-\\beta A_{i}-\\boldsymbol{\\gamma}^{T} \\boldsymbol{X}_{i}-A_{i}\\left(\\boldsymbol{\\delta}^{T} \\boldsymbol{X}_{i}\\right)\\right)^{2} .\n",
    "\\end{aligned}$$\n",
    "\n",
    "Then write down the population version of the least squares problems:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left(\\alpha_{1}, \\beta_{1}\\right) & =\\underset{\\alpha, \\beta}{\\arg \\min } \\mathbb{E}\\left[(Y-\\alpha-\\beta A)^{2}\\right], \\\\\n",
    "\\left(\\alpha_{2}, \\beta_{2}, \\gamma_{2}\\right) & =\\underset{(\\alpha, \\beta, \\boldsymbol{\\gamma})}{\\arg \\min } \\mathbb{E}\\left[\\left(Y-\\alpha-\\beta A-\\boldsymbol{\\gamma}^{T} \\boldsymbol{X}\\right)^{2}\\right], \\\\\n",
    "\\left(\\alpha_{3}, \\beta_{3}, \\boldsymbol{\\gamma}_{3}, \\boldsymbol{\\delta}_{3}\\right) & =\\underset{(\\alpha \\beta, \\boldsymbol{\\gamma}, \\boldsymbol{\\delta})}{\\arg \\min } \\mathbb{E}\\left[\\left(Y-\\alpha-\\beta A-\\boldsymbol{\\gamma}^{T} \\boldsymbol{X}-A \\cdot\\left(\\boldsymbol{\\delta}^{T} \\boldsymbol{X}\\right)\\right)^{2}\\right] .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24338342-6341-4e4f-9f6a-8e00052b9164",
   "metadata": {},
   "source": [
    "> Lemma. Suppose  $\\left(\\boldsymbol{X}_{i}, A_{i}, Y_{i}\\right)$  are iid,  $A \\perp X$, $\\mathbb{E}[\\boldsymbol{X}]=0$. Then  $\\alpha_{1}=\\alpha_{2}=\\alpha_{3}$  and  $\\beta_{1}=\\beta_{2}=\\beta_{3}=\\beta$ .\n",
    "\n",
    "Proof. By taking partial derivatives, we obtain\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}\\left[Y-\\alpha_{3}-\\beta_{3} A-\\gamma_{3}^{T} \\boldsymbol{X}-A\\left(\\boldsymbol{\\delta}_{3}^{T} \\boldsymbol{X}\\right)\\right] & =0 \\\\\n",
    "\\mathbb{E}\\left[A\\left(Y-\\alpha_{3}-\\beta_{3} A-\\gamma_{3}^{T} \\boldsymbol{X}-A\\left(\\boldsymbol{\\delta}_3^{T} \\boldsymbol{X}\\right)\\right)\\right] & =0 .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Using  $\\mathbb{E}[\\boldsymbol{X}]=0$  and  $A \\perp \\boldsymbol{X}$ , they can be simplified to\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}\\left[Y-\\alpha_{3}-\\beta_{3} A\\right] & =0, \\\\\n",
    "\\mathbb{E}\\left[A\\left(Y-\\alpha_{3}-\\beta_{3} A\\right)\\right] & =0 .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Following the same derivation, these two equations also hold for the other estimators. By cancelling  $\\alpha_{3}$  in the equations, we get  $\\beta_{3}=\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb6f2c-260c-4c0f-8afd-be3619b30e38",
   "metadata": {},
   "source": [
    "Suppose $\\hat{\\boldsymbol{\\theta}}$  is an empirical solution to the equation\n",
    "\n",
    "$$\\mathbb{E}[\\boldsymbol{\\psi}(\\boldsymbol{\\theta} ; \\boldsymbol{Z}, Y)]=\\mathbf{0},$$\n",
    "\n",
    "where $\\boldsymbol{\\psi}(\\boldsymbol{\\theta} ; \\boldsymbol{Z}, Y)=\\boldsymbol{Z} \\cdot\\left(Y-\\boldsymbol{Z}^{T} \\boldsymbol{\\theta}\\right)=\\boldsymbol{Z} \\epsilon$. Suppose $Y$ and $Z$\n",
    "have bounded fourth moments, the $Z$-estimation theory shows that\n",
    "\n",
    "$$\\begin{aligned} & \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}) \\xrightarrow{d} \\mathrm{~N}\\left(\\mathbf{0},\\left\\{\\mathbb{E}\\left[\\frac{\\partial \\boldsymbol{\\psi}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\\right]\\right\\}^{-1} \\mathbb{E}\\left[\\boldsymbol{\\psi}(\\boldsymbol{\\theta}) \\boldsymbol{\\psi}(\\boldsymbol{\\theta})^{T}\\right]\\left\\{\\mathbb{E}\\left[\\frac{\\partial \\boldsymbol{\\psi}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\\right]\\right\\}^{-1}\\right) \\\\\n",
    "\\implies  & \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}) \\xrightarrow{d} \\mathrm{~N}\\left(0,\\left\\{\\mathbb{E}\\left[\\boldsymbol{Z} \\boldsymbol{Z}^{T}\\right]\\right\\}^{-1} \\mathbb{E}\\left[\\boldsymbol{Z} \\boldsymbol{Z}^{T} \\epsilon^{2}\\right]\\left\\{\\mathbb{E}\\left[\\boldsymbol{Z} \\boldsymbol{Z}^{T}\\right]\\right\\}^{-1}\\right). \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d65fe-5ba4-4aa5-8f56-62b3b7006434",
   "metadata": {},
   "source": [
    "The asymptotic normality follows from the argument below. Using Taylor’s expansion,\n",
    "$$ \\begin{aligned}\n",
    "0 & =\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{\\psi}\\left(\\hat{\\boldsymbol{\\theta}} ; \\boldsymbol{Z}_{i}, Y_{i}\\right) \\\\\n",
    "& =\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{\\psi}\\left(\\boldsymbol{\\theta} ; \\boldsymbol{Z}_{i}, Y_{i}\\right)+(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta})^{T}\\left[\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\boldsymbol{\\psi}\\left(\\boldsymbol{\\theta} ; \\boldsymbol{Z}_{i}, Y_{i}\\right)\\right]+R_{n}\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5fd07-4e02-46ed-96a0-ddc4be7ab9dd",
   "metadata": {},
   "source": [
    "By using $\\hat{\\boldsymbol{\\theta}} \\xrightarrow{p} \\boldsymbol{\\theta}$, it can be shown that $R_{n}$  is asymptotically smaller than the other two terms and can be ignored. Thus\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}) \\approx \\left[\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\boldsymbol{\\psi}(\\boldsymbol{\\theta} ; \\boldsymbol{Z}, Y)\\right]^{-1}\\left[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\boldsymbol{\\psi}\\left(\\boldsymbol{\\theta} ; \\boldsymbol{Z}_{i}, Y_{i}\\right)\\right].$$\n",
    "\n",
    "The first term on the right hand side converges in probability to  $\\mathbb{E}[\\partial \\boldsymbol{\\psi}(\\boldsymbol{\\theta}) / \\partial \\boldsymbol{\\theta}]^{-1}$. The second term converges in distribution to a normal random variable with variance  $\\mathbb{E}\\left[\\boldsymbol{\\psi}(\\boldsymbol{\\theta}) \\boldsymbol{\\psi}(\\boldsymbol{\\theta})^{T}\\right]$. Using Slutsky's theorem, we arrive at the conclusion.\n",
    "\n",
    "> Let  $\\epsilon_{i 1}, \\epsilon_{i 2}, \\epsilon_{i 3}$  be the error terms in the three regression estimators:\n",
    ">\n",
    "> $$\\epsilon_{i m}=Y_{i}-\\alpha_{m}-\\beta_{m} A_{i}-\\boldsymbol{\\gamma}_{m}^{T} \\boldsymbol{X}_{i}-A_{i}\\left(\\boldsymbol{\\delta}_{m}^{T} \\boldsymbol{X}_{i}\\right), m=1,2,3 .$$\n",
    ">\n",
    "> Conventionally  $\\gamma_{1}=0$  and  $\\boldsymbol{\\delta}_{1}=\\boldsymbol{\\delta}_{2}=\\mathbf{0}$. Then \n",
    ">\n",
    "> $$\\sqrt{n}\\left(\\hat{\\beta}_{m}-\\beta\\right) \\xrightarrow{d} \\mathrm{~N}\\left(0, V_{m}\\right).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a356f62-ef80-4c9e-ad3f-d2c0111e7aaf",
   "metadata": {},
   "source": [
    "\n",
    "> (**Decomposition**) Causal estimator - True causal effect  =  Design bias + Modelling bias + Statistical noise.\n",
    "\n",
    "Concrete, let  $\\boldsymbol{O}$  be all the observed variables with distribution $\\mathcal{O}$; $\\boldsymbol{F}$  (full data) denote the relevant factuals and counterfactuals with  $\\mathcal{F}$  be its distribution.\n",
    "Then the decomposition really is\n",
    "\n",
    "$$\\beta\\left(\\boldsymbol{O}_{[n]} ; \\hat{\\theta}\\right)-\\beta(\\mathcal{F})=\\{\\beta(\\mathcal{O})-\\beta(\\mathcal{F})\\}+\\{\\beta(\\mathcal{O} ; \\theta)-\\beta(\\mathcal{O})\\}+\\left\\{\\beta\\left(\\boldsymbol{O}_{[n]} ; \\hat{\\theta}\\right)-\\beta(\\mathcal{O} ; \\theta)\\right\\}$$\n",
    "\n",
    "where  $\\beta$  is a generic symbol for causal effect functional (estimator),  $\\boldsymbol{O}_{[n]}$  is the observed data of size  $n$, $\\theta$  is the parameter in a statistical model and  $\\hat{\\theta}=\\hat{\\theta}\\left(\\boldsymbol{O}_{[n]}\\right)$  is an estimator of  $\\theta$.\n",
    "\n",
    "> **Example.** In regression adjustment for randomised experiments,  $\\boldsymbol{O}=(\\boldsymbol{X}, A, Y)$ ,  $\\boldsymbol{F}=(Y(0), Y(1))$, $\\beta(\\mathcal{F})=\\mathbb{E}[Y(1)-Y(0)]$, $\\beta(\\mathcal{O})=\\mathbb{E}[Y \\mid A=1]-\\mathbb{E}[Y \\mid A=0]$, $\\beta(\\mathcal{O}, \\theta)$  be the any of (population version) regression estimators,  $\\beta\\left(\\boldsymbol{O}_{[n]}; \\hat{\\theta}\\right)$  be the corresponding regression estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7293f0-7474-4447-b975-7bb06b874a21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Graphic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93fb44-29e3-4fa0-9a93-2b694f40ee2f",
   "metadata": {},
   "source": [
    "> Given a DAG  $\\mathcal{G}=(V=[p], E)$ (**DAG implies a topological ordering**), the random variables  $\\boldsymbol{X}=\\boldsymbol{X}_{[p]}$  satisfy a NPSEM if the observed and interventional distributions of  $\\boldsymbol{X}_{[p]}$  satisfy\n",
    ">\n",
    "> $$X_{i}=f_{i}\\left(\\boldsymbol{X}_{p a_{\\mathcal{G}}(i)}, \\epsilon_{i}\\right), i=1, \\ldots, p,$$\n",
    "> for some functions  $f_{1}, \\ldots, f_{p}$  and random variables  $\\boldsymbol{\\epsilon}_{[p]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5e1b3-5831-4c14-9926-976b7f7bfdfd",
   "metadata": {},
   "source": [
    "Given the above NPSEM, the counterfactual variables  $X_{i}(\\boldsymbol{X}_{J}=\\boldsymbol{x}_{J})$  can be obtained via recursive substitution: For any  $i \\in[p]$, $J \\subseteq[p]$  and  $J \\neq p a(i)$ , we recursively set\n",
    "\n",
    "$$ X_{i}\\left(\\boldsymbol{x}_{J}\\right) = X_{i}\\left(\\boldsymbol{X}_{J}=\\boldsymbol{x}_{J}\\right)=X_{i}\\left(\\boldsymbol{X}_{p a(i) \\cap J}=\\boldsymbol{x}_{p a(i) \\cap J}, \\boldsymbol{X}_{p a(i) \\backslash J}=\\boldsymbol{X}_{p a(i) \\backslash J}\\left(\\boldsymbol{x}_{J}\\right)\\right).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb569e-69f0-436f-a93b-48895ec7f121",
   "metadata": {},
   "source": [
    "Recall that\n",
    "\n",
    "> Given a DAG  $\\mathcal{G}$, a path is blocked by  $K \\subseteq V$  if $\\exists k$  on the path such that either $k$  is not a collider on this path and  $k \\in K$; or $k$  is a collider on this path and  $k$  and all its descendants are not in  $K$.\n",
    ">\n",
    ">  For $\\mathbb{P}$ on $\\mathcal{G}$,  $f(\\boldsymbol{x})=\\prod_{i \\in V} f_{i \\mid p a(i)}\\left(x_{i} \\mid \\boldsymbol{x}_{p a(i)}\\right)$ iff $I \\perp J\\left|K[\\mathcal{G}] \\Longrightarrow \\boldsymbol{X}_{I} \\perp \\boldsymbol{X}_{J}\\right| \\boldsymbol{X}_{K}$ for disjoint $I,J,K$.\n",
    "\n",
    "We provide further that\n",
    "\n",
    "> Given any disjoint  $J, K \\subseteq V$  and any  $i \\in V$. \n",
    ">\n",
    "> **Prop I.** If  $K$  blocks all directed paths from  $J$  to  $i$, then $\\boldsymbol{X}_{i}\\left(\\boldsymbol{x}_{J}, \\boldsymbol{x}_{K}\\right)=\\boldsymbol{X}_{i}\\left(\\boldsymbol{x}_{K}\\right)$. \n",
    ">\n",
    "> **Cor I.** $X_{i}\\left(\\boldsymbol{x}_{J}\\right)=X_{i}\\left(\\boldsymbol{x}_{J \\cap a n(i)} \\right)$.\n",
    "> \n",
    "> **Prop II.** $\\boldsymbol{X}_{J}\\left(\\boldsymbol{x}_{K}\\right)=\\boldsymbol{x}_{J} \\Longrightarrow X_{i}\\left(\\boldsymbol{x}_{J}, \\boldsymbol{x}_{K}\\right)=X_{i}\\left(\\boldsymbol{x}_{K}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9a9a2-c2d2-432d-ae76-4c371d0f0ccd",
   "metadata": {},
   "source": [
    "**Proof.** The first follows from the observation: if  $K$  blocks all directed paths from  $J$  to  $i$, then  $K$  also blocks directed paths from  $J$  to  $p a(i) \\backslash K$. The second follows from the following equations and induction\n",
    "\n",
    "$$\\begin{aligned}\n",
    "X_{i}\\left(\\boldsymbol{x}_{J}, \\boldsymbol{x}_{K}\\right)=X_{i}\\left(\\boldsymbol{x}_{p a(i) \\cap J}, \\boldsymbol{x}_{p a(i) \\cap K}, \\boldsymbol{X}_{p a(i) \\backslash J \\backslash K}\\left(\\boldsymbol{x}_{J}, \\boldsymbol{x}_{K}\\right)\\right), \\\\\n",
    "X_{i}\\left(\\boldsymbol{x}_{K}\\right)=X_{i}\\left(\\boldsymbol{X}_{p a(i) \\cap J}\\left(\\boldsymbol{x}_{K}\\right), \\boldsymbol{x}_{p a(i) \\cap K}, \\boldsymbol{X}_{p a(i) \\backslash J \\backslash K}\\left(\\boldsymbol{x}_{K}\\right)\\right) .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fe874-d96a-4c50-abaf-c5cda3380bd4",
   "metadata": {},
   "source": [
    "> A NPSEM satisfies the single(multiple)-world independence assumptions, if $X_{i}\\left(\\boldsymbol{x}_{p a(i)}\\right)$ $\\big(\\epsilon_{i}\\big)$ are mutually independent.\n",
    ">\n",
    "> A **causal model** is a NPSEM together with the single-world independence assumption.\n",
    "\n",
    "Note that in addition to the single-world independence assumptions, the multiple-world independence assumptions also make the following cross-world independence assumption:\n",
    "$$X_{2}\\left(x_{1}\\right) \\perp X_{3}\\left(\\tilde{x}_{1}, x_{2}\\right) \\text { for any } x_{1} \\neq \\tilde{x}_{1}, x_{2} .$$\n",
    "\n",
    "> The single-world intervention graph (SWIG) $\\mathcal{G}\\left[\\boldsymbol{x}_{J}\\right]$ for the intervention  $\\boldsymbol{X}_{J}=\\boldsymbol{x}_{J}$  is constructed from  $\\mathcal{G}$  via the following two steps:\n",
    ">\n",
    "> (i) Node splitting: For every  $j \\in J$, split the vertex  $X_{j}$  into a random and a fixed component, labelled $X_{j}$ and $x_{j}$  respectively. The random half inherited all edges into  $X_{j} $ and the fixed half inherited all edges out of  $X_{j}$.\n",
    ">\n",
    "> (ii) Labelling: For every random node  $X_{i}$  in the new graph, label it with  $X_{i}\\left(\\boldsymbol{x}_{J}\\right)=   X_{i}\\left(\\boldsymbol{x}_{J \\cap a n(i)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d231ea04-b8e2-4941-8ea1-f0c770f59646",
   "metadata": {},
   "source": [
    "> ( **Factorisation of counterfactual distributions** ). Suppose  $\\boldsymbol{X}$  satisfies the causal model defined by a DAG  $\\mathcal{G}$, then  $\\boldsymbol{X}\\left(\\boldsymbol{x}_{J}\\right)$  factorises according to  the random part $\\mathcal{G}^{*}\\left[\\boldsymbol{X}\\left(\\boldsymbol{x}_{J}\\right)\\right]$ (by removing $x_J$ from $\\mathcal{G}\\left[\\boldsymbol{X}\\left(\\boldsymbol{x}_{J}\\right)\\right]$), $\\forall  J \\subseteq[p]$.\n",
    "\n",
    "**Proof.** It's clear that \n",
    "\n",
    "> For any  $k \\notin J$ s.t. $\\operatorname{de}(k) \\subseteq J$,\n",
    ">\n",
    "> $$\\mathbb{P}\\left(X_{i}\\left(\\boldsymbol{x}_{J}, \\tilde{x}_{k}\\right)=\\tilde{x}_{i} \\mid \\boldsymbol{X}_{p a(i) \\backslash J \\backslash\\{k\\}}\\left(\\boldsymbol{x}_{J}, \\tilde{x}_{k}\\right)=\\tilde{\\boldsymbol{x}}_{p a(i) \\backslash J \\backslash\\{k\\}}\\right) = \\mathbb{P}\\left(X_{i}\\left(\\boldsymbol{x}_{J}\\right)=\\tilde{x}_{i} \\mid \\boldsymbol{X}_{p a(i) \\backslash J}\\left(\\boldsymbol{x}_{J}\\right)=\\tilde{\\boldsymbol{x}}_{p a(i) \\backslash J}\\right) .$$\n",
    "\n",
    "Then we simply prove by reverse induction using $J \\cup\\{k\\} \\subseteq[p] \\text { to } J \\text { where } k \\notin J \\text { and } d e(k) \\subseteq J$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3a20a-0c5a-4c68-942e-28fb6bda93ee",
   "metadata": {},
   "source": [
    "Suppose $\\boldsymbol{X}$ satisfies the causal model, then\n",
    "\n",
    "$$ \\mathbb{P}\\left(X_{i}\\left(\\boldsymbol{x}_{J}\\right)=\\tilde{x}_{i} \\mid \\boldsymbol{X}_{p a(i) \\backslash J}\\left(\\boldsymbol{x}_{J}\\right)=\\tilde{\\boldsymbol{x}}_{p a(i) \\backslash J}\\right) = \\mathbb{P}\\left(X_{i}=\\tilde{x}_{i} \\mid \\boldsymbol{X}_{p a(i) \\backslash J}=\\tilde{\\boldsymbol{x}}_{p a(i) \\backslash J}, \\boldsymbol{X}_{p a(i) \\cap J}=\\boldsymbol{x}_{p a(i) \\cap J}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce86e4d-55b3-4838-8fb3-909831323575",
   "metadata": {},
   "source": [
    "This shows that \n",
    "\n",
    "> $\\mathbb{P}\\left(\\boldsymbol{X}\\left(\\boldsymbol{x}_{J}\\right)=\\tilde{\\boldsymbol{x}}\\right)=\\prod_{i=1}^{p} \\mathbb{P}\\left(X_{i}=\\tilde{x}_{i} \\mid \\boldsymbol{X}_{p a(i) \\cap J}=\\boldsymbol{x}_{p a(i) \\cap J}, \\boldsymbol{X}_{p a(i) \\backslash J}=\\tilde{\\boldsymbol{x}}_{p a(i) \\backslash J}\\right)$.\n",
    ">\n",
    "> $\\mathbb{P}\\left(\\boldsymbol{X}_{I}\\left(\\boldsymbol{x}_{J}\\right)=\\tilde{\\boldsymbol{x}}_{I}\\right)=\\sum_{\\tilde{\\boldsymbol{x}}_{K}} \\prod_{i \\in I \\cup K} \\mathbb{P}\\left(X_{i}=\\tilde{x}_{i} \\mid \\boldsymbol{X}_{p a(i) \\cap J}=\\boldsymbol{x}_{p a(i) \\cap J}, \\boldsymbol{X}_{p a(i) \\backslash J}=\\tilde{\\boldsymbol{x}}_{p a(i) \\backslash J}\\right)$ where $I,J,K$ form a partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155f245-0748-46e2-8ae3-27faa40fbc9d",
   "metadata": {},
   "source": [
    "**Proof.** The second is a corollary of the first equation by seeing that \n",
    "\n",
    "$$\\mathbb{P}\\left(\\boldsymbol{X}_{I}\\left(\\boldsymbol{x}_{J}\\right)=\\tilde{\\boldsymbol{x}}_{I}\\right)=\\sum_{\\tilde{\\boldsymbol{x}}_{K}, \\tilde{\\boldsymbol{x}}_{J}} \\prod_{i=1}^{p} \\mathbb{P}\\left(X_{i}=\\tilde{x}_{i} \\mid \\boldsymbol{X}_{p a(i) \\cap J}=\\boldsymbol{x}_{p a(i) \\cap J}, \\boldsymbol{X}_{p a(i) \\backslash J}=\\tilde{\\boldsymbol{x}}_{p a(i) \\backslash J}\\right) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b48f28-b85d-4315-a1a4-a90cbcdd280f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02142995-2b28-4217-9cd0-705cf1dcf8ee",
   "metadata": {},
   "source": [
    "In the graphical framework, we can check  $Y(a) \\perp A \\mid X$  by d-separation in the SWIG. Because there is no out-going arrow from  $A$  in  $\\mathcal{G}^{*}[a]$, this essentially says that every back-door path from  $A$  to  $Y$  (i.e. the path has an edge going into  $A$) must be blocked by  $X$.\n",
    "\n",
    "> ( **Back-door adjustment** ). Suppose $(\\boldsymbol{X}, A, Y)$  in a causal model  $\\mathcal{G}$  that may contain other unobserved variables. Suppose  $\\boldsymbol{X}$  contains no descendant of  $A$  and blocks every back-door path from  $A$  to  $Y$  in  $\\mathcal{G}$. Then  $Y(a) \\perp A \\mid \\boldsymbol{X}$   and\n",
    ">\n",
    "> $$\\mathbb{P}(Y(a) \\leq y)=\\sum_{\\boldsymbol{x}} \\mathbb{P}(\\boldsymbol{X}=\\boldsymbol{x}) \\cdot \\mathbb{P}(Y \\leq y \\mid A=a, \\boldsymbol{X}=\\boldsymbol{x}), \\forall a, \\boldsymbol{x}, y.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc23e2d2-eb7f-4db3-b346-94a1bd212ace",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. No Unmeasured Confounders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55cd1b-b807-4bcd-846d-7e24d74cb3f5",
   "metadata": {},
   "source": [
    "An observational study is an empirical investigation that utilises observation data (without\n",
    "manipulation or intervention), which envolves two stages: **design** and **analysis**. \n",
    "\n",
    "> ( **Randomised Experiment** ) Randomisation allows us to choose between statistical\n",
    "error and causality. This reasoning is inductive.\n",
    ">\n",
    "> ( **Observational Study** ) Randomisation is replaced by pair matching. As a consequence, apart\n",
    "from statistical error and causality, a third possible **explanation** is that the treated patients\n",
    "and the control patients are systematically different in some other way.\n",
    ">\n",
    "> Assume  $\\left(\\boldsymbol{X}_{i}, A_{i}, Y_{i}(0), Y_{i}(1)\\right)$,  $i=1, \\ldots, n$, are i.i.d. \n",
    "> \n",
    "> **Assumption of no unmeasured confounders.** $ A \\perp Y(a) \\mid \\boldsymbol{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f2c5c-eeb4-44c2-a77a-0c9787218b95",
   "metadata": {},
   "source": [
    "For matching usually we apply **propensity score matching**. We say $b(\\boldsymbol{X})$  is a **balancing score** if  $A \\perp \\boldsymbol{X} \\mid b(\\boldsymbol{X})$, which follows that\n",
    "\n",
    "$$ A \\perp Y(a) \\mid b(\\boldsymbol{X}).$$\n",
    "\n",
    "\n",
    "Among all the balancing scores, of particular interest is the propensity score $\\pi(\\boldsymbol{x})=\\mathbb{P}(A=1 \\mid \\boldsymbol{X}=\\boldsymbol{x})$, which can be written as  a function of any balancing score $b(\\boldsymbol{X})$. And a popular distance measure is the squared distance between the estimated propensity scores in the logit scale:\n",
    "$$d_{\\mathrm{PS}}\\left(\\boldsymbol{X}_{i}, \\boldsymbol{X}_{j}\\right)=\\left[\\log \\left(\\frac{\\hat{\\pi}\\left(\\boldsymbol{X}_{i}\\right)}{1-\\hat{\\pi}\\left(\\boldsymbol{X}_{i}\\right)}\\right)-\\log \\left(\\frac{\\hat{\\pi}\\left(\\boldsymbol{X}_{j}\\right)}{1-\\hat{\\pi}\\left(\\boldsymbol{X}_{j}\\right)}\\right)\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef7865-c297-480d-8fb2-4bda7eb066b8",
   "metadata": {},
   "source": [
    "Next we consider the statistical inference after matching.\n",
    "\n",
    "> Assume treated observation  $i$  is matched to control observation  $i+n_{1}$, $i\\in [n_{1}]$ . \n",
    "\n",
    "Let $D_{i}=\\left(A_{i}-A_{i+n_{1}}\\right)\\left(Y_{i}-Y_{i+n_{1}}\\right)$ be the treated-minus-control difference in pair  $i$. Let \n",
    "\n",
    "$$M=\\left\\{\\boldsymbol{a}_{\\left[2 n_{1}\\right]} \\in\\{0,1\\}^{2 n_{1}} \\mid a_{i}+a_{i+n_{1}}=1, \\forall i \\in\\left[n_{1}\\right]\\right\\}$$\n",
    "\n",
    "be all the reasonable treatment assignments. Let  $\\boldsymbol{C}_{i}=\\left(\\boldsymbol{X}_{i}, Y_{i}(0), Y_{i}(1)\\right)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7d285-7876-4115-b743-9f504fa536d8",
   "metadata": {},
   "source": [
    "> $$\\pi\\left(\\boldsymbol{X}_{i}\\right)=\\pi\\left(\\boldsymbol{X}_{i+n_{1}}\\right) \\implies \\mathbb{P}\\left(\\boldsymbol{A}_{\\left[2 n_{1}\\right]}=\\boldsymbol{a} \\mid \\boldsymbol{C}_{\\left[2 n_{1}\\right]}, \\boldsymbol{A}_{\\left[2 n_{1}\\right]} \\in M\\right)=\\left\\{\\begin{array}{ll}\n",
    "2^{-n_{1}}, & \\text { if } \\boldsymbol{a} \\in M \\\\\n",
    "0, & \\text { otherwise }\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca561de-3405-4623-be79-bb43b423ada6",
   "metadata": {},
   "source": [
    "This means that $\\pi\\left(\\boldsymbol{X}_{i}\\right)=\\pi\\left(\\boldsymbol{X}_{i+n_{1}}\\right)$ implies the **assumption that matching reconstructs a pairwise randomised experiment**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9feec-31e9-463c-8b31-0ad4baf74d9c",
   "metadata": {},
   "source": [
    "Consider $H_{\\beta}: Y_{i}(1)-Y_{i}(0)=\\beta, \\forall i$. Under  $H_{0}$, the counterfactual values of  $\\boldsymbol{D}_{\\left[n_{1}\\right]}$  can be imputed as\n",
    "\n",
    "$$D_{i}\\left(\\boldsymbol{a}_{\\left[2 n_{1}\\right]}\\right)=\\left(a_{i}-a_{i+n_{1}}\\right) \\cdot\\left(Y_{i}\\left(a_{i}\\right)-Y_{i+n_{1}}\\left(a_{i+n_{1}}\\right)\\right)=\\left\\{\\begin{array}{ll}\n",
    "D_{i}, & \\text { if } a_{i}=1, a_{i+n_{1}}=0 \\\\\n",
    "2 \\beta-D_{i}, & \\text { if } a_{i}=0, a_{i+n_{1}}=1\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b966e2a-74ec-4469-bad9-a863769f7c3d",
   "metadata": {},
   "source": [
    "Consider any test statistic  $T=T\\left(\\boldsymbol{D}_{\\left[n_{1}\\right]}\\right)$. Next we construct a randomisation test based on the randomisation distribution of  $T\\left(\\boldsymbol{D}_{\\left[n_{1}\\right]}\\left(\\boldsymbol{A}_{\\left[2 n_{1}\\right]}\\right)\\right)$. Let  $F(t)$  denote its cumulative distribution function given  $\\boldsymbol{C}_{\\left[2 n_{1}\\right]}$  and  $\\boldsymbol{A}_{\\left[2 n_{1}\\right]} \\in M$  under  $H_{0}$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "F\\left(t ; \\boldsymbol{D}_{\\left[n_{1}\\right]}, \\beta\\right) & =\\mathbb{P}\\left(T \\leq t \\mid \\boldsymbol{C}_{\\left[n_{1}\\right]}, \\boldsymbol{A}_{\\left[2 n_{1}\\right]} \\in M, H_{\\beta}\\right), \\\\\n",
    "& =\\sum_{\\boldsymbol{a}_{\\left[2 n_{1}\\right]} \\in M}\\left(\\frac{1}{2}\\right)^{n_{1}} \\cdot I\\left(T\\left(\\boldsymbol{D}_{\\left[n_{1}\\right]}\\left(\\boldsymbol{a}_{\\left[2 n_{1}\\right]}\\right)\\right) \\leq t\\right) .\n",
    "\\end{aligned}$$\n",
    "\n",
    "> Under the assumption that matching reconstructs a pairwise randomised experiment,  $\\mathbb{P}\\left(P_{2} \\leq \\alpha\\right) \\leq \\alpha$  under  $H_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5ce14-640c-4b7c-be38-50249e60d704",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5240b8-2324-4828-bd3d-e73e3d9cbbf0",
   "metadata": {},
   "source": [
    "Next we apply some semiparametric inference. \n",
    "\n",
    "> **Assumption of Positivity.** $\\pi_{a}(\\boldsymbol{x})=\\mathbb{P}(A=a \\mid \\boldsymbol{X}=\\boldsymbol{x})>0$, $\\forall a, \\boldsymbol{x}$.\n",
    ">\n",
    "> This is also called the overlap assumption, because by the Bayes rule, it is equivalent to assuming that the distribution  $\\boldsymbol{X}$  has the same support given  $A=a$  for all  $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d992a-bbf2-4983-9fdf-d33a37c186f0",
   "metadata": {},
   "source": [
    "We have $\\mathrm{ATE}=\\mathbb{E}[Y(1)-Y(0)]=\\mathbb{E}\\{\\mathbb{E}[Y \\mid A=1, \\boldsymbol{X}]-\\mathbb{E}[Y \\mid A=0, \\boldsymbol{X}]\\}$. Then it suffices to estimate \n",
    "\n",
    "$$\\beta_{a}=\\mathbb{E}\\{\\mathbb{E}[Y \\mid A=a, \\boldsymbol{X}]\\}=\\sum_{\\boldsymbol{x}} \\mu_{a}(\\boldsymbol{x}) \\mathbb{P}(\\boldsymbol{X}=\\boldsymbol{x}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad306574-6bf1-4554-89bd-e4b4d8747211",
   "metadata": {},
   "source": [
    "Given an iid sample from the population, we can empirically estimate by\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hat{\\mu}_{a}(\\boldsymbol{x}) & = \\frac{\\sum_{i=1}^{n} I\\left(A_{i}=a, \\boldsymbol{X}_{i}=\\boldsymbol{x}\\right) Y_{i}}{\\sum_{i=1}^{n} I\\left(A_{i}=a, \\boldsymbol{X}_{i}=\\boldsymbol{x}\\right)}, \\\\\n",
    "\\hat{\\mathbb{P}}(\\boldsymbol{X}=\\boldsymbol{x}) & = \\frac{1}{n} \\sum_{i=1}^{n} I\\left(\\boldsymbol{X}_{i}=\\boldsymbol{x}\\right)\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34627adc-4784-4e1b-b695-76aaaef1263f",
   "metadata": {},
   "source": [
    "Therefore, we obtain the OR estimator $$\\hat{\\beta}_{a, \\mathrm{OR}}=\\sum_{\\boldsymbol{x}} \\hat{\\mu}_{a}(\\boldsymbol{x}) \\hat{\\mathbb{P}}\\left(\\boldsymbol{X}_{i}=\\boldsymbol{x}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{\\boldsymbol{x}} \\hat{\\mu}_{a}(\\boldsymbol{x}) I\\left(\\boldsymbol{X}_{i}=\\boldsymbol{x}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\hat{\\mu}_{a}\\left(\\boldsymbol{X}_{i}\\right) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f396ac2-ae14-4d65-a022-7f3017b0ca28",
   "metadata": {},
   "source": [
    "> The ATE can be estimated by $\\hat{\\beta}_{\\mathrm{OR}}=\\hat{\\beta}_{1, \\mathrm{OR}}-\\hat{\\beta}_{0, \\mathrm{OR}}=\\frac{1}{n} \\left(\\sum_{i=1}^{n} \\hat{\\mu}_{1}(\\boldsymbol{X}_{i}\\right)-\\hat{\\mu}_{0}\\left(\\boldsymbol{X}_{i})\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96196ea0-fbc3-4421-8430-b950bc086f61",
   "metadata": {},
   "source": [
    "Recall  $\\pi_{a}(\\boldsymbol{x})=\\mathbb{P}(A=a \\mid \\boldsymbol{X}=\\boldsymbol{x})$, which can be estimated by\n",
    "\n",
    "$$\\hat{\\pi}_{a}(\\boldsymbol{x})=\\frac{\\sum_{i=1}^{n} I\\left(A_{i}=a, \\boldsymbol{X}_{i}=\\boldsymbol{x}\\right)}{\\sum_{i=1}^{n} I\\left(\\boldsymbol{X}_{i}=\\boldsymbol{x}\\right)} .$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53fc69-209f-46f8-94da-abb3eaad2343",
   "metadata": {},
   "source": [
    "> The IPW estimator  is \n",
    "> $$\\hat{\\beta}_{a, \\mathrm{IPW}}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{I\\left(A_{i}=a\\right)}{\\hat{\\pi}_{a}\\left(\\boldsymbol{X}_{i}\\right)} Y_{i} = \\hat{\\beta}_{a, O R},$$ \n",
    "> where we suppose $\\hat{\\pi}_{a}(\\boldsymbol{x})>0$, which follows that $\\hat{\\beta}_{O R}=\\hat{\\beta}_{I P W}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d96e3-0683-467b-b6b1-4654970bfc1c",
   "metadata": {},
   "source": [
    "We have, by adding and subtracting $\\mu_{a}(\\boldsymbol{X})$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\sqrt{n}\\left(\\hat{\\beta}_{a, \\mathrm{IPW}}-\\beta_{a}\\right) \\\\\n",
    "= & \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\frac{I\\left(A_{i}=a\\right)}{\\hat{\\pi}_{a}\\left(\\boldsymbol{X}_{i}\\right)}\\left[Y_{i}-\\mu_{a}\\left(\\boldsymbol{X}_{i}\\right)\\right]+\\mu_{a}\\left(\\boldsymbol{X}_{i}\\right)-\\beta_{a} \\\\\n",
    "= & \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left\\{\\frac{I\\left(A_{i}=a\\right)}{\\pi_{a}\\left(\\boldsymbol{X}_{i}\\right)}\\left[Y_{i}-\\mu_{a}\\left(\\boldsymbol{X}_{i}\\right)\\right]+\\mu_{a}\\left(\\boldsymbol{X}_{i}\\right)-\\beta_{a}\\right\\}+R_{n} .\n",
    "\\end{aligned}$$\n",
    "\n",
    "The residual term\n",
    "\n",
    "$$R_{n}=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} I\\left(A_{i}=a\\right)\\left[\\frac{1}{\\hat{\\pi}_{a}\\left(\\boldsymbol{X}_{i}\\right)}-\\frac{1}{\\pi_{a}\\left(\\boldsymbol{X}_{i}\\right)}\\right]\\left[Y_{i}-\\mu_{a}\\left(\\boldsymbol{X}_{i}\\right)\\right] \\xrightarrow{p} 0$$\n",
    "\n",
    "as  $n \\rightarrow \\infty$. This is because  $\\hat{\\pi}_{a}(\\boldsymbol{x})$  generally converges to  $\\pi_{a}(\\boldsymbol{x})$  at  $1 / \\sqrt{n}$  rate and the other term  $I\\left(A_{i}=a\\right)\\left[Y_{i}-\\mu_{a}\\left(\\boldsymbol{X}_{i}\\right)\\right]$  is iid with mean $0$. This shows that  $\\hat{\\beta}_{a, \\text { IPW }}$  admits asymptotic linear expansion with the influence function\n",
    "\n",
    "$$\\psi_{\\beta_{a}}(\\boldsymbol{D})=\\frac{I(A=a)}{\\pi_{a}(\\boldsymbol{X})}\\left[Y-\\mu_{a}(\\boldsymbol{X})\\right]+\\mu_{a}(\\boldsymbol{X})-\\beta_{a} =: m_{a}\\left(\\boldsymbol{D} ; \\mu_{a}, \\pi_{a}\\right)-\\beta_{a}.$$\n",
    "\n",
    "> Therefore, $\\sqrt{n}\\left(\\hat{\\beta}_{a, O R}-\\beta_{a}\\right) \\xrightarrow{d} \\mathrm{~N}\\left(0, \\operatorname{Var}\\left(\\psi_{\\beta_{a}}\\left(\\boldsymbol{D}_{i}\\right)\\right)\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87233777-abfa-4cb9-974d-e55ea81d2d47",
   "metadata": {},
   "source": [
    "Combining the OR estimator and the IPW estimator, we obtain a more efficient\n",
    "and robust estimator $\\hat{\\beta}_{a, \\mathrm{DR}}=\\frac{1}{n} \\sum_{i=1}^{n} m_{a}\\left(\\boldsymbol{D}_{i}, \\hat{\\mu}_{a}, \\hat{\\pi}_{a}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204783b3-671d-4287-aee0-3bdb2b696271",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Unmeasured Confounders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52beabf6-80de-475d-a470-6c49a1028af4",
   "metadata": {},
   "source": [
    "Let $\\pi_{i}=\\mathbb{P}\\left(A_{i}=1 \\mid \\boldsymbol{C}_{i}\\right), i \\in\\left[2 n_{1}\\right]$ where $\\boldsymbol{C}_{i}=\\left(\\boldsymbol{X}_{i}, Y_{i}(0), Y_{i}(1)\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947d5bd-242d-4444-beb2-4f08fd080874",
   "metadata": {},
   "source": [
    "> For a given value  $\\Gamma \\geq 1$, we have\n",
    "> $$\\frac{1}{\\Gamma} \\leq \\frac{\\pi_{i} \\big/\\left(1-\\pi_{i}\\right)}{\\pi_{n_{1}+i} \\big/\\left(1-\\pi_{n_{1}+i}\\right)} \\leq \\Gamma, \\forall i \\in\\left[n_{1}\\right].$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a63895-f902-41a0-a5ac-e91237ca7cd0",
   "metadata": {},
   "source": [
    "Then we have \n",
    "$$ \\frac{1}{1+\\Gamma} \\leq \\mathbb{P}\\left(A_{i}=1, A_{n_{1}+i}=0 \\mid \\boldsymbol{C}_{\\left[2 n_{1}\\right]}, A_{i}+A_{n_{1}+i}=1\\right)=\\frac{\\pi_{i}\\left(1-\\pi_{n_{1}+i}\\right)}{\\pi_{i}\\left(1-\\pi_{n_{1}+i}\\right)+\\pi_{n_{1}+i}\\left(1-\\pi_{i}\\right)} \\leq \\frac{\\Gamma}{1+\\Gamma}, $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef4be8-8ef7-4b21-9999-80863bdd5586",
   "metadata": {},
   "source": [
    "which follows that $\\Gamma=1$ recovers no unmeasured confounders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588eba88-d1af-4b42-9afd-d88f854133d3",
   "metadata": {},
   "source": [
    "Next we consider the randomisation distribution of the signed score statistic $T_{\\psi}\\left(\\boldsymbol{D}_{\\left[n_{1}\\right]}\\right)=\\sum_{i=1}^{n_{1}} \\operatorname{sgn}\\left(D_{i}\\right) \\psi\\left(\\frac{\\operatorname{rank}\\left(\\left|D_{i}\\right|\\right)}{n_{1}+1}\\right)$ under Rosenbaum's sensitivity model. Given  $H_{0}$  and conditioning on  $\\boldsymbol{C}_{\\left[2 n_{1}\\right]} $,\n",
    "\n",
    "$$T\\left.\\bigg(\\boldsymbol{A}_{\\left[2 n_{1}\\right]}, \\boldsymbol{Y}_{\\left[2 n_{1}\\right]}(0)\\right) \\left\\lvert\\, \\boldsymbol{A}_{\\left[2 n_{1}\\right]} \\in M \\bigg)\\stackrel{d}{=} \\sum_{i=1}^{n_{1}} S_{i} \\psi\\left(\\frac{\\operatorname{rank}\\left(\\left|Y_{i}(0)-Y_{n_{1}+i}(0)\\right|\\right)}{n_{1}+1}\\right)\\right.,$$\n",
    "\n",
    "where  $S_{i}=\\left(A_{i}-A_{n_{1}+i}\\right) \\cdot \\operatorname{sgn}\\left(Y_{i}(0)-Y_{n_{1}+i}(0)\\right)$. $X$  stochastically dominates $Y$, written as  $X \\succeq Y$, if  $\\mathbb{P}(X>t) \\geq \\mathbb{P}(Y>t)$. Notice that $S_{i}$  stochastically dominates the following random variable\n",
    "\n",
    "$$S_{i}^{-}=\\left\\{\\begin{array}{ll}\n",
    "-1, & \\text { with probability } \\Gamma /(1+\\Gamma) \\\\\n",
    "1, & \\text { with probability } 1 /(1+\\Gamma)\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "This can be used to obtain a (sharp) bound on the $p$-value:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8787f-e8d8-4ad1-b460-f99735f78639",
   "metadata": {},
   "source": [
    "> Using the signed score statistic and given  $H_{0}$, $T\\left(\\boldsymbol{A}_{\\left[2 n_{1}\\right]}, \\boldsymbol{Y}_{\\left[2 n_{1}\\right]}(0)\\right) \\succeq \\sum_{i=1}^{n_{1}} S_{i}^{-} \\psi\\left(\\frac{\\operatorname{rank}\\left(\\left|D_{i}-\\beta\\right|\\right)}{n_{1}+1}\\right)$.\n",
    "\n",
    "**Proof.** This follows from noticing  $\\left|D_{i}-\\beta\\right|=\\left|Y_{i}(0)-Y_{n_{1}+i}(0)\\right|$  and the following property of stochastic ordering: If  $X_{i} \\succeq Y_{i}$ and  $X_{i} \\perp X_{j}, Y_{i} \\perp Y_{j}$  for all  $i \\neq j$, then  $$\\sum_{i=1}^{n} X_{i} \\succeq \\sum_{i=1}^{n} Y_{i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40bedc-1b80-4da0-8e08-6e3813b2929a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the rest, we consider structural specificity. First consider the unobserved confonders. Given iid observations of treatment  $A_{i}$, outcome  $Y_{i}$, instrumental variables  $\\boldsymbol{Z}_{i}$, observed confounders  $\\boldsymbol{X}_{i}$, and unobserved confounders  $\\boldsymbol{U}_{i}$, we assume the structural equations for  $A$  and outcome  $Y$  are given by\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "A=\\beta_{0 A}+\\boldsymbol{\\beta}_{Z A}^{T} \\boldsymbol{Z}+\\boldsymbol{\\beta}_{X A}^{T} \\boldsymbol{X}+\\boldsymbol{\\beta}_{U A}^{T} \\boldsymbol{U}+\\epsilon_{A}, \\\\\n",
    "Y=\\beta_{0 Y}+\\beta_{A Y} A+\\boldsymbol{\\beta}_{X Y}^{T} \\boldsymbol{X}+\\boldsymbol{\\beta}_{U Y}^{T} \\boldsymbol{U}+\\epsilon_{Y} .\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7e420-8965-4284-b648-6157a7e13b72",
   "metadata": {},
   "source": [
    "By using  $\\boldsymbol{Z} \\perp \\boldsymbol{U}, \\epsilon_{A}, \\epsilon_{Y}$, we obtain\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\mathbb{E}[A \\mid \\boldsymbol{Z}, \\boldsymbol{X}]=\\tilde{\\beta}_{0 A}+\\boldsymbol{\\beta}_{Z A}^{T} \\boldsymbol{Z}+\\tilde{\\boldsymbol{\\beta}}_{X A}^{T} \\boldsymbol{X}, \\\\\n",
    "\\mathbb{E}[Y \\mid \\boldsymbol{Z}, \\boldsymbol{X}]=\\tilde{\\beta}_{0 Y}+\\beta_{A Y} \\mathbb{E}[A \\mid \\boldsymbol{Z}, \\boldsymbol{X}]+\\tilde{\\boldsymbol{\\beta}}_{X Y}^{T} \\boldsymbol{X}.\n",
    "\\end{array}$$\n",
    "\n",
    "This motivates the **two-stage least squares estimator** of  $\\beta_{A Y}$  :\n",
    "\n",
    "(1) Estimate  $\\mathbb{E}[A \\mid \\boldsymbol{Z}, \\boldsymbol{X}]$  by a least squares regression of $A$  on $ \\boldsymbol{Z}$  and  $\\boldsymbol{X}$ . Let the fitted model be  $\\hat{\\mathbb{E}}[A \\mid \\boldsymbol{Z}, \\boldsymbol{X}]$.\n",
    "\n",
    "(2) Fit another regression of  $Y$  on  $\\hat{\\mathbb{E}}[A \\mid \\boldsymbol{Z}, \\boldsymbol{X}]$  and  $\\boldsymbol{X}$  by least squares, and let  $\\hat{\\beta}_{A Y}$  be the coefficient of  $\\hat{\\mathbb{E}}[A \\mid \\boldsymbol{Z}, \\boldsymbol{X}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6dcc61-3e49-43cf-b06c-d551102325df",
   "metadata": {},
   "source": [
    "Rigorously, we make the following assumptions about $Z$.\n",
    "\n",
    "> Relevance:  $\\boldsymbol{Z} \\not \\perp A $.\n",
    ">\n",
    "> Exogeneity: $ \\boldsymbol{Z} \\perp\\{A(\\boldsymbol{z}), Y(\\boldsymbol{z}, a)\\}$.\n",
    ">\n",
    "> Exclusion restriction:  $Y(\\boldsymbol{z}, a)=Y(a)$.\n",
    "\n",
    "Assume the causal effect of  $A$  on $Y$  is a constant  $\\beta$, $Y(a)-Y(\\tilde{a})=(a-\\tilde{a}) \\beta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288fbb9-4167-4f3e-92ff-1eb356010287",
   "metadata": {},
   "source": [
    "Let  $\\tilde{a}=0$, this gives $Y(0)=Y-\\beta A$. Let  $\\alpha=\\mathbb{E}[Y(0)]$. The exogeneity and exclusion restriction imply that, for any function  $g(\\boldsymbol{z})$,\n",
    "\n",
    "$$\\mathbb{E}[(Y-\\alpha-\\beta A) g(\\boldsymbol{Z})]=\\mathbb{E}[(Y(0)-\\alpha) g(\\boldsymbol{Z})]=0.$$\n",
    "\n",
    "Let  $\\hat{\\alpha}=\\bar{Y}-\\beta \\bar{A}$, where  $\\bar{A}=\\sum_{i=1}^{n} A_{i} / n$  and  $\\bar{Y}=\\sum_{i=1}^{n} Y_{i} / n$. The method of moments estimator of  $\\beta$  is given by solving the empirical version of above (and with $ \\alpha$  replaced by  $\\hat{\\alpha} $ ). After some algebra, we obtain\n",
    "\n",
    "$$\\hat{\\beta}_{g}=\\frac{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right) g\\left(\\boldsymbol{Z}_{i}\\right)}{\\frac{1}{n} \\sum_{i=1}^{n}\\left(A_{i}-\\bar{A}\\right) g\\left(\\boldsymbol{Z}_{i}\\right)} .$$\n",
    "\n",
    "\n",
    "This as an empirical estimator of  $\\operatorname{Cov}(Y, g(\\boldsymbol{Z})) / \\operatorname{Cov}(A, g(\\boldsymbol{Z}))$, the Wald ratio with  $Z$  replaced by  $g(\\boldsymbol{Z})$  in. In view of this,  $g(\\boldsymbol{Z})$  is a one-dimensional summary statistic of all the instrumental variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e543d-d607-4670-a3ac-8e82d50cb89f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6b6e9-b656-4348-97e2-51d828842db4",
   "metadata": {},
   "source": [
    "Then consider the  mediation analysis problem. In this case, the linear  $\\mathrm{SEM}$  is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "M & =\\beta_{A M} A+\\boldsymbol{\\beta}_{X M}^{T} \\boldsymbol{X}+\\epsilon_{M}, \\\\\n",
    "Y & =\\beta_{A Y} A+\\beta_{M Y} M+\\boldsymbol{\\beta}_{X Y}^{T} \\boldsymbol{X}+\\epsilon_{Y} .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69d1f6-ccc7-4555-8929-7eef02c726f6",
   "metadata": {},
   "source": [
    "This shows that $Y(a, m) \\perp A \\mid \\boldsymbol{X}$  and  $Y(m) \\perp M \\mid A, \\boldsymbol{X}$, which follows that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}[Y(a, m)] & =\\mathbb{E}\\{\\mathbb{E}[Y(a, m) \\mid \\boldsymbol{X}]\\} =\\mathbb{E}\\{\\mathbb{E}[Y(a, m) \\mid A=a, \\boldsymbol{X}]\\} \\\\\n",
    "& =\\mathbb{E}\\{\\mathbb{E}[Y(m) \\mid A=a, \\boldsymbol{X}]\\} =\\mathbb{E}\\{\\mathbb{E}[Y(m) \\mid A=a, M=m, \\boldsymbol{X}]\\} \\\\\n",
    "& =\\mathbb{E}\\{\\mathbb{E}[Y \\mid A=a, M=m, \\boldsymbol{X}]\\}.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310c61c-eb1a-4add-b091-56e5c3760cd4",
   "metadata": {},
   "source": [
    "Therefore, we have\n",
    "> $C D E(m) = \\mathbb{E}[Y(1, m)-Y(0, m)] = \\mathbb{E}[\\mathbb{E}[Y \\mid A=1, M=m, \\boldsymbol{X}]]-\\mathbb{E}[\\mathbb{E}[Y \\mid A=0, M=m, \\boldsymbol{X}]]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be4a8c-d7d8-4d34-9f33-c45a3ef3751a",
   "metadata": {},
   "source": [
    "Formally, \n",
    "\n",
    "> **Assumption of no unmeasured confounders**.\n",
    ">\n",
    "> (1) No unmeasured treatment-outcome confounders:  $Y(a, m) \\perp A \\mid \\boldsymbol{X}$. \n",
    ">\n",
    "> (2) No unmeasured mediator-outcome confounders:  $Y(m) \\perp M \\mid A, \\boldsymbol{X}$. \n",
    ">\n",
    "> (3) No unmeasured treatment-mediator confounders:  $M(a) \\perp A \\mid \\boldsymbol{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9416b3e-8918-47c5-a83f-8428e3fd2355",
   "metadata": {},
   "source": [
    "Together with **assumption of no treatment-induced mediator-outcome confounding**, $\\boldsymbol{X} \\cap   d e(A)=\\emptyset$, we can show that $Y(a, m) \\perp M\\left(a^{\\prime}\\right) \\mid \\boldsymbol{X}$, which follows that \n",
    "\n",
    "> $$\\mathbb{E}[Y(1, M(0))] =  \\sum_{m, \\boldsymbol{x}} \\mathbb{E}[Y \\mid A=1, M=m, \\boldsymbol{X}=\\boldsymbol{x}] \\cdot \\mathbb{P}(M=m \\mid A=0, \\boldsymbol{X}=\\boldsymbol{x}) \\cdot \\mathbb{P}(\\boldsymbol{X}=\\boldsymbol{x}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "2. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "3. Geoff K. Nicholls. Bayes Methods.\n",
    "4. Patrick J. Laub. Hawkes Processes.\n",
    "5. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
