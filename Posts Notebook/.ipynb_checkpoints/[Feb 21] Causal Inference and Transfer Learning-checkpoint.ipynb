{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Feb 21] Causal Inference and Transfer Learning\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c723a98-16e1-4224-997f-2b80431b4d02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.  Causal Inference Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc02ff-28a1-42e6-8cb4-55691668c09b",
   "metadata": {},
   "source": [
    "For randomised experiments, the basic postulates follow.\n",
    "\n",
    "> The assignment mechanism for $n$ units is $\\mathbb{P}\\left(\\boldsymbol{A}_{[n]}=\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{X}_{[n]}=\\boldsymbol{x}_{[n]}\\right)=\\pi\\left(\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{x}_{[n]}\\right)$ with treatments conditional on covariates. Here follows some examples.\n",
    ">\n",
    "> ( **Bernoulli trial with covariate** ) $\\pi\\left(\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{x}_{[n]}\\right)=\\prod_{i=1}^{n} \\pi\\left(\\boldsymbol{x}_{i}\\right)^{a_{i}}\\left\\{1-\\pi\\left(\\boldsymbol{x}_{i}\\right)\\right\\}^{1-a_{i}}$\n",
    ">\n",
    "> ( **Sample without replacement** ) $\\pi\\left(\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{x}_{[n]}\\right)=\\left\\{\\begin{array}{ll}\n",
    "\\left(\\begin{array}{c}\n",
    "n \\\\\n",
    "n_{1}\n",
    "\\end{array}\\right)^{-1}, & \\text { if } \\sum_{i=1}^{n} a_{i}=n_{1} \\\\\n",
    "0, & \\text { otherwise. }\n",
    "\\end{array}\\right.$\n",
    ">\n",
    "> The **PO** model introduces: an observed factual (outcome) is linked with **counterfactuals (potential outcomes)** via $Y_{i}=Y_{i}\\left(\\boldsymbol{A}_{[n]}\\right) = \\sum \\mathbb{1}(A_n=a_n) Y_i(a_n)$, together with **assumption of no interference**,  i.e. $Y_{i}\\left(\\boldsymbol{a}_{[n]}\\right)=Y_{i}\\left(a_{i}\\right) \\text { for all } i \\in[n] \\text { and } \\boldsymbol{a}_{[n]} \\in \\mathcal{A}^{n}$. ( note the abuse of notation and therefore, $Y_{i} \\neq Y_i(A_i) $ generally )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d7d31-419e-4946-9219-ca4163cbde2b",
   "metadata": {},
   "source": [
    "The potential outcome framework allows as to view causal inference as a missing data\n",
    "problem, which consider two populations: $\\mathrm{SATE}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}(1)-Y_{i}(0)$ and $\\mathrm{PATE}=\\mathbb{E}\\left[Y_{i}(1)-Y_{i}(0)\\right]$. ( The latter implicitly assumes that the $n$ units are sampled from a superpopulation )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83068b-b640-4eee-82cb-f79530d43730",
   "metadata": {},
   "source": [
    "> **Assumption of randomisation.** $\\boldsymbol{A}_{[n]} \\perp \\boldsymbol{Y}_{[n]}\\left(\\boldsymbol{a}_{[n]}\\right) \\mid \\boldsymbol{X}_{[n]} \\text { for } \\boldsymbol{a}_{[n]} \\in \\mathcal{A}^{n}$.\n",
    "\n",
    "Note that **assumption of randomisation** is different from  $\\boldsymbol{A}_{[n]} \\perp \\boldsymbol{Y}_{[n]} \\mid \\boldsymbol{X}_{[n]}$, as  $Y_{i}=Y_{i}\\left(A_{i}\\right)$  generally depends on  $A_{i}$. \n",
    "\n",
    "> We are using  $\\boldsymbol{X}$, $A$, $Y$, and $Y(a)$  to refer to a generic  $\\boldsymbol{X}_{i}$, $A_{i}$, $Y_{i}$, and $Y_{i}(a)$ when they are iid.\n",
    ">\n",
    "> **Thm.** ( **Causal identification in randomised experiments** ) Consider any assignment mechanism where  $\\left\\{\\boldsymbol{X}_{i}, A_{i}, Y_{i}(a), a \\in \\mathcal{A}\\right\\}$  are iid. Suppose the above assumptions are given, then\n",
    ">\n",
    "> $$\\begin{aligned} \\mathbb{P}(A=a \\mid \\boldsymbol{X}=\\boldsymbol{x})>0 & \\implies  (Y(a) \\mid \\boldsymbol{X}=\\boldsymbol{x}) \\stackrel{d}{=}(Y \\mid A=a, \\boldsymbol{X}=\\boldsymbol{x}) \\\\\n",
    "& \\implies  A T E=\\mathbb{E}[Y(1)-Y(0)]=\\mathbb{E}\\{\\mathbb{E}[Y \\mid A=1, \\boldsymbol{X}]-\\mathbb{E}[Y \\mid A=0, \\boldsymbol{X}]\\}. \\end{aligned} $$\n",
    "\n",
    "**Proof.** For the first implication, computation shows that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}(Y(a) \\leq y \\mid \\boldsymbol{X}=\\boldsymbol{x}) & =\\mathbb{P}\\left(Y_{i}(a) \\leq y \\mid \\boldsymbol{X}=\\boldsymbol{x}, A=a\\right), \\\\\n",
    "& =\\mathbb{P}(Y_i(A) \\leq y \\mid \\boldsymbol{X}=\\boldsymbol{x}, A=a), \\\\\n",
    "& =\\mathbb{P}(Y \\leq y \\mid \\boldsymbol{X}=\\boldsymbol{x}, A=a),\n",
    "\\end{aligned}$$\n",
    "\n",
    "where the first equality uses assumption of randomisation.\n",
    "\n",
    "> As a special case, if $\\mathbb{P}(A=1 \\mid \\boldsymbol{X})$  does not depend on  $\\boldsymbol{X}$  ( i.e. $A \\perp \\boldsymbol{X}$ ), then $PATE=\\mathbb{E}[Y \\mid A=1]-\\mathbb{E}[Y \\mid A=0]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99ee8a-4389-4cb6-83cf-44fc79c84830",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Neyman considered the following difference-in-means estimator:\n",
    "\n",
    "$$\\hat{\\beta}=\\bar{Y}_{1}-\\bar{Y}_{0}, \\text { where } \\bar{Y}_{1}=\\frac{\\sum_{i=1}^{n} A_{i} Y_{i}}{\\sum_{i=1}^{n} A_{i}}, \\bar{Y}_{0}=\\frac{\\sum_{i=1}^{n}\\left(1-A_{i}\\right) Y_{i}}{\\sum_{i=1}^{n} 1-A_{i}} \\text {. }$$\n",
    "\n",
    "Denote  $\\boldsymbol{Y}(a)=\\left(Y_{1}(a), Y_{2}(a), \\ldots, Y_{n}(a)\\right)^{T}$. \n",
    "\n",
    "> Neyman studied the conditional distribution of  $\\hat{\\beta}$  given the potential outcomes  $\\boldsymbol{Y}(0), \\boldsymbol{Y}(1)$. We may refer to this as the **randomization distribution**, because the only randomness left in  $\\hat{\\beta}$  comes from the randomization of $\\boldsymbol{A}_{[n]}$. \n",
    "\n",
    "Set $\\bar{Y}(a)=\\sum_{i=1}^{n} Y_{i}(a) / n$. Suppose the treatment assignments $A_i$ are sampled without replacement, by using  $\\mathbb{E}\\left[A_{i}\\right]=n_{1} / n$, ( For simplicity of exposition, we omit the conditioning on  $\\boldsymbol{Y}(a)$ )\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}[\\hat{\\beta}] & =\\mathbb{E}\\left[\\frac{1}{n_{1}} \\sum_{i=1}^{n} A_{i} Y_{i}-\\frac{1}{n_{0}} \\sum_{i=1}^{n}\\left(1-A_{i}\\right) Y_{i}\\right] =\\mathbb{E}\\left[\\frac{1}{n_{1}} \\sum_{i=1}^{n} A_{i} Y_{i}(1)-\\frac{1}{n_{0}} \\sum_{i=1}^{n}\\left(1-A_{i}\\right) Y_{i}(0)\\right] \\\\\n",
    "& =\\frac{1}{n_{1}} \\sum_{i=1}^{n} \\frac{n_{1}}{n} Y_{i}(1)-\\frac{1}{n_{0}} \\sum_{i=1}^{n} \\frac{n_{0}}{n} Y_{i}(0) =\\bar{Y}(1)-\\bar{Y}(0) .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "> Suppose the treatment assignments $A_i$ are sampled without replacement, $\\mathbb{E}[\\hat{\\beta} \\mid \\boldsymbol{Y}(0), \\boldsymbol{Y}(1)]=S A T E$ and \n",
    "> $$\\operatorname{Var}(\\hat{\\beta} \\mid \\boldsymbol{Y}(0), \\boldsymbol{Y}(1))=\\frac{1}{n_{0}} S_{0}^{2}+\\frac{1}{n_{1}} S_{1}^{2}-\\frac{S_{01}^{2}}{n},$$\n",
    ">\n",
    "> where  $n_{0}=n-n_{1}$, $S_{a}^{2}=\\sum_{i=1}^{n}\\left(Y_{i}(a)-\\bar{Y}(a)\\right)^{2} /(n-1)$, and  $S_{01}^{2}=\\sum_{i=1}^{n}\\left(Y_{i}(1)-Y_{i}(0)-S A T E\\right)^{2} /(n-1)$.\n",
    ">\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d2b15-4f12-4e61-836e-9643e709f554",
   "metadata": {},
   "source": [
    "\n",
    "For the variance, we can show that $$\\operatorname{Var}(\\hat{\\beta} \\mid \\boldsymbol{Y}(0), \\boldsymbol{Y}(1))=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n} \\frac{A_{i}}{n_{1}} Y_{i}^{*}(1)-\\frac{1-A_{i}}{n_{0}} Y_{i}^{*}(0)\\right)^{2}\\right].$$\n",
    "\n",
    "where $Y_{i}^{*}(a)=Y_{i}(a)-\\bar{Y}(a)$. Expand the sum of squares and use\n",
    "\n",
    "$$\\mathbb{E}\\left[A_{i} A_{i^{\\prime}}\\right]=\\frac{n_{1}}{n} \\frac{n_{1}-1}{n-1}, \\mathbb{E}\\left[A_{i} (1-A_{i^{\\prime}})\\right]=\\frac{n_{1}}{n} \\frac{n_{0}}{n-1}, i \\neq i^{\\prime} \\text { and } \\sum_{i=1}^{n} Y_{i}^{*}(a)=0,$$\n",
    "\n",
    "then we arrives at the conclusion. One drawback of Neyman’s randomisation inference is that it is difficult to extend it to settings with covariates unless the covariates are discrete. The main obstacle is that the randomisation distribution necessarily depends on unobserved potential outcomes.\n",
    "\n",
    "> It is common to estimate the variance  by  $\\hat{S}_{0}^{2} / n_{0}+\\hat{S}_{1}^{2} / n_{1}$, where \n",
    ">\n",
    "> $$\\hat{S}_{1}^{2}=\\frac{1}{n_{1}-1} \\sum_{i=1}^{n} A_{i}\\left(Y_{i}-\\bar{Y}_{1}\\right)^{2}, \\hat{S}_{0}^{2}=\\frac{1}{n_{0}-1} \\sum_{i=1}^{n}\\left(1-A_{i}\\right)\\left(Y_{i}-\\bar{Y}_{0}\\right)^{2}.$$\n",
    ">\n",
    "> This is an unbiased estimator of $S_{0}^{2} / n_{0}+S_{1}^{2} / n_{1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409742a4-169f-4f14-bce6-24d7122a1fdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Fisher is the first to grasp fully the importance of randomisation. Consider $H_{0}: Y_{i}(1)-Y_{i}(0)=\\beta, \\forall i \\in[n]$. Using the consistency assumption, the hypothesis allow us to impute the potential outcomes as\n",
    "\n",
    "$$Y_{i}(a)=\\left\\{\\begin{array}{ll}\n",
    "Y_{i}, & \\text { if } a=A_{i} \\\\\n",
    "Y_{i}+\\beta, & \\text { if } a>A_{i} \\\\\n",
    "Y_{i}-\\beta, & \\text { if } a<A_{i}\n",
    "\\end{array}\\right.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f3ddf-2fe2-4a5b-a7f3-329b24b0204e",
   "metadata": {},
   "source": [
    "A more compact form is $\\boldsymbol{Y}_{[n]}\\left(\\boldsymbol{a}_{[n]}\\right)=\\boldsymbol{Y}_{[n]}+\\beta\\left(\\boldsymbol{a}_{[n]}-\\boldsymbol{A}_{[n]}\\right)$. The key step is to derive the **randomisation distribution** of $T = T\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}\\right)$. There are two ways to do this, which is shown below. $\\big($**In both cases, the randomness comes from the randomisation of $\\boldsymbol{A}_{[n]}$**$\\big)$\n",
    "\n",
    "> Consider the distribution of  $T_{1}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right)$  given  $\\boldsymbol{X}_{[n]}$  and  $\\boldsymbol{Y}_{[n]}(0)$;\n",
    ">\n",
    "> Consider the distribution of  $T_{2}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}\\left(\\boldsymbol{A}_{[n]}\\right)\\right)$  given  $\\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)$, and  $\\boldsymbol{Y}_{[n]}(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43a02b-8d76-4c0b-9685-984ed125180e",
   "metadata": {},
   "source": [
    "Let  $\\mathcal{F}=\\left(\\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0), \\boldsymbol{Y}_{[n]}(1)\\right)$. The randomisation distributions in the two approaches above are given by\n",
    "\n",
    "$$F_{1}(t)=\\mathbb{P}\\left(T_{1}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right) \\leq t \\mid \\mathcal{F}\\right) \\quad \\text{and} \\quad F_{2}(t)=\\mathbb{P}\\left(T_{2}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}\\left(\\boldsymbol{A}_{[n]}\\right)\\right) \\leq t \\mid \\mathcal{F}\\right). $$\n",
    "\n",
    "The observed test statistics are\n",
    "\n",
    "$$T_{1}=T_{1}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}-\\beta \\boldsymbol{A}_{[n]}\\right), T_{2}=T_{2}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}\\right) .$$\n",
    "\n",
    "The one-sided  $p$-value is the probability of observing the same or a more extreme test statistic than the observed statistic $T$, which is denoted by $P_{m}=F_{m}\\left(T_{m}\\right)$. An equivalent and perhaps more informative representation is\n",
    "\n",
    "$$P_{1}=\\mathbb{P}^{*}\\left(T_{1}\\left(\\boldsymbol{A}_{[n]}^{*}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right) \\leq T_{1} \\mid \\mathcal{F}\\right),$$\n",
    "\n",
    "where  $\\boldsymbol{A}_{[n]}^{*}$  is an independent copy of  $\\boldsymbol{A}$, so  $\\boldsymbol{A}_{[n]}^{*} \\mid \\boldsymbol{X}_{[n]} \\sim \\pi$  but  $\\boldsymbol{A}^{*} \\perp \\boldsymbol{A}$, and  $\\mathbb{P}^{*}$ is w.r.t.  $\\boldsymbol{A}^{*}$. The other  $p$-value  $P_{2}$  can be similarly defined. \n",
    "\n",
    "> A level- $\\alpha$  randomisation test then rejects  $H_{0}$  if  $P_{m} \\leq \\alpha$ .\n",
    "\n",
    "**Proof.** We know that \n",
    "\n",
    ">  If $F(t)$ is the distribution function of a random variable $T$, then $\\mathbb{P}(F(T) \\leq \\alpha)=\\mathbb{P}\\left(T<F^{-1}(\\alpha)\\right)=\\lim _{t \\uparrow F^{-1}(\\alpha)} \\mathbb{P}(T \\leq t) \\leq \\alpha$. Here $F^{-1}(\\alpha)=\\sup \\{t \\mid F(t) \\leq \\alpha\\}$. \n",
    "\n",
    "This shows that under assumption of randomness and $ H_{0}$, \n",
    "\n",
    "> $$\\mathbb{P}\\left(P_{m} \\leq \\alpha\\right) \\leq   \\alpha, \\forall 0<\\alpha<1, m=1,2.$$\n",
    "\n",
    "which enables us to do the level-$\\alpha$  randomisation test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad733f3-b857-4855-aae1-35f0f6b2673b",
   "metadata": {},
   "source": [
    "The randomisation assumption make the $p$-values possible to compute. To see this, by definition we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "F_{1}(t) & =\\mathbb{P}\\left(T_{1}\\left(\\boldsymbol{A}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right) \\leq t \\mid \\mathcal{F}\\right) \\\\\n",
    "& =\\sum_{\\boldsymbol{a}_{[n]} \\in \\mathcal{A}^{n}} \\mathbb{P}\\left(\\boldsymbol{A}_{[n]}=\\boldsymbol{a}_{[n]} \\mid \\mathcal{F}\\right) \\cdot I\\left(T\\left(\\boldsymbol{a}_{[n]}, \\boldsymbol{X}_{[n]}, \\boldsymbol{Y}_{[n]}(0)\\right) \\leq t\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Assumption of randomness and  $H_{0}$  allow us to replace the first term by\n",
    "\n",
    "$$\\mathbb{P}\\left(\\boldsymbol{A}_{[n]}=\\boldsymbol{a}_{[n]} \\mid \\mathcal{F}\\right)=\\mathbb{P}\\left(\\boldsymbol{A}_{[n]}=\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{X}_{[n]}\\right)=\\pi\\left(\\boldsymbol{a}_{[n]} \\mid \\boldsymbol{X}_{[n]}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938cca83-0419-46c3-83bd-75600d2c1e0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8edb4-ba32-41d8-b7b4-144500ab5a82",
   "metadata": {},
   "source": [
    "Next we consider a different inference paradigm where the potential outcomes are drawn\n",
    "from a “super-population”.\n",
    "\n",
    "> Asymptotic super-population inference will be discusses by considering the **simple** Bernoulli trial (Example 2.1), so  $A_{i} \\perp \\boldsymbol{X}_{i}$. \n",
    ">\n",
    "> Further, suppose  $\\left(A_{i}, \\boldsymbol{X}_{i}, Y_{i}(0), Y_{i}(1)\\right)$  are i.i.d. and $\\mathbb{E}[\\boldsymbol{X}]=\\mathbf{0}$.\n",
    "\n",
    "Denote  $\\pi=\\mathbb{P}(A=1)$, $\\boldsymbol{\\Sigma}=\\mathbb{E}\\left[\\boldsymbol{X} \\boldsymbol{X}^{T}\\right]$, and  $\\beta=\\mathbb{E}[Y \\mid A=1]-\\mathbb{E}[Y \\mid A=0]$.\n",
    "\n",
    "> In a randomised experiment, $\\beta =$ PATE.\n",
    "\n",
    "We shall consider three regression estimators of  $\\beta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left(\\hat{\\alpha}_{1}, \\hat{\\beta}_{1}\\right) & =\\underset{\\alpha, \\beta}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\alpha-\\beta A_{i}\\right)^{2}, \\\\\n",
    "\\left(\\hat{\\alpha}_{2}, \\hat{\\beta}_{2}, \\hat{\\gamma}_{2}\\right) & =\\underset{(\\alpha, \\beta, \\boldsymbol{\\gamma})}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\alpha-\\beta A_{i}-\\boldsymbol{\\gamma}^{T} \\boldsymbol{X}_{i}\\right)^{2}, \\\\\n",
    "\\left(\\hat{\\alpha}_{3}, \\hat{\\beta}_{3}, \\hat{\\gamma}_{3}, \\hat{\\boldsymbol{\\delta}}_{3}\\right) & =\\underset{(\\alpha, \\beta, \\boldsymbol{\\gamma}, \\boldsymbol{\\delta})}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\alpha-\\beta A_{i}-\\boldsymbol{\\gamma}^{T} \\boldsymbol{X}_{i}-A_{i}\\left(\\boldsymbol{\\delta}^{T} \\boldsymbol{X}_{i}\\right)\\right)^{2} .\n",
    "\\end{aligned}$$\n",
    "\n",
    "Then write down the population version of the least squares problems:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left(\\alpha_{1}, \\beta_{1}\\right) & =\\underset{\\alpha, \\beta}{\\arg \\min } \\mathbb{E}\\left[(Y-\\alpha-\\beta A)^{2}\\right], \\\\\n",
    "\\left(\\alpha_{2}, \\beta_{2}, \\gamma_{2}\\right) & =\\underset{(\\alpha, \\beta, \\boldsymbol{\\gamma})}{\\arg \\min } \\mathbb{E}\\left[\\left(Y-\\alpha-\\beta A-\\boldsymbol{\\gamma}^{T} \\boldsymbol{X}\\right)^{2}\\right], \\\\\n",
    "\\left(\\alpha_{3}, \\beta_{3}, \\boldsymbol{\\gamma}_{3}, \\boldsymbol{\\delta}_{3}\\right) & =\\underset{(\\alpha \\beta, \\boldsymbol{\\gamma}, \\boldsymbol{\\delta})}{\\arg \\min } \\mathbb{E}\\left[\\left(Y-\\alpha-\\beta A-\\boldsymbol{\\gamma}^{T} \\boldsymbol{X}-A \\cdot\\left(\\boldsymbol{\\delta}^{T} \\boldsymbol{X}\\right)\\right)^{2}\\right] .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24338342-6341-4e4f-9f6a-8e00052b9164",
   "metadata": {},
   "source": [
    "> Lemma. Suppose  $\\left(\\boldsymbol{X}_{i}, A_{i}, Y_{i}\\right)$  are iid,  $A \\perp X$, $\\mathbb{E}[\\boldsymbol{X}]=0$. Then  $\\alpha_{1}=\\alpha_{2}=\\alpha_{3}$  and  $\\beta_{1}=\\beta_{2}=\\beta_{3}=\\beta$ .\n",
    "\n",
    "Proof. By taking partial derivatives, we obtain\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}\\left[Y-\\alpha_{3}-\\beta_{3} A-\\gamma_{3}^{T} \\boldsymbol{X}-A\\left(\\boldsymbol{\\delta}_{3}^{T} \\boldsymbol{X}\\right)\\right] & =0 \\\\\n",
    "\\mathbb{E}\\left[A\\left(Y-\\alpha_{3}-\\beta_{3} A-\\gamma_{3}^{T} \\boldsymbol{X}-A\\left(\\boldsymbol{\\delta}_3^{T} \\boldsymbol{X}\\right)\\right)\\right] & =0 .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Using  $\\mathbb{E}[\\boldsymbol{X}]=0$  and  $A \\perp \\boldsymbol{X}$ , they can be simplified to\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}\\left[Y-\\alpha_{3}-\\beta_{3} A\\right] & =0, \\\\\n",
    "\\mathbb{E}\\left[A\\left(Y-\\alpha_{3}-\\beta_{3} A\\right)\\right] & =0 .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Following the same derivation, these two equations also hold for the other estimators. By cancelling  $\\alpha_{3}$  in the equations, we get  $\\beta_{3}=\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb6f2c-260c-4c0f-8afd-be3619b30e38",
   "metadata": {},
   "source": [
    "Suppose $\\hat{\\boldsymbol{\\theta}}$  is an empirical solution to the equation\n",
    "\n",
    "$$\\mathbb{E}[\\boldsymbol{\\psi}(\\boldsymbol{\\theta} ; \\boldsymbol{Z}, Y)]=\\mathbf{0},$$\n",
    "\n",
    "where $\\boldsymbol{\\psi}(\\boldsymbol{\\theta} ; \\boldsymbol{Z}, Y)=\\boldsymbol{Z} \\cdot\\left(Y-\\boldsymbol{Z}^{T} \\boldsymbol{\\theta}\\right)=\\boldsymbol{Z} \\epsilon$. Suppose $Y$ and $Z$\n",
    "have bounded fourth moments, the $Z$-estimation theory shows that\n",
    "\n",
    "$$\\begin{aligned} & \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}) \\xrightarrow{d} \\mathrm{~N}\\left(\\mathbf{0},\\left\\{\\mathbb{E}\\left[\\frac{\\partial \\boldsymbol{\\psi}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\\right]\\right\\}^{-1} \\mathbb{E}\\left[\\boldsymbol{\\psi}(\\boldsymbol{\\theta}) \\boldsymbol{\\psi}(\\boldsymbol{\\theta})^{T}\\right]\\left\\{\\mathbb{E}\\left[\\frac{\\partial \\boldsymbol{\\psi}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\\right]\\right\\}^{-1}\\right) \\\\\n",
    "\\implies  & \\sqrt{n}(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}) \\xrightarrow{d} \\mathrm{~N}\\left(0,\\left\\{\\mathbb{E}\\left[\\boldsymbol{Z} \\boldsymbol{Z}^{T}\\right]\\right\\}^{-1} \\mathbb{E}\\left[\\boldsymbol{Z} \\boldsymbol{Z}^{T} \\epsilon^{2}\\right]\\left\\{\\mathbb{E}\\left[\\boldsymbol{Z} \\boldsymbol{Z}^{T}\\right]\\right\\}^{-1}\\right). \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d65fe-5ba4-4aa5-8f56-62b3b7006434",
   "metadata": {},
   "source": [
    "The asymptotic normality follows from the argument below. Using Taylor’s expansion,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ae5a6-641f-435b-aaf5-fd772d6b01ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a356f62-ef80-4c9e-ad3f-d2c0111e7aaf",
   "metadata": {},
   "source": [
    "\n",
    "> (**Decomposition**) Causal estimator - True causal effect  =  Design bias + Modelling bias + Statistical noise.\n",
    "\n",
    "Concrete, let  $\\boldsymbol{O}$  be all the observed variables with distribution $\\mathcal{O}$; $\\boldsymbol{F}$  (full data) denote the relevant factuals and counterfactuals with  $\\mathcal{F}$  be its distribution.\n",
    "Then the decomposition really is\n",
    "\n",
    "$$\\beta\\left(\\boldsymbol{O}_{[n]} ; \\hat{\\theta}\\right)-\\beta(\\mathcal{F})=\\{\\beta(\\mathcal{O})-\\beta(\\mathcal{F})\\}+\\{\\beta(\\mathcal{O} ; \\theta)-\\beta(\\mathcal{O})\\}+\\left\\{\\beta\\left(\\boldsymbol{O}_{[n]} ; \\hat{\\theta}\\right)-\\beta(\\mathcal{O} ; \\theta)\\right\\}$$\n",
    "\n",
    "where  $\\beta$  is a generic symbol for causal effect functional (estimator),  $\\boldsymbol{O}_{[n]}$  is the observed data of size  $n$, $\\theta$  is the parameter in a statistical model and  $\\hat{\\theta}=\\hat{\\theta}\\left(\\boldsymbol{O}_{[n]}\\right)$  is an estimator of  $\\theta$.\n",
    "\n",
    "> **Example.** In regression adjustment for randomised experiments,  $\\boldsymbol{O}=(\\boldsymbol{X}, A, Y)$ ,  $\\boldsymbol{F}=(Y(0), Y(1))$, $\\beta(\\mathcal{F})=\\mathbb{E}[Y(1)-Y(0)]$, $\\beta(\\mathcal{O})=\\mathbb{E}[Y \\mid A=1]-\\mathbb{E}[Y \\mid A=0]$, $\\beta(\\mathcal{O}, \\theta)$  be the any of  (2.14),(2.15) , or (2.16),  $\\beta\\left(\\boldsymbol{O}_{[n]}; \\hat{\\theta}\\right)$  be the corresponding (2.11), (2.12), or  (2.13) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7293f0-7474-4447-b975-7bb06b874a21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Graphic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93fb44-29e3-4fa0-9a93-2b694f40ee2f",
   "metadata": {},
   "source": [
    "> Given a DAG  $\\mathcal{G}=(V=[p], E)$ (**DAG implies a topological ordering**), the random variables  $\\boldsymbol{X}=\\boldsymbol{X}_{[p]}$  satisfy a NPSEM if the observed and interventional distributions of  $\\boldsymbol{X}_{[p]}$  satisfy\n",
    ">\n",
    "> $$X_{i}=f_{i}\\left(\\boldsymbol{X}_{p a_{\\mathcal{G}}(i)}, \\epsilon_{i}\\right), i=1, \\ldots, p,$$\n",
    "> for some functions  $f_{1}, \\ldots, f_{p}$  and random variables  $\\boldsymbol{\\epsilon}_{[p]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5e1b3-5831-4c14-9926-976b7f7bfdfd",
   "metadata": {},
   "source": [
    "Given the above NPSEM, the counterfactual variables  $X_{i}(\\boldsymbol{X}_{J}=\\boldsymbol{x}_{J})$  can be obtained via recursive substitution: For any  $i \\in[p]$, $J \\subseteq[p]$  and  $J \\neq p a(i)$ , we recursively set\n",
    "\n",
    "$$ X_{i}\\left(\\boldsymbol{x}_{J}\\right) = X_{i}\\left(\\boldsymbol{X}_{J}=\\boldsymbol{x}_{J}\\right)=X_{i}\\left(\\boldsymbol{X}_{p a(i) \\cap J}=\\boldsymbol{x}_{p a(i) \\cap J}, \\boldsymbol{X}_{p a(i) \\backslash J}=\\boldsymbol{X}_{p a(i) \\backslash J}\\left(\\boldsymbol{x}_{J}\\right)\\right).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb569e-69f0-436f-a93b-48895ec7f121",
   "metadata": {},
   "source": [
    "> Consider any disjoint  $J, K \\subseteq V$  and any  $i \\in V$. If  $K$  blocks all directed paths from  $J$  to  $i$, then $\\boldsymbol{X}_{i}\\left(\\boldsymbol{x}_{J}, \\boldsymbol{x}_{K}\\right)=\\boldsymbol{X}_{i}\\left(\\boldsymbol{x}_{K}\\right)$.\n",
    "\n",
    "\n",
    "Proof. This follows from recursive substitution and the next observation: if  K  blocks all directed paths from  J  to  i , then  K  also blocks directed paths from  J  to  p a(i) \\backslash K ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9f6bd-c7fb-416d-94cf-94d8de3fa510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968e7b6-ba44-42a2-9584-93e8a3a9f873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc23e2d2-eb7f-4db3-b346-94a1bd212ace",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. No Unmeasured Confounders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55cd1b-b807-4bcd-846d-7e24d74cb3f5",
   "metadata": {},
   "source": [
    "An observational study is an empirical investigation that utilises observation data (without\n",
    "manipulation or intervention), which envolves two stages: **design** and **analysis**. \n",
    "\n",
    "> ( **Randomised experiment** ) Randomisation allows us to choose between statistical\n",
    "error and causality. This reasoning is inductive.\n",
    ">\n",
    "> ( **Observational Study** ) Randomisation is replaced by pair matching. As a consequence, apart\n",
    "from statistical error and causality, a third possible **explanation** is that the treated patients\n",
    "and the control patients are systematically different in some other way.\n",
    "\n",
    "> Assume  $\\left(\\boldsymbol{X}_{i}, A_{i}, Y_{i}(0), Y_{i}(1)\\right)$,  $i=1, \\ldots, n$, are i.i.d. \n",
    "> \n",
    "> **Assumption of no unmeasured confounders.** $ A \\perp Y(a) \\mid \\boldsymbol{X}$ for $a=0,1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f2c5c-eeb4-44c2-a77a-0c9787218b95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d27e2-05d3-4e70-85e5-8ccc0ea5d6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ef7865-c297-480d-8fb2-4bda7eb066b8",
   "metadata": {},
   "source": [
    "Next we consider the statistical inference after matching.\n",
    "\n",
    "> Assume treated observation  $i$  is matched to control observation  $i+n_{1}$, $i\\in [n_{1}]$ . \n",
    "\n",
    "Let $D_{i}=\\left(A_{i}-A_{i+n_{1}}\\right)\\left(Y_{i}-Y_{i+n_{1}}\\right)$ be the treated-minus-control difference in pair  $i$. Let $M=\\left\\{\\boldsymbol{a}_{\\left[2 n_{1}\\right]} \\in\\{0,1\\}^{2 n_{1}} \\mid a_{i}+a_{i+n_{1}}=1, \\forall i \\in\\left[n_{1}\\right]\\right\\}$ be all the reasonable treatment assignments . Let  $\\boldsymbol{C}_{i}=\\left(\\boldsymbol{X}_{i}, Y_{i}(0), Y_{i}(1)\\right)$.\n",
    "\n",
    "There are two ways to proceed from here. The first approach is to use the sample average of  D_{i} ,\n",
    "\n",
    "\\bar{D}=\\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}} D_{i}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d196384-a9f4-4ad1-bb15-4cf201d04d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c735fa5-dcd8-4851-b5ec-59c2c4780573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b5240b8-2324-4828-bd3d-e73e3d9cbbf0",
   "metadata": {},
   "source": [
    " semiparametric inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469a05e-7b46-463c-badb-db29eed23d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7782f84-1827-4913-9ced-f04c89a14553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "204783b3-671d-4287-aee0-3bdb2b696271",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Unmeasured Confounders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52beabf6-80de-475d-a470-6c49a1028af4",
   "metadata": {},
   "source": [
    "Let $\\pi_{i}=\\mathbb{P}\\left(A_{i}=1 \\mid \\boldsymbol{C}_{i}\\right), i \\in\\left[2 n_{1}\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea9566-9c93-4722-bf8b-27e25a210ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c7c64-6e12-4e6b-915b-9fc9c358b332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d5b03-de95-4d29-a740-5d7d6b3774fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89108-ffde-4464-98f6-71736825df1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec6d36-2855-466b-b73a-b8aff7890d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59803b90-6fc7-49b5-8746-c83fadfa58be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "2. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "3. Geoff K. Nicholls. Bayes Methods.\n",
    "4. Patrick J. Laub. Hawkes Processes.\n",
    "5. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0933978-3704-44b8-939b-460c8e7e238e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
