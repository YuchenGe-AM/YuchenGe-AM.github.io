{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Dec 17] Markov Decision Process IV\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716c726",
   "metadata": {},
   "source": [
    "Model first, then solve.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a5e95-999a-4029-bb66-5fe6047f0c9c",
   "metadata": {},
   "source": [
    "### 1. MDP Formulation\n",
    "\n",
    "A ( **Infinite-Horizon** ) MDP $M$ includes \n",
    "\n",
    "> **transition** $P: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ with $P(\\cdot \\mid s, a)$, **reward** $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow[0,1]$, and an **initial state distribution** $\\mu \\in \\Delta(\\mathcal{S})$.\n",
    "\n",
    "Consider $\\tau_{t}=\\left(s_{0}, a_{0}, r_{0}, s_{1}, \\ldots, s_{t}, a_{t}, r_{t}\\right)$ as a trajectory, then \n",
    "\n",
    "> a policy is $\\pi: \\mathcal{H} \\rightarrow \\Delta(\\mathcal{A})$ where $\\mathcal{H}$ is the set of all possible trajectories (of all lengths). Specially, we have **stationary policy** $\\pi: \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$, i.e.  $a_{t} \\sim \\pi\\left(\\cdot \\mid s_{t}\\right)$, and **deterministic policy** $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$.\n",
    ">\n",
    "> Set $V^{\\pi}(s)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r\\left(s_{t}, a_{t}\\right) \\mid \\pi, s_{0}=s\\right]$ and $Q^{\\pi}(s, a)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r\\left(s_{t}, a_{t}\\right) \\mid \\pi, s_{0}=s, a_{0}=a\\right]$ where $\\gamma$ is a discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47970f27-e5f3-4891-8c6e-a470d2ade0dc",
   "metadata": {},
   "source": [
    "The goal of MDP is to solve $\\max _{\\pi} V^{\\pi}(s)$. A stationary policy $\\pi$ induces $P_{(s, a),\\left(s^{\\prime}, a^{\\prime}\\right)}^{\\pi}:=P\\left(s^{\\prime} \\mid s, a\\right) \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right)$.\n",
    "\n",
    "> For a stationary $\\pi$,\n",
    "> \n",
    "> $$ \\begin{aligned} V^{\\pi}(s) & = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} Q^{\\pi}(s, a) . \\\\ Q^{\\pi}(s, a) & =r(s, a)+\\gamma \\mathbb{E}_{s^{\\prime} \\sim P(\\cdot \\mid s, a)}\\left[V^{\\pi}\\left(s^{\\prime}\\right)\\right] \\end{aligned} \\implies Q^{\\pi} = r+\\gamma P V^{\\pi} = r+\\gamma P^{\\pi} Q^{\\pi}.$$\n",
    "> \n",
    "> Here we view  $V^{\\pi}$  as vector of length  $|\\mathcal{S}|$, $Q^{\\pi}$  and  $r$  as vectors of length  $|\\mathcal{S}| \\cdot|\\mathcal{A}|$, $P$  as a matrix of size  $(|\\mathcal{S}| \\cdot|\\mathcal{A}|) \\times|\\mathcal{S}|$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28345b3-59ea-48cd-bfd2-209e4e1a0572",
   "metadata": {},
   "source": [
    "If we can show $I-\\gamma P^{\\pi}$ is invertible, then $Q^{\\pi}=\\left(I-\\gamma P^{\\pi}\\right)^{-1} r$. This is clear since for $\\gamma<1$, $x \\neq 0$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\left\\|\\left(I-\\gamma P^{\\pi}\\right) x\\right\\|_{\\infty} & =\\left\\|x-\\gamma P^{\\pi} x\\right\\|_{\\infty} \\\\\n",
    "& \\geq\\|x\\|_{\\infty}-\\gamma\\left\\|P^{\\pi} x\\right\\|_{\\infty} &  \\geq\\|x\\|_{\\infty}-\\gamma\\|x\\|_{\\infty} & \\text { (each element of } P^{\\pi} x \\text { is an average of } x \\text { ) } \\\\\n",
    "& =(1-\\gamma)\\|x\\|_{\\infty}>0 & (\\gamma<1, x \\neq 0)\n",
    "\\end{aligned}$$\n",
    "\n",
    "which implies  $I-\\gamma P^{\\pi}$  is full rank.\n",
    "\n",
    "> Define  $\\Pi = \\{ \\text{ non-stationary and randomized policies } \\}$, $V^{\\star}(s) :=\\sup _{\\pi \\in \\Pi} V^{\\pi}(s)$ and $Q^{\\star}(s, a) :=\\sup _{\\pi \\in \\Pi} Q^{\\pi}(s, a)$. Then there exists a stationary and deterministic policy  $\\pi$  s.t. $V^{\\pi}(s) = V^{\\star}(s)$ and $Q^{\\pi}(s, a) =Q^{\\star}(s, a)$.\n",
    "\n",
    "\n",
    "We refer to such  a $\\pi$  as an optimal policy.\n",
    "\n",
    "\n",
    "The Bellman optimality operator  \\mathcal{T}_{M}: \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|} \\rightarrow \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}  is defined as:\n",
    "\n",
    "\\mathcal{T} Q:=r+\\gamma P V_{Q}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caab3a9-95d9-42de-bc31-e39307d33683",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t} r\\left(s_{t}, a_{t}\\right) \\mid \\pi,\\left(S_{0}, A_{0}, R_{0}, S_{1}\\right)=\\left(s, a, r, s^{\\prime}\\right)\\right]=\\gamma \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r\\left(s_{t}, a_{t}\\right) \\mid \\pi_{(s, a, r)}, S_{0}=s^{\\prime}\\right]=\\gamma V^{\\pi_{(s, a, r)}}\\left(s^{\\prime}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6cf57-0510-4318-b594-f775a8f6be49",
   "metadata": {},
   "source": [
    "A **finite-horizon ( and time-dependent )** MDP $M$ differs from the tradtional one only by indexing $P_{h}$ and $r_h$ with $h \\in\\{0, \\ldots H-1\\}$. \n",
    "\n",
    ">\n",
    ">\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b24a1-6719-4dda-ad5c-e5ffbca4991d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "131ab663-2a9e-48d2-995f-d4c81b015435",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea4b6f46-bb64-4c9b-b705-783a2d61c5ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03f5331e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Alekh Agarwal. Reinforcement Learning: Theory and Algorithms.\n",
    "2. Warren B. Powell. Reinforcement Learning and Stochastic Optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
