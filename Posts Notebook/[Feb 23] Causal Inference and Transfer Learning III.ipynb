{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Feb 23] Causal Inference and Transfer Learning III\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c723a98-16e1-4224-997f-2b80431b4d02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.  Bayesian Method Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42668b62-7326-44b1-8d4e-ee3d2dcc4664",
   "metadata": {},
   "source": [
    "In a Bayesian setting, we have some unknown real-world quantity  $\\Theta$  takes values in a parameter space  $\\Omega$. Typically  $\\Omega \\subset R^{p}$. \n",
    "\n",
    "> Let  $S \\subseteq \\Omega$, is  $\\Theta$  in  $S$  ?\n",
    "\n",
    "If knowledge of the world is coherent (as is shown below), then $\\exists$ a unique **prior distribution** with density  $\\pi(\\theta)$  on  $\\Omega$ and \n",
    "\n",
    "> $$\\int_{S} \\pi(\\theta) d \\theta=\\operatorname{Pr}(\\Theta \\in S). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fab03-631b-401a-9ce7-15217dab004f",
   "metadata": {},
   "source": [
    "Observations  $Y=\\left(Y_{1}, \\ldots, Y_{n}\\right)$, $Y \\in \\mathcal{Y}$  are distributed according to an observation model with probability density  $p(y \\mid \\theta)$  for realisation  $Y=y$  with  $y=\\left(y_{1}, \\ldots, y_{n}\\right)$  given  $\\Theta=\\theta$. If the observations are iid then we have an **observation model** with probability density\n",
    "\n",
    "$$p(y \\mid \\theta)=\\prod_{i=1}^{n} p\\left(y_{i} \\mid \\theta\\right).$$\n",
    "\n",
    "Therefore, our beliefs about $\\Theta \\in S$ change with the **posterior density** of the posterior distribution being\n",
    "\n",
    "$$\\pi(S \\mid y)=\\operatorname{Pr}(\\Theta \\in S \\mid Y=y) \\pi(\\theta \\mid y)=\\frac{p(y \\mid \\theta) \\pi(\\theta)}{p(y)} \\quad \\text{ where } \\quad p(y)=\\int_{\\Omega} p(y \\mid \\theta) \\pi(\\theta) d \\theta\n",
    "$$\n",
    "\n",
    "is the normalising marginal likelihood (also, **the prior predictive distribution**). Answers to questions about  $\\Theta$  can be given in terms of  $\\pi(\\theta \\mid y)$ via $\\operatorname{Pr}(\\Theta \\in S \\mid Y=y)=\\int_{S} \\pi(\\theta \\mid y) d \\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efe07c-5315-4acf-a954-496a1226ed19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561ad4e-6763-4e90-afac-6cd06e024415",
   "metadata": {},
   "source": [
    "Given observation model  $Y \\sim p(\\cdot \\mid \\theta)$, the  $\\Theta$-estimator $\\delta: \\mathcal{Y} \\rightarrow \\mathbb{R}^{p}$, has risk  $\\mathcal{R}(\\theta, \\delta)$  at $\\Theta=\\theta$  given by\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathcal{R}(\\theta, \\delta) & =E_{Y \\mid \\Theta=\\theta}(L(\\theta, \\delta(Y))) \\\\\n",
    "& =\\int_{\\mathcal{Y}} L(\\theta, \\delta(y)) p(y \\mid \\theta) d y .\n",
    "\\end{aligned}$$\n",
    "\n",
    ">  The Bayes risk,  $\\rho(\\pi, \\delta)$, is the risk averaged over the prior,\n",
    ">\n",
    "> $$\\begin{aligned}\n",
    "\\rho(\\pi, \\delta) & =E_{\\Theta}(\\mathcal{R}(\\theta, \\delta)) =E_{\\Theta, Y}(L(\\Theta, \\delta(Y))) \\\\\n",
    "& =\\int_{\\Omega} \\int_{\\mathcal{Y}} L(\\theta, \\delta(y)) p(y \\mid \\theta) \\pi(\\theta) d y d \\theta .\n",
    "\\end{aligned}$$\n",
    "> \n",
    "> where we have a prior  $\\pi(\\theta)$, posterior  $\\pi(\\theta \\mid y)$  and marginal likelihood  $p(y)$.\n",
    "\n",
    "\n",
    "A Bayes estimator  $\\delta^{\\pi}$  for  $\\theta$  minimises the Bayes risk $\\delta^{\\pi}=\\arg \\min _{\\delta} \\rho(\\pi, \\delta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ae5b2-d098-4a35-9e81-ed2971f7f08c",
   "metadata": {},
   "source": [
    "This is not straightforward as $\\delta$ is a function, then we consider the **expected posterior\n",
    "loss**\n",
    "$$\\rho(\\pi, \\delta \\mid y) = E_{\\Theta \\mid Y=y}(L(\\Theta, \\delta(y))) =\\int_{\\Omega} L(\\theta, \\delta(y)) \\pi(\\theta \\mid y) d \\theta \\implies \\rho(\\pi, \\delta)=\\int_{\\mathcal{Y}} \\rho(\\pi, \\delta \\mid y) p(y) d y.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112e472-c571-49bb-b891-72f163b6f229",
   "metadata": {},
   "source": [
    "> $\\delta^{\\pi}(y)=\\arg \\min _{\\delta} \\rho(\\pi, \\delta \\mid y)$ minimizes the Bayes risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ffcf6-0cb9-48ed-9e25-0d7a53329f66",
   "metadata": {},
   "source": [
    "For example, let $L(\\Theta, \\delta(Y))=(\\Theta-\\delta(Y))^{2}$, in this case $ E_{\\Theta \\mid y}(L(\\Theta, \\delta)) $ is minimised over actions by the posterior mean  $\\delta^{*}=E_{\\Theta \\mid y}(\\Theta)$  (just differentiate wrt  $\\delta(y)$  at fixed  $y$  ). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4764d-1a1b-42e5-9751-f24296202324",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Suppose we wish to estimate the expectation w.r.t. $f(\\theta)$. If the loss is the square error then we estimate  $f(\\Theta)$  with the posterior mean $ E_{\\Theta \\mid Y=y}(f(\\Theta))$. Simulate  $\\theta^{(t)} \\sim \\pi(\\cdot \\mid y)$, $t=1, \\ldots, T$  and compute\n",
    "$$\\hat{f}=\\frac{1}{T} \\sum_{t=1}^{T} f\\left(\\theta^{(t)}\\right) .$$\n",
    "\n",
    "> For example, if  $S \\in \\mathcal{B}_{\\Omega}$  and  $f(\\theta)=\\mathbb{I}_{\\theta \\in S}$  then  $\\hat{f}$  estimates  $\\pi(S \\mid y)$.\n",
    "\n",
    "We also commonly report posterior credible sets in order to quantify uncertainty. A level-$\\alpha$  Highest Posterior Density (HPD) credible set  $C_{\\alpha}$  satisfies\n",
    "\n",
    "$$\\int_{\\Omega \\cap C_{\\alpha}} \\pi(\\theta \\mid y) d \\theta=1-\\alpha, \\text { and } \\theta \\in C_{\\alpha}, \\theta^{\\prime} \\in \\Omega \\backslash C_{\\alpha} \\Rightarrow \\pi(\\theta \\mid y) \\geq \\pi\\left(\\theta^{\\prime} \\mid y\\right) .$$\n",
    "\n",
    "An HPD set (or general credible set with fixed posterior probability mass) is qualitatively\n",
    "different in meaning from a frequentist confidence interval since it involves the **the probability of parameters**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8a3cb-17a4-4057-a0dc-aa3ab2e2f77d",
   "metadata": {},
   "source": [
    "> The HPD set can be estimated from Monte Carlo samples  $\\theta^{(t)} \\sim \\pi(\\cdot \\mid y)$. Specifically, the HPD set is a Bayes estimator by supposing the action space is  $\\delta \\in \\Delta = \\left\\{A \\in \\mathcal{B}_{\\Omega}: \\pi(A \\mid y)=1-\\alpha\\right\\}$ and the loss  $L(\\Theta, \\delta)=\\mathbb{I}_{\\Theta \\notin \\delta}+|\\delta|$  where  $|\\delta|=\\int_{\\delta} d \\theta$. \n",
    ">\n",
    "> The expected posterior loss is minimised over the action space by  $\\delta^{*}=C_{\\alpha}$,  an HPD set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2462fb9-6e78-42f1-b238-e0ecb7ce6aca",
   "metadata": {},
   "source": [
    "After that, we obtain the **posterior predictive distribution** of the data $p\\left(y^{\\prime} \\mid y\\right)=\\int_{\\Omega} p\\left(y^{\\prime} \\mid \\theta\\right) \\pi(\\theta \\mid y) d \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da905d-23c8-4537-955e-87c162ee0215",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00c2d1-f293-48c9-b0ea-c2985a776320",
   "metadata": {},
   "source": [
    "In Bayesian inference we have, for model  $m$, a parameter prior  $\\Theta \\sim   \\pi(\\theta \\mid m)$ and an observation model  $Y \\sim p(y \\mid \\theta, m)$. The parameter space may vary from model to model. \n",
    "\n",
    "> The \"model\" is the joint model for the \"generative process\" for  $\\Theta, Y$, with joint density  $\\pi(\\theta \\mid m) p(y \\mid \\theta, m)$  and state space  $\\Omega_{m} \\times \\mathcal{Y}$. \n",
    "\n",
    "Conditioning on  $Y=y$  we get the posterior under model  $M=m$,\n",
    "\n",
    "$$\\pi(\\theta \\mid y, m)=\\frac{p(y \\mid \\theta, m) \\pi(\\theta \\mid m)}{p(y \\mid m)}$$\n",
    "\n",
    "with $p(y \\mid m)=\\int_{\\Omega_{m}} p(y \\mid \\theta, m) \\pi(\\theta \\mid m) d \\theta$. Similarly, the posterior model probability is $\\pi(m \\mid y)= p(y \\mid m) \\pi_{M}(m) /p(y)$ where  $\\pi_{M}(m)$  is our prior probability and $p(y)=\\sum_{m \\in \\mathcal{M}} p(y \\mid m) \\pi_{M}(m)$.\n",
    "\n",
    "\n",
    "> Under the  $0-1$  loss function with truth  $M$  and action  $\\delta \\in \\mathcal{M}$  the loss is  $L(M, \\delta)=\\mathbb{I}_{M \\neq \\delta}$.  The Bayes estimator is the\n",
    "maximum a posteriori (MAP) model.\n",
    "\n",
    "This can be seen that the expected posterior loss  $E_{M \\mid y}(L(M, \\delta))=1-\\pi(\\delta \\mid y)$  and this is minimised by the choice  $\\delta=m^{*}$  with  $m^{*}$  the mode, the most probable model a posteriori.\n",
    "\n",
    "> The model averaged posterior which allows for uncertainty is $\\pi(\\theta \\mid y)=\\sum_{m \\in \\mathcal{M}} \\pi(\\theta \\mid y, m) \\pi_{M}(m \\mid y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42d1c6-1c02-4ae4-a471-b6ac9837eb4f",
   "metadata": {},
   "source": [
    "### 2. MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cecd1ea-e004-47a9-bf58-2e3aaa0d8885",
   "metadata": {},
   "source": [
    "MCMC is a family of algorithms for simulating  $X_{0}, X_{1}, X_{2}, \\ldots \\xrightarrow{D} p $ for a user-defined probability distribution  $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8d540-00cb-4a1b-85f9-9f152d2e47f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e424db9-32c4-4d0c-9cbe-c9096fa25430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cd882-b79e-4b35-83ba-db006409ea22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5df08e7-576a-4bfd-a157-eef7ee4c1cf7",
   "metadata": {},
   "source": [
    "### 3. Coherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913560f-b74d-427e-b92f-889d1ffe6883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952f11a-0eac-45b9-8d85-799cb0009e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b327e44-f31c-479f-ad16-c6f39efef023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70882d72-f204-4184-9c97-5285cfcc9801",
   "metadata": {},
   "source": [
    "### 4. Model Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c334de1-ab4b-4eee-b5c2-f0c427ee8c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b54a61-8a3e-448e-8122-b705cd6e0318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd44f37-17f5-4d60-bf54-d7f8b920ede2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ebd1d-bd25-4714-b575-740836d1b55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "2. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "3. Geoff K. Nicholls. Bayes Methods.\n",
    "4. Patrick J. Laub. Hawkes Processes.\n",
    "5. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0933978-3704-44b8-939b-460c8e7e238e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
