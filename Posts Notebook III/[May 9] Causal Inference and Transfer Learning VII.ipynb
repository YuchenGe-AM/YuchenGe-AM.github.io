{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [May 9] Causal Inference and Transfer Learning VII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd53c1a-7b00-4471-90e9-65cd0ae3c359",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd183e-4cbd-4e45-8b0d-139b20201e96",
   "metadata": {},
   "source": [
    "### 1.Nonparametric Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e0e2d-ebee-41b2-b6a7-a35945ba6ee9",
   "metadata": {},
   "source": [
    ">\n",
    ">\n",
    ">\n",
    "\n",
    "\n",
    "Consider a nonparametric class of functions  $\\Theta$ (e.g. the Hölder class and Sobolev class) with $\\left\\{P_{\\theta}, \\theta \\in \\Theta\\right\\}$ and some semi-distance $d$. It's natural to consider the **maximum risk** $r\\left(\\hat{\\theta}_{n}\\right) \\triangleq \\sup _{\\theta \\in \\Theta} \\mathbf{E}_{\\theta}\\left[d^{2}\\left(\\hat{\\theta}_{n}, \\theta\\right)\\right]$. We can establish\n",
    "\n",
    "$$\n",
    "c \\psi_{n}^{2} \\leq \\sup _{\\theta \\in \\Theta} \\mathbf{E}_{\\theta}\\left[d^{2}\\left(\\hat{\\theta}_{n}, \\theta\\right)\\right] \\leq C \\psi_{n}^{2}\n",
    "$$\n",
    "for certain estimators  $\\hat{\\theta}_{n}$, certain positive sequences  $\\psi_{n} \\rightarrow 0$. Also we consider the **minimax risk** $\\mathcal{R}_{n}^{*} \\triangleq \\inf _{\\hat{\\theta}_{n}} \\sup _{\\theta \\in \\Theta}$ $\\mathbf{E}_{\\theta}\\left[d^{2}\\left(\\hat{\\theta}_{n}, \\theta\\right)\\right]$, and the corresponding \n",
    "\n",
    "$$ c \\leq \\liminf _{n \\rightarrow \\infty} \\psi_{n}^{-2} \\mathcal{R}_{n}^{*} \\leq  \\limsup _{n \\rightarrow \\infty} \\psi_{n}^{-2} \\mathcal{R}_{n}^{*} \\leq C$$\n",
    "for certain positive sequences  $\\psi_{n} \\rightarrow 0$ (**optimal rate of convergence**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f54672f-3dc2-41e8-9350-6df9feed2849",
   "metadata": {},
   "source": [
    "> A **rate optimal estimator** is an estimator $\\theta_{n}^{*}$ satisfying $\\sup _{\\theta \\in \\Theta} \\mathbf{E}_{\\theta}\\left[d^{2}\\left(\\theta_{n}^{*}, \\theta\\right)\\right] \\leq C^{\\prime} \\psi_{n}^{2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e35e1-efba-4691-a29e-e5a45bcce10a",
   "metadata": {},
   "source": [
    "We can also consider the asymptotically efficient estimator $\\theta_{n}^{*}$ where $ \\lim_{n \\rightarrow \\infty}  r\\left(\\theta_{n}^{*}\\right)/\\mathcal{R}_{n}^{*} = 1 $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70800f-61bc-4a0c-a47d-f0492e9cdeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84a596db-0830-4e78-aaed-a95a43cea0bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Bayesian Nonparametrics for Causal Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca27f15-242a-4873-aab6-b25fe6eb73db",
   "metadata": {},
   "source": [
    "In observational studies, the observed data is $\\left\\{Y_{i}, A_{i}, L_{i} ; i=1, \\cdots, N\\right\\}$ and covariates $L$ influence both $Y$ and $A$.\n",
    "\n",
    "> **Ignorability**: $\\{Y(0), Y(1)\\} \\perp A \\mid L$\n",
    ">\n",
    "> **Positivity**: $\\operatorname{Pr}(A=a \\mid L=\\ell)>0$. ( to identify the ACE )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452c7a0-9f66-49fd-9ddf-4a2da5f11e36",
   "metadata": {},
   "source": [
    "Recall the g-formula of static treatment\n",
    "\n",
    "$$f\\{Y(a)=y\\}=\\sum_{\\ell_{1}=0}^{1} \\cdots \\sum_{\\ell_{p}=0}^{1} f\\left(Y=y \\mid A=a, L_{1}=\\ell_{1}, \\ldots, L_{p}=\\ell_{p}\\right) \\times \\operatorname{Pr}\\left(L_{1}=\\ell_{1}, \\ldots, L_{p}=\\ell_{p}\\right) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0ce4c-cf4c-4ac8-863a-7dbedf49b29e",
   "metadata": {},
   "source": [
    "Define $\\left(A_{1}, \\cdots, A_{t}\\right)$ and $\\bar{L}_{t}=\\left(L_{1}, \\cdots, L_{t}\\right)$, we generalize the g-formula for timevarying treatments and confounders in a longitudinal data setting.\n",
    "\n",
    "> Under $Y\\left(a_{1}, \\ldots, a_{T}\\right) \\perp A_{t} \\mid \\bar{L}_{t}, \\bar{A}_{t-1}$ (ignorability), we know\n",
    ">\n",
    "> $$E\\left\\{Y\\left(a_{1}, \\cdots, a_{T}\\right)\\right\\} = \\int_{\\ell_{1}} \\cdots \\int_{\\ell_{T}} E\\left(Y \\mid \\bar{A}_{T}=\\bar{a}_{T}, \\bar{L}_{T}=\\bar{\\ell}_{T}\\right) \\times \\prod_{t=1}^{T} f\\left(\\ell_{t} \\mid \\bar{A}_{t-1}=\\bar{a}_{t-1}, \\bar{L}_{t-1}=\\ell_{t-1}\\right) d \\ell_{1} \\cdots d \\ell_{T} .$$\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb035621-b1d4-45ab-a09a-f5930dcd70ef",
   "metadata": {},
   "source": [
    "Consider propensity score $e(\\ell)=\\operatorname{Pr}(A=1 \\mid L=\\ell)$, with which we can estimate estimating ACE, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d450c3-0fa4-4482-acae-7918772194bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463dd6e-770a-439e-82d0-50dde6dbe4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732800b9-4746-4464-a93b-1367e0d10d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4433c2-cbd0-4571-99ea-bc3f6b29c8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b349cb-2941-4ea0-a171-e96981221b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "3. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "4. Geoff K. Nicholls. Bayes Methods.\n",
    "5. Patrick J. Laub. Hawkes Processes.\n",
    "6. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View.\n",
    "7. Jonas Peters. Elements of Causal Inference.\n",
    "8. Alessio Zanga. A Survey on Causal Discovery.\n",
    "9. Qing Zhou. Directed Mixed Graphs for Latent Variables.\n",
    "10. Peter Spirtes. Causation, Prediction, and Search. \n",
    "11. Thomas Richardson. Ancestral Graph Markov Models.\n",
    "12. Jonas Peters. Causal Inference Using Invariant Prediction.\n",
    "13. Jonathan Baxter. A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
