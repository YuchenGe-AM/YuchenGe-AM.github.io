{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Jun 13] Causal Inference and Transfer Learning XI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd53c1a-7b00-4471-90e9-65cd0ae3c359",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f8e0b-fb71-4da4-bd7d-511c1ea437e1",
   "metadata": {},
   "source": [
    "We follow the framework in $\\textit{Toward Better Practice of Covariate Adjustment\n",
    "in Analyzing Randomized Clinical Trials}$. First **observe** i.i.d. data for each patient $i\\in [n]$\n",
    "\n",
    "> $\\big($**potential responses**, **covariates for randomization** , **covariates**$\\big)$ = $\\left(Y_{i}^{(1)}, \\ldots, Y_{i}^{(k)}, Z_{i}, X_{i}\\right) \\sim \\left(Y^{(1)}, \\ldots, Y^{(k)}, Z, X\\right)$ with corresponding $k$-dim **treatment indicator** $A_{i}$, $i \\in [n]$, \n",
    "\n",
    "satisfying the following conditions\n",
    "\n",
    "1. (**consistency**) $Y_{i}=Y_{i}^{(t)}$ if and only if $A_{i}=a_{t}$;\n",
    "\n",
    "2. (**regular**) $\\Sigma_{X}=\\operatorname{var}\\left(X_{i}\\right)$ is positive definite;\n",
    "\n",
    "3. (**randomization**) $ \\left\\{A_{i} \\right\\} \\perp  \\left\\{\\left(Y_{i}^{(1)}, \\ldots, Y_{i}^{(k)}, X_{i}\\right)\\right\\} \\mid \\left\\{Z_{i}\\right\\}$; $n_{t}(z) / n(z) \\rightarrow \\pi_{t}$  almost surely.\n",
    "\n",
    "Here $n(z)$ = # patients with  $Z=z$  and  $n_{t}(z)$ = # patients with  $Z=z$  and treatment $ t $. Then we set  pre-specified treatment assignment proportions as $P\\left(A_{i}=a_{t} \\mid Z_{1}, \\ldots, Z_{n}\\right)=\\pi_{t}$ where $\\pi_t \\in (0,1)$, $t \\in [k]$, satisfy $\\sum_{t=1}^{k} \\pi_{t}=1$. We are interested in "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c5e81-e2da-48a0-9e77-105d3449f57a",
   "metadata": {},
   "source": [
    "> functions $\\theta_{t}-\\theta_{s}$, $\\theta_{t} / \\theta_{s}$, or $\\left\\{\\theta_{t} /\\left(1-\\theta_{t}\\right)\\right\\} /\\left\\{\\theta_{s} /\\left(1-\\theta_{s}\\right)\\right\\}$ where $\\theta_{t}=E\\left(Y^{(t)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb6944-a657-4534-9a6a-c562623081de",
   "metadata": {},
   "source": [
    "Let  $\\theta=\\left(E\\left(Y^{(1)}, \\ldots, E\\left(Y^{(k)}\\right)\\right)^{\\top}\\right.$  and  $\\beta=\\sum_{t=1}^{k} \\pi_{t} \\beta_{t}$ where $\\beta_{t}=\\Sigma_{X}^{-1} \\operatorname{cov}\\left(X_{i}, Y_{i}^{(t)}\\right), t=1, \\ldots, k$. Then (from lemma 2 in [14]) we know that if $P\\left(A_{i}=a_{t} \\mid Z_{1}, \\ldots, Z_{n}\\right)=\\pi_{t}$ holds then \n",
    "\n",
    "$$(\\theta, \\beta)=\\arg \\min _{(\\vartheta, \\not \\beta)} E\\left[\\left\\{Y_{i}-\\vartheta^{\\top} A_{i}-\\not \\beta^{\\top}\\left(X_{i}-\\mu_{X}\\right)\\right\\}^{2}\\right] \\quad \\& \\quad \\left(\\theta, \\beta_{1}, \\ldots, \\beta_{k}\\right)=\\arg \\min _{\\left(\\vartheta, \\not \\beta_{1}, \\ldots, \\not \\beta_{k}\\right)} E\\left[\\left\\{Y_{i}-\\vartheta^{\\top} A_{i}-\\sum_{t=1}^{k} \\not \\beta_{t}^{\\top}\\left(X_{i}-\\mu_{X}\\right) I\\left(A_{i}=a_{t}\\right)\\right\\}^{2}\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f3e43-f00e-4f35-9e4a-9b413569e4a5",
   "metadata": {},
   "source": [
    "Both can be proved via checking the optimality points $(\\theta, \\beta)$. This justifies that we use the following working models\n",
    "\n",
    "> 1. (**homogeneous**) $E\\left(Y_{i} \\mid A_{i}, X_{i}\\right)=\\vartheta^{T} A_{i}+\\not \\beta^{T}\\left(X_{i}-\\mu_{X}\\right)$;\n",
    ">\n",
    "> 2. (**heterogeneous**) $E\\left(Y_{i} \\mid A_{i}, X_{i}\\right)=\\vartheta^{T} A_{i}+\\sum_{t=1}^{k} \\not \\beta_{t}^{T}\\left(X_{i}-\\mu_{X}\\right) I\\left(A_{i}=a_{t}\\right)$.\n",
    "\n",
    "The paper then investigates **asypototic behavior** of a general class of estimators\n",
    "of the form\n",
    "\n",
    "$$  \\hat{\\theta}\\left(\\hat{b}_{1}, \\ldots, \\hat{b}_{k}\\right)=\\left(\\bar{Y}_{1}-\\hat{b}_{1}^{T}\\left(\\bar{X}_{1}-\\bar{X}\\right) \\ldots, \\bar{Y}_{k}-\\hat{b}_{k}^{T}\\left(\\bar{X}_{k}-\\bar{X}\\right)\\right)^{T} $$\n",
    "\n",
    "where (fixed or dependent) $\\hat{b}_{t}$'s have the same dimension as  $X$  and $\\bar{X}_{t}$ $(\\bar{Y}_{t})$ =  sample mean of  $X_{i}$'s ($Y_{i}$'s) under treatment $t$. This is a **generalization of least\n",
    "squares estimators**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e77f4a-cd61-4d2c-9f07-bcbaf2655cdc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "Consider **another** trial \n",
    "\n",
    "> $\\big($**potential responses**, **covariates for randomization** , **covariates**$\\big)$ = $\\left(\\tilde{Y}_{i}^{(1)}, \\ldots, \\tilde{Y}_{i}^{(k)}, \\tilde{Z}_{i}, \\tilde{X}_{i}\\right)$ with corresponding $k$-dim **treatment indicator** $\\tilde{A}_{i}$, $i=1, \\ldots, \\tilde{n}$.\n",
    "\n",
    "Fix some $C_1, C_2 > 0$, then we know  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\theta, \\beta, \\tilde{\\theta}, \\tilde{\\beta}) & = \\arg \\min_{(\\vartheta, \\not \\beta, \\tilde{\\vartheta}, \\tilde{\\not \\beta})} E\\left[ C_1 \\left\\{Y_{i}- \\vartheta^{\\top} A_{i} - \\not \\beta^{\\top}\\left(X_{i}-\\mu_{X}\\right) \\right\\}^{2} + C_2 \\left\\{ \\tilde{Y}_j - \\tilde{\\vartheta}^{\\top} \\tilde{A}_{j} - \\tilde{\\not \\beta}^{\\top}\\left(\\tilde{X}_{j}-\\mu_{\\tilde{X}}\\right) \\right\\}^{2} \\right] \\\\\n",
    "& \\approx \\arg \\min_{(\\vartheta, \\not \\beta, \\tilde{\\vartheta}, \\tilde{\\not \\beta})} \\frac{1}{n+n_1} \\big( C_1 \\sum_n \\left\\{Y_{i}- \\vartheta^{\\top} A_{i} - \\not \\beta^{\\top}\\left(X_{i}-\\mu_{X}\\right) \\right\\}^{2} + C_2 \\sum_{n_1} \\left\\{ \\tilde{Y}_j - \\tilde{\\vartheta}^{\\top} \\tilde{A}_{j} - \\tilde{\\not \\beta}^{\\top}\\left(\\tilde{X}_{j}-\\mu_{\\tilde{X}}\\right) \\right\\}^{2} \\big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be proved via seeing that optimality points satisfy  \n",
    "$$\\begin{aligned}\n",
    "E\\left[I\\left(A_{i} =a_{t}\\right)\\left\\{Y_{i}-\\theta^{\\top} A_{i}-\\beta^{\\top}\\left(X_{i}-\\mu_{X}\\right)\\right\\}\\right] & =0, \\\\\n",
    "E\\left[\\left(X_{i}-\\mu_{X}\\right)\\left\\{Y_{i}-\\theta^{\\top} A_{i}-\\beta^{\\top}\\left(X_{i}-\\mu_{X}\\right)\\right\\}\\right] & =0 .\n",
    "\\end{aligned}$$\n",
    "and correspondingly the equations for $\\tilde{\\vartheta}, \\tilde{\\not \\beta}$. Here are some remarks.\n",
    "\n",
    "> 1. We don't rult out the possibiliy that $X$ and $\\tilde{X}$ share some covarites, which enforces that $\\not \\beta$ and $\\tilde{\\not \\beta}$ share some paramter space.\n",
    ">\n",
    "> 2. An intuitive simple model is to consider \n",
    ">\n",
    "> $$ Y=\\beta_{1} \\cdot A_{1}+\\gamma_{0} \\mathbf{X}_{0}+\\gamma_{1} \\mathbf{X}_{1}+\\epsilon_{1}\\quad \\& \\quad Y=\\beta_{2} \\cdot A_{2}+\\gamma_{0} \\mathbf{X}_{0}+\\gamma_{2} \\mathbf{X}_{2}+\\epsilon_{2}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebb520-fb7f-46f0-8e83-08f7d49241f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042ee5f8-d50c-44df-b613-f3141f0a88c7",
   "metadata": {},
   "source": [
    "0. covariate adjustment, invariant learning; consistency, efficiency | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f94bd-ec6a-4551-aa4a-62eedd8cc11e",
   "metadata": {},
   "source": [
    "1. Causal inference by using invariant prediction: identification and confidence intervals\n",
    "2. Anchor regression: Heterogeneous data meet causality\n",
    "3. Estimation of Causal Effects with Multiple Treatments: A Review and New Ideas\n",
    "4. Causal inference under multiple versions of treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f7440-e5e5-45e3-88b2-706062916d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "3. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "4. Geoff K. Nicholls. Bayes Methods.\n",
    "5. Patrick J. Laub. Hawkes Processes.\n",
    "6. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View.\n",
    "7. Jonas Peters. Elements of Causal Inference.\n",
    "8. Alessio Zanga. A Survey on Causal Discovery.\n",
    "9. Qing Zhou. Directed Mixed Graphs for Latent Variables.\n",
    "10. Peter Spirtes. Causation, Prediction, and Search. \n",
    "11. Thomas Richardson. Ancestral Graph Markov Models.\n",
    "12. Jonas Peters. Causal Inference Using Invariant Prediction.\n",
    "13. Jonathan Baxter. A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling.\n",
    "14. Ting Ye. Toward Better Practice of Covariate Adjustment in Analyzing Randomized Clinical Trials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
