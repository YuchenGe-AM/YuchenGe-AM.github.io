{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Feb 21] Causal Inference and Transfer Learning\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c723a98-16e1-4224-997f-2b80431b4d02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Causal Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacddc8c-47d8-4ad9-aa7c-ed309b5bbbcf",
   "metadata": {},
   "source": [
    "In **causal inference**, we assume that **for a graph there is at most one edge between any two vertices [ Peter Spirtes. Causation, Prediction, and Search. P28 ]**. Recall that \n",
    "\n",
    "> A SCM $\\mathfrak{C}:=\\left(\\mathbf{S}, P_{\\mathrm{N}}\\right)$  consists of $X_{j}:=f_{j}\\left(\\mathbf{P A}_{j}, N_{j}\\right)$ on a DAG.\n",
    ">\n",
    "> An intervention $P_{\\mathbf{X}}^{\\tilde{\\mathbb{C}}}=: P_{\\mathbf{X}}^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{f}\\left(\\widetilde{\\mathbf{P A}_{k}}, \\tilde{N}_{k}\\right)\\right)}$ is via replacing $X_k$ with $\\tilde{f}\\left(\\widetilde{\\mathbf{P A}}_{k}, \\tilde{N}_{k}\\right)$.\n",
    ">\n",
    "> A counterfactual SCM is $\\mathfrak{C}_{\\mathbf{X}=\\mathbf{x}}:=\\left(\\mathbf{S}, P_{\\mathbf{N}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}\\right)$ where $P_{\\mathbf{N}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}:=P_{\\mathbf{N} \\mid \\mathbf{X}=\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15312391-dc98-436d-ad76-d5cce6a4f31e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aae6a87-8808-4a92-be15-c517d63e227a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9bbf9-c040-4158-b7c4-6f533e2bc2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a6066-9047-488c-8aec-5626b8bec05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec66c49-c3fb-41b5-add5-aa93a4f11ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b92e7bfd-05d9-4727-9d73-037d1a825413",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bf885-78c7-44b2-b10a-80841cf79827",
   "metadata": {},
   "source": [
    "Then we shall formulate the causal discovery problem. Let  $\\mathbf{G}$  be the set of graphs defined over the variables $ \\mathbf{V}$  of a data set  $\\mathbf{D}$  and  $G^{*} \\in \\mathbf{G}$.\n",
    "\n",
    "> ( **Causal Discovery Problem** ). The causal discovery problem consists in recovering $G^{*}$  from given $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0bdd1-08ee-4fea-bbed-abc391018fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b8def-934a-4a5b-83d2-ada44aef8d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b90c525-fafc-4362-b64c-1e3180d85bb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "091f423a-b8e3-40ce-b565-463a5fdbdd54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> [ **Thomas Richardson. Ancestral Graph Markov Models. Appendix.** ] Let  $\\mathcal{E}=\\{-, \\leftarrow, \\rightarrow, \\leftrightarrow\\}$  be the set of edges.  A mixed graph is a pair $\\mathscr{g}=(V, E)$  with $E: V \\times V \\rightarrow \\mathcal{P}(\\mathcal{E})$ subject to \n",
    "> $$\\begin{aligned}\n",
    "E(\\alpha, \\alpha)=\\varnothing, & \\quad -\\in E(\\alpha, \\beta) \\Longleftrightarrow-\\in E(\\beta, \\alpha), \\\\\n",
    "\\leftarrow \\in E(\\alpha, \\beta) \\Longleftrightarrow \\rightarrow \\in E(\\beta, \\alpha), & \\quad   \\leftrightarrow \\in E(\\alpha, \\beta) \\Longleftrightarrow \\leftrightarrow \\in E(\\beta, \\alpha) .\n",
    "\\end{aligned}$$\n",
    ">\n",
    "> An **ADMG** is a mixed graph containing no directed cycles, which can be viewed as a DAG adding  bidirected arrows ($\\leftrightarrow$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2435c-f61c-4290-aaf9-758fff79cb95",
   "metadata": {},
   "source": [
    "If $a \\leftrightarrow b$, then $a$ is a sibling of $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19cccb-d422-4572-b709-d4dc470b6659",
   "metadata": {},
   "source": [
    "> ( **Latent projection of a DAG** )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4e08a-dbe4-44ea-ac2f-15b690a4f22a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76512fdb-e3a1-4134-8799-8f2a3df0d831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefff2c-b20e-4a11-a0df-9962d50bfa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42932347-5a7e-47d0-ae9f-c766eedc9a79",
   "metadata": {},
   "source": [
    "### 2. Invariant Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49700d1-c1a1-4e57-a5a3-a6b3d7a3a595",
   "metadata": {},
   "source": [
    "Assume that we want to predict a target $Y \\in \\mathbb{R}$ from some predictor variable $\\mathbf{X} \\in \\mathbb{R}^{p}$.\n",
    "\n",
    "> DG has training data from $\\left(\\mathbf{X}^{1}, Y^{1}\\right), \\ldots,\\left(\\mathbf{X}^{D}, Y^{D}\\right), \\mathbf{X}^{D+1}$.\n",
    ">\n",
    "> AMTL has training data from $\\left(\\mathbf{X}^{1}, Y^{1}\\right), \\ldots,\\left(\\mathbf{X}^{D}, Y^{D}\\right), \\mathbf{X}^{D}$.\n",
    ">\n",
    "> SMTL has training data from $\\left(\\mathbf{X}^{1}, Y^{1}\\right), \\ldots,\\left(\\mathbf{X}^{D}, Y^{D}\\right), \\mathbf{X}^{1}, \\ldots, \\mathbf{X}^{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16315f3-4b9a-4261-bca8-02934c86fc7f",
   "metadata": {},
   "source": [
    "Consider a transfer learning regression problem with $\\mathbb{P}^{1}, \\ldots, \\mathbb{P}^{D}$, where  $\\left(\\mathbf{X}^{k}, Y^{k}\\right) \\sim   \\mathbb{P}^{k}$ , $\\forall k \\in\\{1, \\ldots, D\\}$. We now formulate our main assumptions.\n",
    "\n",
    "> (A1) $\\exists$ invariant set $S^{*} \\subseteq\\{1, \\ldots, p\\}$  s.t. $Y^{k}|\\mathbf{X}_{S^{*}}^{k} \\stackrel{d}{=} Y^{k^{\\prime}}| \\mathbf{X}_{S^{*}}^{k^{\\prime}} \\quad \\forall k, k^{\\prime} \\in\\{1, \\ldots, D\\}$.\n",
    ">\n",
    "> (A1') This invariance also holds in the test task  $T$, i.e., the equations in (A1) holds for all  $k, k^{\\prime} \\in\\{1, \\ldots, D, T\\}$.\n",
    ">\n",
    "> (A2) $\\exists \\alpha \\in \\mathbb{R}^{\\left|S^{*}\\right|}$  and $\\epsilon$  s.t. $Y^{k}=\\alpha^{t} \\mathbf{X}_{S^{*}}^{k}+\\epsilon^{k}$, with  $\\epsilon^{k} \\perp \\mathbf{X}_{S^{*}}^{k}$  and $\\epsilon^{k} \\stackrel{d}{=} \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4ca21-6ac1-4647-8932-4f57dde315b6",
   "metadata": {},
   "source": [
    "Set $\\mathcal{E}_{\\mathbb{P} T}(\\beta)=\\mathbb{E}_{\\left(\\mathbf{X}^{T}, Y^{T}\\right) \\sim \\mathbb{P}^{T}}\\left(Y^{T}-\\beta^{t} \\mathbf{X}^{T}\\right)^{2}$. Consider $\\left(\\beta^{C S\\left(S^{*}\\right)}\\right)^{t} \\mathbf{x} = \\mathbb{E}\\left[Y^{1} \\mid \\mathbf{X}_{S^{*}}^{1}=\\mathbf{x}_{S^{*}}\\right]$ where we add zeros to the dimensions corresponding to covariates outside of $S^∗$.\n",
    "\n",
    "> Consider  $D$  tasks  $\\left(\\mathbf{X}^{1}, Y^{1}\\right) \\sim \\mathbb{P}^{1}, \\ldots,\\left(\\mathbf{X}^{D}, Y^{D}\\right) \\sim \\mathbb{P}^{D}$  that satisfy (A1). Then \n",
    ">\n",
    ">  $$\\beta^{C S\\left(S^{*}\\right)} \\in \\underset{\\beta \\in \\mathbb{R}^{p}}{\\arg \\min } \\sup _{\\mathbb{P}^{T} \\in \\mathcal{P}} \\mathcal{E}_{\\mathbb{P}^{T}}(\\beta),$$\n",
    ">\n",
    "> where  $\\mathcal{P}$  contains all distributions over  $\\left(\\mathbf{X}^{T}, Y^{T}\\right)$, $T=D+  1$, that are absolutely continuous w.r.t. some product measure  $\\mu$  and satisfy  $Y^{T} |\\mathbf{X}_{S^{*}}^{T} \\stackrel{d}{=} Y^{1} | \\mathbf{X}_{S^{*}}^{1}$.\n",
    "\n",
    "Consider any $f$. It suffices to show that $\\forall \\mathbb{Q} \\in \\mathcal{P}$, $\\exists \\mathbb{P} \\in \\mathcal{P}$  such that\n",
    "\n",
    "$$\\int(y-f(\\mathbf{x}))^{2} d \\mathbb{P} \\geq \\int\\left(y-f_{S^{*}}(\\mathbf{x})\\right)^{2} d \\mathbb{Q}.$$\n",
    "\n",
    "Specifically, we have \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\int(y-f(\\mathbf{x}))^{2} d \\mathbb{P} & =\\int_{\\mathbf{x}_{N}} \\int_{\\mathbf{x}_{S^{*}}, y}\\left(y-f\\left(\\mathbf{x}_{S^{*}}, \\mathbf{x}_N\\right)\\right)^{2} p\\left(\\mathbf{x}_{S^{*}}, y\\right) d \\mathbf{x}_{S^{*}} d y p\\left(\\mathbf{x}_{N}\\right) d \\mathbf{x}_{N} \\\\\n",
    "& \\geq \\int_{\\mathbf{x}_{N}} \\int_{\\mathbf{x}_{S^{*}, y}}\\left(y-f_{S^{*}}\\left(\\mathbf{x}_{S^{*}}\\right)\\right)^{2} p\\left(\\mathbf{x}_{S^{*}}, y\\right) d \\mathbf{x}_{S^{*}} d y p\\left(\\mathbf{x}_{N}\\right) d \\mathbf{x}_{N} \\\\\n",
    "& = \\int_{\\mathbf{x}, y}\\left(y-f_{S^{*}}\\left(\\mathbf{x}_{S^{*}}\\right)\\right)^{2} q\\left(\\mathbf{x}_{S^{*}}, \\mathbf{x}_{N}, y\\right) d \\mathbf{x}_{S^{*}} d y d \\mathbf{x}_{N} \\\\\n",
    "& =\\int\\left(y-f_{S^{*}}(\\mathbf{x})\\right)^{2} d \\mathbb{Q}.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b8cc2-1021-4ed5-9f54-f184c978646c",
   "metadata": {},
   "source": [
    "When the test tasks become diverse, predicting using above outperforms pooling on average over all tasks. Specifically, set $Y^{k}=\\alpha^{t}\\mathbf{X}_{S^{*}}^{k}+\\epsilon^{k}$ and $Z^{k}=\\gamma^{k} Y^{k}+\\eta^{k}$. Suppose $\\gamma^{1}, \\ldots, \\gamma^{D}, \\gamma^{T}=\\gamma^{D+1}$  are i.i.d. with mean zero and variance  $\\Sigma^{2}>0$, then for $\\beta^{C S}=\\left(\\beta_{S^{*}}^{C S}, \\beta_{Z}^{C S}\\right)$,\n",
    "\n",
    "$$\\mathbb{E}_{\\gamma^{T}}\\left(\\mathcal{E}_{\\mathbb{P} T}\\left(\\beta^{C S}\\right)\\right) \\geq \\mathbb{E}_{\\gamma^{T}}\\left(\\mathcal{E}_{\\mathbb{P} T}\\left(\\beta^{C S\\left(S^{*}\\right)}\\right)\\right).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78404a3-0b92-440c-bbd1-f860adb3f4be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In MTL, a labeled sample $\\left(\\mathbf{X}_{i}^{T}, Y_{i}^{T}\\right)_{i=1}^{n_{T}}$ is available from the test task.\n",
    "\n",
    "Consider\n",
    "\n",
    "$$q^{k}\\left(\\mathbf{x}_{S^{*}}, \\mathbf{x}_{N}, y\\right):=p^{k}\\left(\\mathbf{x}_{S^{*}}, y\\right) p^{T}\\left(\\mathbf{x}_{N} \\mid \\mathbf{x}_{S^{*}}, y\\right) .$$\n",
    "\n",
    "We trivially have  $q^{T}=p^{T}$ and $q^{k}\\left(\\mathbf{x}_{S^{*}}, y\\right) =p^{k}\\left(\\mathbf{x}_{S^{*}}, y\\right)$.\n",
    "\n",
    "\n",
    "Second, we prove that the conditional  $q^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}, \\mathbf{x}_{N}\\right)$  is the same in all tasks. Indeed, by applying Bayes' rule:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}, \\mathbf{x}_{N}\\right) & =q^{k}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right) \\frac{q^{k}\\left(y, \\mathbf{x}_{S^{*}}\\right)}{q^{k}\\left(\\mathbf{x}_{S^{*}}, \\mathbf{x}_{N}\\right)} \\\\\n",
    "& =p^{T}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right) \\frac{q^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}\\right)}{q^{k}\\left(\\mathbf{x}_{N} \\mid \\mathbf{x}_{S^{*}}\\right)} \\\\\n",
    "& =p^{T}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right) \\frac{p^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}\\right)}{\\int_{\\mathbb{R}} q^{k}\\left(y, \\mathbf{x}_{N} \\mid \\mathbf{x}_{S^{*}}\\right) d y} \\\\\n",
    "& =p^{T}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right) \\frac{p^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}\\right)}{\\int_{\\mathbb{R}} q^{k}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right) q^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}\\right) d y} \\\\\n",
    "& =p^{T}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right) \\frac{p^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}\\right)}{\\int_{\\mathbb{R}} p^{T}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right) p^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}\\right) d y} .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "We have used the fact that  $q^{k}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right)=p^{T}\\left(\\mathbf{x}_{N} \\mid y, \\mathbf{x}_{S^{*}}\\right) $. Therefore,\n",
    "\n",
    "> ( **Correctness of transfer** ) Let  $S^{*}$  be an invariant set verifying (A1) and (A2). $\\exists q: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{+}$ s.t.\n",
    ">\n",
    "> 1.  $q^{k}\\left(\\mathbf{x}_{S^{*}}, y\\right)=p^{k}\\left(\\mathbf{x}_{S^{*}}, y\\right)$.\n",
    ">\n",
    "> 2.  $q^{k}\\left(y \\mid \\mathbf{x}_{S^{*}}, \\mathbf{x}_{N}\\right)= p^{T}\\left(y \\mid \\mathbf{x}_{S^{*}}, \\mathbf{x}_{N}\\right) = q\\left(y \\mid \\mathbf{x}_{S^{*}}, \\mathbf{x}_{N}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d322d-2493-4876-8e42-716d7ddc998c",
   "metadata": {},
   "source": [
    "The goal of learning the regression model from  $Y$  on  $\\mathbf{X}_{S^{*}},  \\mathbf{X}_{N}$  in  $\\mathbb{P}^{T}$  coincides with regression learning in  $\\mathbb{Q}^{T}$. Property 2 implies that we can pool the data from all tasks  $\\mathbb{Q}^{k}$. However, property 1 implies that we only have access to the marginal  $\\left(\\mathbf{X}_{S^{*}}^{k}, Y^{k}\\right)$  from  $\\mathbb{Q}^{k}$. Therefore, it suffices to seek methods from **missing data approach**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d8e0d-71cf-450c-b7ec-bf60c09844f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Above, we've demonstrated the significance of $S^*$. Consider an SEM over variables  $(\\mathbf{X}, Y)$. Suppose further that $\\mathbb{P}^{1}, \\ldots, \\mathbb{P}^{D}$  are intervention distributions of an underlying SEM with graph structure  $\\mathcal{G}$. \n",
    "\n",
    "> If the target variable has not been intervened on, then $S^{*}:=\\mathbf{P A}_{Y}^{\\mathcal{G}}$  satisfies Assumptions (A1) and (A1').\n",
    "\n",
    "This means that as long as the interventions will not take place at the target variable, the set  $S^{*}$  of causal parents will satisfy Assumptions (A1) and (A1')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e043e8-b2be-481f-a92a-bfe394cac7a6",
   "metadata": {},
   "source": [
    "> Intuitively, we may consider $\\mathrm{HSIC}_{b}\\left(\\left(R_{\\beta^{C S(S)}, i}, K_{i}\\right)_{i=1}^{n}\\right)$ where $R_{\\beta^{C S(S)}}=Y-(\\beta^{C S(S)})^{t} \\mathbf{X}$. After that, return $\\hat{S}= S$ with $\\min \\text { MSE }$ in an aim to maximize predictive accuracy on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "3. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "4. Geoff K. Nicholls. Bayes Methods.\n",
    "5. Patrick J. Laub. Hawkes Processes.\n",
    "6. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View.\n",
    "7. Jonas Peters. Elements of Causal Inference.\n",
    "8. Alessio Zanga. A Survey on Causal Discovery.\n",
    "9. Qing Zhou. Directed Mixed Graphs for Latent Variables.\n",
    "10. Peter Spirtes. Causation, Prediction, and Search. \n",
    "11. Thomas Richardson. Ancestral Graph Markov Models.\n",
    "12. Jonas Peters. Causal Inference Using Invariant Prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
