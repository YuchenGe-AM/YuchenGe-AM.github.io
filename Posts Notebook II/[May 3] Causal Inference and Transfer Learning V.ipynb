{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [May 3] Causal Inference and Transfer Learning V\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd53c1a-7b00-4471-90e9-65cd0ae3c359",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1a35b-c1a6-4040-a77b-244b6ce58dfa",
   "metadata": {},
   "source": [
    "### 1. Sklar's theorem\n",
    "\n",
    "An **n-copula**  $C\\left(u_{1}, \\ldots, u_{n}\\right)$ $[0,1]^{n} \\rightarrow[0,1]$  is a multivariate cdf with uniform univariate marginals: $C\\left(u_{1}, u_{2}, \\ldots, u_{n}\\right)$ = $P\\left(U_{1} \\leq u_{1}, U_{2} \\leq u_{2}, \\ldots, U_{n} \\leq u_{n}\\right)$ where $U_i \\sim U[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7db5b-daa2-4324-a046-12a36500767a",
   "metadata": {},
   "source": [
    "> (**Sklar's theorem**) Let  $H$  be an n-dimensional distribution function with marginal distribution functions  $F_{1}, F_{2}, \\ldots, F_{n}$. Then $\\exists$ an  n-copula  $C$  s.t. for all  $\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right) \\in[-\\infty, \\infty]^{n}$,\n",
    ">\n",
    "> $$H\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)=C\\left(F_{1}\\left(x_{1}\\right), F_{2}\\left(x_{2}\\right), \\ldots, F_{n}\\left(x_{n}\\right)\\right)=C\\left(u_{1}, u_{2}, \\ldots, u_{n}\\right)$$\n",
    ">\n",
    "> where $C$ is uniquely determined on $\\operatorname{Range}\\left(F_{1}\\right) \\times \\cdots \\times \\operatorname{Range}\\left(F_{d}\\right)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57116bbd-7246-4bac-a2a3-bc6ce8ff5e99",
   "metadata": {},
   "source": [
    "**Proof.** Details can be seen in [2012, Nonlinear Analysis: Theory, Methods & Applications, 75(2), 769-774]. Define $\\varphi_{\\varepsilon}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$  by\n",
    "\n",
    "$$\\varphi_{\\varepsilon}(\\mathbf{x}):=\\frac{1}{\\varepsilon^{d}} \\varphi\\left(\\frac{\\mathbf{x}}{\\varepsilon}\\right) \\quad \\text{where} \\quad \\varphi(\\mathbf{x}):=k \\exp \\left(\\frac{1}{|\\mathbf{x}|^{2}-1}\\right) \\mathbf{1}_{B_{1}(\\mathbf{0})}(\\mathbf{x}).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa3ad4-02c8-4442-81ca-1bce879e36d6",
   "metadata": {},
   "source": [
    "Consider $H_{n}(\\mathbf{x}):=\\int_{\\mathbb{R}^{d}} H(\\mathbf{x}-\\mathbf{y}) \\varphi_{1 / n}(\\mathbf{y}) d \\mathbf{y}=\\int_{\\mathbb{R}^{d}} \\varphi_{1 / n}(\\mathbf{x}-\\mathbf{y}) H(\\mathbf{y}) d \\mathbf{y}$, and we know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7d802-6034-4070-9634-c309657621d0",
   "metadata": {},
   "source": [
    "> If $H$ is continuous at $\\mathbf{x} \\in \\mathbb{R}^{d}$, then $\\lim _{n \\rightarrow+\\infty} H_{n}(\\mathbf{x})=H(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4929a4b7-a8e1-41f9-a38e-c7dcab4ecb63",
   "metadata": {},
   "source": [
    "The continuous case (proved stratforward via inverse) ensures that $\\exists$ a  d -copula  $C_{n}$ s.t.\n",
    "\n",
    "$$H_{n}(\\mathbf{x})=C_{n}\\left(F_{n, 1}\\left(x_{1}\\right), \\ldots, F_{n, d}\\left(x_{d}\\right)\\right).$$\n",
    "\n",
    "Because of the compactness of  $\\mathscr{C}_{d}$ (set of all copulas), Ascoli–Arzela Theorem shows that $\\exists$ a subsequence  $\\left(C_{n(k)}\\right)_{k \\in \\mathbb{N}} \\subset\\left(C_{n}\\right)_{n \\in \\mathbb{N}}$  that converges to a copula  $C$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9619cb-6713-4408-9036-0de60b4fc36d",
   "metadata": {},
   "source": [
    "### 2. Theory of Transfer Learning and Sample Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d5196-6a70-4839-b751-613784b876b1",
   "metadata": {},
   "source": [
    "Let $\\mathcal{F}$ = $\\{$ tasks mappings  $\\mathbb{R}^{r} \\rightarrow \\mathbb{R}$ $\\}$ and  $\\mathcal{H}$  = $\\{$ features mapping  $\\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{r}$ $\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d4adc-31d6-4774-8afb-46f154f06622",
   "metadata": {},
   "source": [
    "For a particular task $j\\in[t]$, we observe multiple i.i.d data pairs  $\\left\\{\\left(\\mathbf{x}_{j i}, y_{j i}\\right)\\right\\} \\sim \\mathbb{P}_{j}$, supported over  $\\mathcal{X} \\times \\mathcal{Y}$  and defined as follows:\n",
    "\n",
    "$$\\mathbb{P}_{j}(\\mathbf{x}, y)=\\mathbb{P}_{f_{j}^{\\star} \\circ \\mathbf{h}^{\\star}}(\\mathbf{x}, y)=\\mathbb{P}_{\\mathbf{x}}(\\mathbf{x}) \\mathbb{P}_{y \\mid \\mathbf{x}}\\left(y \\mid f_{j}^{\\star} \\circ \\mathbf{h}^{\\star}(\\mathbf{x})\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e61fb0-2064-4348-8e6e-2051fbba20b3",
   "metadata": {},
   "source": [
    "Notice that $\\mathbf{h}^{\\star}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{r}$ is the **shared feature representation**. We consider transfer learning methods in two phases: $t$ tasks with $n$ samples per task and $m$ fresh samples from the $0$-th task, with the corresponding risks being \n",
    "\n",
    "$$\\hat{R}_{\\text {train }}(\\mathbf{f}, \\mathbf{h}):=\\frac{1}{n t} \\sum_{j=1}^{t} \\sum_{i=1}^{n} \\ell\\left(f_{j} \\circ \\mathbf{h}\\left(\\mathbf{x}_{j i}\\right), y_{j i}\\right) \\quad \\text{ and } \\quad \\hat{R}_{\\text {test }}(f, \\mathbf{h}):=\\frac{1}{m} \\sum_{i=1}^{m} \\ell\\left(f \\circ \\mathbf{h}\\left(\\mathbf{x}_{0 i}\\right), y_{0 i}\\right). \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609776a6-b418-4bc1-a74a-997c0ddc68a3",
   "metadata": {},
   "source": [
    "Two estimators follow\n",
    "\n",
    "> 1. shared data representation: $\\hat{\\mathbf{h}}=\\operatorname{argmin}_{\\mathbf{h} \\in \\mathcal{H}} \\min _{\\mathbf{f} \\in \\mathcal{F}^{\\otimes t}} \\hat{R}_{\\text {train }}(\\mathbf{f}, \\mathbf{h})$ .\n",
    ">\n",
    "> 2. underlying function $f_{0}^{\\star}$: $\\hat{f}_{0}=\\operatorname{argmin}_{f \\in \\mathcal{F}} \\hat{R}_{\\text {test }}(f, \\hat{\\mathbf{h}})$.\n",
    "\n",
    "We consider **Transfer Learning Risk** $R_{\\text {test }}\\left(\\hat{f}_{0}, \\hat{\\mathbf{h}}\\right)-R_{\\text {test }}\\left(f_{0}^{\\star}, \\mathbf{h}^{\\star}\\right)$.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b45f3e-fb3e-4949-b44c-f9aa9e2ad600",
   "metadata": {},
   "source": [
    "Consider a vector valued random variable,  $\\mathrm{U}=   \\left\\{X_{1}, \\ldots, X_{n}\\right\\}$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6b7aa-cf75-4349-90da-d3a12de46049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "461f97af-5c90-490d-94cf-ac44666ea0be",
   "metadata": {},
   "source": [
    "### 3. Multivariate Point Processes and Copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7aba86-b496-4a9d-a8f3-dde22c367ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9329f-4367-45aa-a84d-01f11a71aa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e612f-f8f3-4b8f-9644-c31e85e53e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0fc71-1bb6-4f2f-b0fb-e48ef8410977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5df143-5735-42f6-a088-a865dfa05c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb5e59-30fb-426d-ba35-15e503414e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99fd183e-4cbd-4e45-8b0d-139b20201e96",
   "metadata": {},
   "source": [
    "### 4. Multivariate Processes and Causal Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648d5c4-b02a-431c-9b35-f6d5c3859496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b349cb-2941-4ea0-a171-e96981221b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "3. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "4. Geoff K. Nicholls. Bayes Methods.\n",
    "5. Patrick J. Laub. Hawkes Processes.\n",
    "6. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View.\n",
    "7. Jonas Peters. Elements of Causal Inference.\n",
    "8. Alessio Zanga. A Survey on Causal Discovery.\n",
    "9. Qing Zhou. Directed Mixed Graphs for Latent Variables.\n",
    "10. Peter Spirtes. Causation, Prediction, and Search. \n",
    "11. Thomas Richardson. Ancestral Graph Markov Models.\n",
    "12. Jonas Peters. Causal Inference Using Invariant Prediction.\n",
    "13. Fabrizio Durante. Sklar’s Theorem Obtained via Regularization Techniques.\n",
    "14. Nilesh Tripuraneni. On the Theory of Transfer Learning: The Importance of Task Diversity.\n",
    "15. Nir Friedman. On the Sample Complexity of Learning Bayesian Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b080f-eca9-458a-a53d-6d769746a106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
