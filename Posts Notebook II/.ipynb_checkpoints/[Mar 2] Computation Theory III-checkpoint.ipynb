{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Mar 2] Computation Theory III\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f4f87-8543-42fb-9743-918cd46c4d70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789ca6a-2827-4b30-9878-eef18193a80d",
   "metadata": {},
   "source": [
    "### 1. Graph Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b0c93-e949-47c6-bdd6-ef1fed8766bf",
   "metadata": {},
   "source": [
    "Assume edges are unweighted and undirectional. Each node  $v_{i} \\in \\mathcal{V}$ (each node is a token)  associates  $D$-dimensional feature vector  $\\mathbf{x}_{i} \\in \\mathbb{R}^{1 \\times D}$, and then stack the feature vectors into a single matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$. A message passing GNN layer  $l$  is computed as\n",
    "\n",
    "$$\\mathbf{x}_{i}^{(l+1)}=\\phi\\left(\\mathbf{x}_{i}^{(l)}, \\bigoplus_{j \\in \\mathcal{N}\\left(v_{i}\\right)} \\psi\\left(\\mathbf{x}_{i}^{(l)}, \\mathbf{x}_{j}^{(l)}\\right)\\right)$$\n",
    "\n",
    "where $\\bigoplus$ is a  permutation-invariant aggregation function. In practice, we convolve node features with its neighbours \n",
    "\n",
    "$$\\left(\\mathbf{A X}^{(l)}\\right)_{i}=\\sum_{j \\in \\mathcal{N}\\left(v_{i}\\right)} \\mathbf{x}_{j}^{(l)} \\quad \\text{ and } \\quad \\left(\\hat{\\mathbf{A}} \\mathbf{X}^{(l)}\\right)_{i}=\\mathbf{x}_{i}^{(l)}+\\sum_{j \\in \\mathcal{N}\\left(v_{i}\\right)} \\mathbf{x}_{j}^{(l)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70d118-1b5f-4f9c-b4e9-b2dc8e687865",
   "metadata": {},
   "source": [
    "> For GCNs, we use $\\underbrace{\\mathbf{X}^{(l+1)}}_{N \\times D_{\\text {out }}}=\\sigma(\\underbrace{\\tilde{\\mathbf{A}}}_{N \\times N} \\underbrace{\\mathbf{X}^{(l)}}_{N \\times D_{i n}} \\underbrace{\\mathbf{W}^{(l)}}_{D_{\\text {in }} \\times D_{\\text {out }}})=\\sigma(\\underbrace{\\hat{\\mathbf{D}}^{-1 / 2}}_{N \\times N} \\underbrace{\\hat{\\mathbf{A}}}_{N \\times N} \\underbrace{\\hat{\\mathbf{D}}^{-1 / 2}}_{N \\times N} \\underbrace{\\mathbf{X}^{(l)}}_{N \\times D_{i n}} \\underbrace{\\mathbf{W}^{(l)}}_{D_{i n} \\times D_{\\text {out }}})$.\n",
    "\n",
    ">\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b829-af10-4182-9aea-cf0d5a524805",
   "metadata": {},
   "source": [
    "Concretely, $\\left(\\hat{\\mathbf{D}}^{-1 / 2} \\hat{\\mathbf{A}} \\hat{\\mathbf{D}}^{-1 / 2} \\mathbf{X}^{(l)} \\mathbf{W}^{(l)}\\right)_{i}=\\left(\\hat{\\mathbf{D}}^{-1 / 2} \\hat{\\mathbf{A}} \\hat{\\mathbf{D}}^{-1 / 2} \\mathbf{X}^{(l)}\\right)_{i} \\mathbf{W}^{(l)}=\\left(c_{i i} \\mathbf{x}_{i}^{(l)}+\\sum_{j \\in \\mathcal{N}\\left(v_{i}\\right)} c_{i j} \\mathbf{x}_{j}^{(l)}\\right) \\mathbf{W}^{(l)}$ where $c_{i j}=\\frac{1}{\\sqrt{\\operatorname{deg}\\left(v_{i}\\right) \\operatorname{deg}\\left(v_{j}\\right)}}$. And $\\mathbf{W}^{(l)}$ simply performs a linear projection of the input features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67fdc4f-20ff-41b7-8fdb-6394dde3f7f0",
   "metadata": {},
   "source": [
    "> For multihead GATs, we use $\\underbrace{\\mathbf{X}^{(l+1)}}_{N \\times D_{\\text {out }}}=\\sigma(\\frac{1}{K} \\sum_{k=1}^{K} \\underbrace{\\tilde{\\mathbf{A}}^{(l, k)}}_{N \\times N} \\underbrace{\\mathbf{X}^{(l)}}_{N \\times D_{\\text {in }}} \\underbrace{\\mathbf{W}^{(l, k)}}_{D_{\\text {in }} \\times D_{\\text {out }}}) $ where $\\tilde{\\mathbf{A}}^{(l, k)}=\\tilde{\\mathbf{A}}^{(l, k)}\\left(\\mathbf{X}^{(l)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ecaac0-5b57-4335-a7a7-45feb0f38a0b",
   "metadata": {},
   "source": [
    "Intuitively, the operations performed by the transformer block are\n",
    "\n",
    "$$ \\mathrm{h}=\\operatorname{MHSA}(\\operatorname{LayerNorm}(\\mathrm{x}))+\\mathrm{x}, \\text{ and } \\mathrm{y}=\\operatorname{MLP}(\\operatorname{LayerNorm}(\\mathrm{h}))+\\mathrm{h},$$\n",
    "\n",
    "which are complemented with LayerNorms and skip connections aidding in training. \n",
    "\n",
    "> In words, MHSA processes the input feature matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ column-wise, whereas MLP row-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8e7e3-a5ea-47a2-8160-c5d5d1e40549",
   "metadata": {},
   "source": [
    "Given  $\\mathbf{Q}^{(l)}$,  $\\mathbf{K}^{(l)}$, and  $\\mathbf{V}^{(l)}$  are the query, key, and value matrices at layer  $l$, for MHSA, we introduce transformations below\n",
    "\n",
    "$$\\mathbf{Q}^{(l, k)}=\\mathbf{X}^{(l)} \\mathbf{W}_{Q}^{(l, k)} ; \\quad \\mathbf{K}^{(l, k)}=\\mathbf{X}^{(l)} \\mathbf{W}_{K}^{(l, k)} ; \\quad \\mathbf{V}^{(l, k)}=\\mathbf{X}^{(l)} \\mathbf{W}_{V}^{(l, k)}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7d29a-32ae-47d1-8860-e5fc1f388690",
   "metadata": {},
   "source": [
    "Then for each head $k$, $\\tilde{\\mathbf{A}}^{(l, k)}=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q}^{(l, k)}\\left(\\mathbf{K}^{(l, k)}\\right)^{T}}{\\sqrt{D_{k}}}\\right)$ descrbes the cross-token interaction. The output for each head is $\\mathbf{H}^{(l, k)}=\\tilde{\\mathbf{A}}^{(l, k)} \\mathbf{V}^{(l, k)}$. Next the output of all attention heads are concatenated and projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188ebba-25a5-4c13-9b06-89211514203a",
   "metadata": {},
   "source": [
    "$$\\underbrace{\\mathbf{X}^{(l+1)}}_{N \\times D_{\\text {out }}}=\\underbrace{\\left(\\|_{k=1}^{K} \\mathbf{H}^{(l, k)}\\right)}_{N \\times K D_{\\text {out }}} \\underbrace{\\mathbf{W}_{O}^{(l)}}_{K D_{\\text {out }} \\times D_{\\text {out }}}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64844e6-bb30-46c6-85bc-413ba03f9fcc",
   "metadata": {},
   "source": [
    "Here all $W_{\\square}$  are learnable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb5335-b681-49b7-a25e-969d723f2111",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fbacb-b54b-4504-b24e-84a1c8b3a800",
   "metadata": {},
   "source": [
    "### 2. Skip connections, Layer Normalization, and Other Details\n",
    "\n",
    "Skip Connections allow the information from earlier layers to be directly passed to later layers, mitigating the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2fdc7-fd66-4b9d-af4b-becd6ea8c5e6",
   "metadata": {},
   "source": [
    "Mathematically, write $z=l(x)+x$. Then \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{(L)}} \\cdot \\ldots \\cdot \\frac{\\partial z^{(l+1)}}{\\partial z^{(l)}} \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0ef31-f6cf-4439-8197-0cab969d54cf",
   "metadata": {},
   "source": [
    "where $\\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} =$ the parameters $W^{(l)}$ of layer $l$. Then we know \n",
    "\n",
    "$$ \\frac{\\partial z^{(l+1)}}{\\partial z^{(l)}}=\\frac{\\partial\\left(l\\left(z^{(l)}\\right)+z^{(l)}\\right)}{\\partial z^{(l)}}=\\frac{\\partial l\\left(z^{(l)}\\right)}{\\partial z^{(l)}}+\\frac{\\partial z^{(l)}}{\\partial z^{(l)}}=0+1 \\rightarrow \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} \\neq 0. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac548a-cffd-49b5-ae00-4a481a8d8dbc",
   "metadata": {},
   "source": [
    "LayerNorm enhances training stability and convergence speed, which operates by normalizing neuron activations within each layer independently. Specifically, \n",
    "\n",
    "$$\\operatorname{LayerNorm}\\left(x_{i}\\right)=\\gamma_{i} \\frac{x_{i}-\\mu_{i}}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}+\\beta_{i}$$\n",
    "\n",
    "where $\\epsilon > 0$ ensures numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454fe490-a4c2-4699-b1ea-489f0afd91b2",
   "metadata": {},
   "source": [
    "> BatchNorm calculates statistics across instances in a batch, while LayerNorm calculates statistics across the feature dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a07338-4108-4a80-bef8-da75a7eecd7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd389c-81a4-4013-b9ee-1d2da462111d",
   "metadata": {},
   "source": [
    "### 3. Graph Transformers Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d8fa1-367c-44ec-aabb-5c30437b48e1",
   "metadata": {},
   "source": [
    "In NLP, Transformers embrace full attention when constructing feature representations for words, treating sentences as fully connected graphs.\n",
    "\n",
    "> Note: scale allows for computationally feasible attention w.r.t. memory/time requirements. (not necessarily hold true for Graph Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702f7cb-1f2f-4257-bdb2-c6a870ef6d9f",
   "metadata": {},
   "source": [
    "However, ggraph datasets exhibit arbitrary connectivity structures. Modeling of long-range dependencies and interactions between nodes is achieved by **global attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d246d-e796-4435-abff-ade30fa3e43c",
   "metadata": {},
   "source": [
    "Specifically, \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathrm{h}= & \\text { GlobalAttention }(\\operatorname{LayerNorm}(\\mathrm{x}))+\\operatorname{MPNN}(\\operatorname{LayerNorm}(\\mathrm{x}, \\mathrm{A}))+\\mathrm{x} \\text {, } \\\\\n",
    "\\mathrm{y}= & \\operatorname{ MLP}(\\operatorname{LayerNorm}(\\mathrm{h}))+\\mathrm{h} .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e2f1f-e4cd-403a-9535-8a6cea686089",
   "metadata": {},
   "source": [
    "Graph Transformers address prevalent shortcomings of traditional message passing GNNs, including\n",
    "over-smoothing, over-squashing, and limited expressivity. \n",
    "\n",
    ">  **Local message-passing and Expressivity**: the expressivity of GNNs is bounded by the WL test (test group isomorphism).\n",
    ">\n",
    "> **Over-smoothing**: multiple layers promotes interacting nodes going into higher-order neighborhoods and having similar feature, making interacting nodes become indistinguishable.\n",
    ">\n",
    "> **Over-squashing**ï¼šas receptive field of nodes exponentially expands, representations must be compressed into fixed-size vectors. Then over-squashing occurs when messages exchanged between distant nodes become distorted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc3420-4182-4f96-8f52-5153a7d4cd0e",
   "metadata": {},
   "source": [
    "In traditional Transformers, positional encodings are added to the input embeddings.\n",
    "\n",
    "> $$\\operatorname{PE}(p, 2 d)=\\sin \\left(\\frac{p}{10000^{(2 d / D)}}\\right) \\quad \\text{ and } \\quad \\mathrm{PE}(p, 2 d+1)=\\cos \\left(\\frac{p}{10000^{(2 d / D)}}\\right)$$ \n",
    "> \n",
    "> where $p = $ position of a token in the sequence and $D =$ \\# dimensions of the model input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba459751-ba2b-4551-a8cb-7ae1fdbc01a6",
   "metadata": {},
   "source": [
    "**Laplace Eigenfunction Encodings** are the counterpart in graph transformers.\n",
    "\n",
    ">  They discern between various graph structures and sub-structures, akin to interpreting them as the frequencies of resonance within the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ab1bb-64fe-4823-8ecf-9340afbe0a21",
   "metadata": {},
   "source": [
    "Specifically, we use $\\mathbf{L} \\Phi=\\lambda \\Phi$, whose  eigenvectors are $\\|_{m=1}^{M} \\Phi_{i m} \\in \\mathbb{R}^{N \\times M}$. With each entry $\\Phi_{i} \\in \\mathbb{R}^{1 \\times M}$ corresponding to $v_{i}$. These features can be used to compute for instance, relative encodings between nodes  $v_{i}$  and  $v_{j}$  =  $d\\left(v_{i}, v_{j}\\right)=d\\left(\\Phi_{i}, \\Phi_{j}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5331e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Elucidating Graph Neural Networks, Transformers, and Graph Transformers.\n",
    "2. Relating Graph Neural Networks to Structural Causal Models. \n",
    "3. Fine-Tuning Graph Neural Networks via Graph Topology induced Optimal Transport.\n",
    "4. What Improves the Generalization of Graph Transformer? A Theoretical Dive into Self-attention and Positional Encoding.\n",
    "5. Advective Diffusion Transformers for Topological Generalization in Graph Learning.\n",
    "6. Deep Prompt Tuning for Graph Transformers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
