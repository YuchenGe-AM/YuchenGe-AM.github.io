{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Feb 21] Causal Inference and Transfer Learning\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c723a98-16e1-4224-997f-2b80431b4d02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Multivariate Causal Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacddc8c-47d8-4ad9-aa7c-ed309b5bbbcf",
   "metadata": {},
   "source": [
    "In **causal inference**, assume that **for a DAG $\\exists$ at most one edge between any two vertices [ Peter Spirtes. Causation, Prediction, and Search. P28 ]**. Recall that \n",
    "\n",
    "> A SCM $\\mathfrak{C}$ defines $X_{j}:=f_{j}\\left(\\mathbf{P A}_{j}, N_{j}\\right)$, which by assumption induces an  acyclic DAG.\n",
    ">\n",
    "> (**randomized experiments**) An intervention $P_{\\mathbf{X}}^{\\tilde{\\mathbb{C}}}=: P_{\\mathbf{X}}^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{f}\\left(\\widetilde{\\mathbf{P A}_{k}}, \\tilde{N}_{k}\\right)\\right)}$ is defined via setting $X_k:=\\tilde{f}\\left(\\widetilde{\\mathbf{P A}}_{k}, \\tilde{N}_{k}\\right)$. An intervention with $\\widetilde{\\mathbf{P A}}_{k}=\\mathbf{P A}_{k}$ is called imperfect. \n",
    ">\n",
    "> A counterfactual SCM is $\\mathfrak{C}_{\\mathbf{X}=\\mathbf{x}}:=\\left(\\mathbf{S}, P_{\\mathbf{N}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}\\right)$ where $P_{\\mathbf{N}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}:=P_{\\mathbf{N} \\mid \\mathbf{X}=\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15312391-dc98-436d-ad76-d5cce6a4f31e",
   "metadata": {},
   "source": [
    "Here TFAE for $P_{\\mathbf{X}}$ on a DAG:\n",
    "1. **global Markov property**: $\\mathbf{A} \\perp_{\\mathcal{G}} \\mathbf{B}|\\mathbf{C} \\Rightarrow \\mathbf{A} \\perp \\mathbf{B}| \\mathbf{C}$.\n",
    "2. **local Markov property**: each variable is independent of its non-descendants given its parents.\n",
    "3. **Markov factorization property**: $p(\\mathbf{x})=p\\left(x_{1}, \\ldots, x_{d}\\right)=\\prod_{j=1}^{d} p\\left(x_{j} \\mid \\mathbf{p} \\mathbf{a}_{j}^{\\mathcal{G}}\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae6a87-8808-4a92-be15-c517d63e227a",
   "metadata": {},
   "source": [
    "Set $\\mathcal{M}(\\mathcal{G}):=\\{P: P \\text { satisfies the global (local) Markov property with respect to } \\mathcal{G}\\}$. \n",
    "\n",
    "> Two DAGs $\\mathcal{G}_{1}$ and $\\mathcal{G}_{2}$ are Markov equivalent iff $\\mathcal{M}\\left(\\mathcal{G}_{1}\\right)=\\mathcal{M}\\left(\\mathcal{G}_{2}\\right)$ iff they satisfy the same set of d-separations. \n",
    ">\n",
    "> $P_{\\mathrm{X}}$ is induced by an SCM $\\implies P_{\\mathrm{X}} \\text { is Markovian with respect to } \\mathcal{G}$. \n",
    "\n",
    "It's clear that every Markov equivalence class can be represented a $CPDAG(\\mathcal{G})=(V, \\mathcal{E})$ where  $(i, j) \\in \\mathcal{E}$  iff one member of the Markov equivalence class does.\n",
    "\n",
    "> (**Markov blanket**) Consider a DAG  $\\mathcal{G}=(\\mathbf{V}, \\mathcal{E})$  and $Y$. The Markov blanket of  $Y$  is the smallest set  $M$  such that\n",
    "> \n",
    "> $$Y \\perp_{\\mathcal{G}} \\mathbf{V} \\backslash(\\{Y\\} \\cup M) \\text { given } M .$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a8073-ea5d-47d2-929c-ef504737fff2",
   "metadata": {},
   "source": [
    "Actually, $M=\\mathbf{P A}_{Y} \\cup \\mathbf{C H}_{Y} \\cup \\mathbf{P A}_{\\mathbf{C H}_{Y}}$. Assume that $ X$ and $Y$ are embedded into some SCM, then (**Reichenbach’s common cause principle** tells that) $\\exists$\n",
    "1. either a directed path from  $X$  to  $Y$, or\n",
    "2. from  $Y$  to  $X$, or\n",
    "3. a node  $Z$  with a directed path from  $Z$  to  $X$  and from  $Z$  to  $Y$. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a32a64-0a8b-4c10-a25b-406f7999747a",
   "metadata": {},
   "source": [
    "> A causal graphical model defines a collection of $f_{j}\\left(x_{j}, x_{\\mathbf{P A}_{j}^{\\mathcal{G}}}\\right)$ over $\\mathcal{G}$ such that $\\int f_{j}\\left(x_{j}, x_{\\mathbf{P A}_{j}^{\\mathcal{G}}}\\right) d x_{j}=1$, which defines a distribution $p\\left(x_{1}, \\ldots, x_{d}\\right)=\\prod_{j=1}^{d} f_{j}\\left(x_{j}, x_{\\mathbf{P A}_{j}^{\\mathcal{G}}}\\right)$.\n",
    "\n",
    "Then \n",
    "\n",
    "> $P_{\\mathrm{X}} \\text { is Markovian with respect to } \\mathcal{G}$ and $\\exists$\n",
    "strictly positive continuous density $p$ $\\implies \\left(P_{\\mathbf{X}}, \\mathcal{G}\\right) \\text { defines a causal graphical model}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b27fe8-2124-4ce8-b733-18c09e3183b9",
   "metadata": {},
   "source": [
    "$P_{\\mathbf{X}}$  is faithful to the  DAG $\\mathcal{G}$  if $\\mathbf{A} \\perp \\mathbf{B}\\left|\\mathbf{C} \\Rightarrow \\mathbf{A} \\perp_{\\mathcal{G}} \\mathbf{B}\\right| \\mathbf{C}$. Since one can d-separate any two nodes in a DAG $\\mathcal{G}$ that are not directly connected,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c7e17-0463-4871-acb6-bd01c2188753",
   "metadata": {},
   "source": [
    "> $P_{\\mathbf{X}}$  is faithful and Markovian to the  DAG $\\mathcal{G}$  $\\implies$ causal minimality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed55445-9a70-4e8b-876e-37dede6d418b",
   "metadata": {},
   "source": [
    "---\n",
    "Causal relationships are autonomous under interventions in a SCM,$ p^{\\tilde{\\mathbb{C}}}\\left(x_{j} \\mid x_{p a(j)}\\right)=p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right) $ for $\\tilde{\\mathfrak{C}}$ constructed from  $\\mathfrak{C}$  by intervening on (some)  $X_{k}$  but not $X_{j}$, which follows that \n",
    "\n",
    "$$\\begin{aligned}\n",
    "p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{1}, \\ldots, x_{d}\\right) & =\\prod_{j \\neq k} p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{j} \\mid x_{p a(j)}\\right) \\cdot p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{k}\\right) \\\\\n",
    "& =\\prod_{j \\neq k} p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right) \\tilde{p}\\left(x_{k}\\right)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624002e7-0919-4634-99ac-0f49cc895cb9",
   "metadata": {},
   "source": [
    "where $\\tilde{N}_{k}$ allows for the density $\\tilde{p}$. Moreover, for a singleton we can ajust for some Variables $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447f773-0f0d-402e-b51c-4d5ced65b2ff",
   "metadata": {},
   "source": [
    "> (**Valid adjustment set**) Consider an SCM  $\\mathfrak{C}$ and  $Y \\notin \\mathbf{P A}_{X}$. $\\mathbf{Z} \\subseteq \\mathbf{V} \\backslash\\{X, Y\\}$ is a valid adjustment set for the ordered pair  $(X, Y)$  if\n",
    ">\n",
    "> $$p^{\\mathfrak{C} ; d o(X:=x)}(y)=\\sum_{\\mathbf{Z}} p^{\\mathfrak{C}}(y \\mid x, \\mathbf{z}) p^{\\mathfrak{C}}(\\mathbf{z}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6521f51-9f8f-458f-a721-544e34cf4073",
   "metadata": {},
   "source": [
    "We can write  \n",
    "\n",
    "$$\\begin{aligned}\n",
    "p^{\\mathfrak{C} ; d o(X:=x)}(y) & =\\sum_{\\mathbf{z}} p^{\\mathfrak{C} ; d o(X:=x)}(y, \\mathbf{z}) \\\\\n",
    "& =\\sum_{\\mathbf{z}} p^{\\mathfrak{C} ; d o(X:=x)}(y \\mid x, \\mathbf{z}) p^{\\mathfrak{C} ; d o(X:=x)}(\\mathbf{z})\n",
    "\\end{aligned}$$\n",
    "\n",
    "If we have the following invariant observations, \n",
    "\n",
    "$$p^{\\mathfrak{C} ; d o(X:=x)}(y \\mid x, \\mathbf{z})=p^{\\mathfrak{C}}(y \\mid x, \\mathbf{z}) \\text { and } p^{\\mathfrak{C} ; d o(X:=x)}(\\mathbf{z})=p^{\\mathfrak{C}}(\\mathbf{z})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262b79e-2539-40ae-8ac4-4cc2b4eecdbb",
   "metadata": {},
   "source": [
    "which can be checked with intervention variables. Usually, we consider \n",
    "\n",
    "1. **parent adjustment**: $\\mathbf{Z}:=\\mathbf{P A}_{X}$\n",
    "2. **backdoor criterion**:  $\\mathbf{Z}$  contains no descendant of  $X$  & $\\mathbf{Z}$  blocks all paths from  $X$  to  $Y$ s.t. $X \\leftarrow \\ldots$. (use do-calculus rule 2 and 3)\n",
    "3. **toward necessity**:  $\\mathbf{Z}$  contains no descendant of any node on a directed path from  $X$  to  $Y$  (except for descendants of  $X$ not on the directed path ) & $\\mathbf{Z}$  blocks all non-directed paths from  $X$  to  $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe889e-0d3a-4ef2-9f5f-b88f2f0bff24",
   "metadata": {},
   "source": [
    "Note that $2 \\implies 3$. Consider an SCM, we have the powerful tool -- rules of do-calculus:\n",
    "\n",
    "1. If $\\mathbf{Y}$  and  $\\mathbf{Z}$  are  d-separated by  $\\mathbf{X}, \\mathbf{W}$  in a graph where incoming edges in  $\\mathbf{X}$  have been removed, then \n",
    "\n",
    "$$p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x})}(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{w})=p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x})}(\\mathbf{y} \\mid \\mathbf{w}).$$\n",
    "\n",
    "2. If  $\\mathbf{Y}$ and  $\\mathbf{Z}$  are  d-separated by  $\\mathbf{X}, \\mathbf{W}$  in a graph where incoming edges in  $\\mathbf{X}$  and outgoing edges from  $\\mathbf{Z}$  have been removed, then \n",
    "\n",
    "$$p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x}, \\mathbf{Z}=\\mathbf{z})}(\\mathbf{y} \\mid \\mathbf{w})=p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x})}(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{w}).$$\n",
    "\n",
    "3. If  $\\mathbf{Y}$  and  $\\mathbf{Z}$  are  d-separated by  $\\mathbf{X}, \\mathbf{W}$  in a graph where incoming edges in  $\\mathbf{X}$  and  $\\mathbf{Z}(\\mathbf{W})$  have been removed, then\n",
    "\n",
    "$$ p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x}, \\mathbf{Z}=\\mathbf{z})}(\\mathbf{y} \\mid \\mathbf{w})=p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x})}(\\mathbf{y} \\mid \\mathbf{w}). $$\n",
    "\n",
    "These rules can be seen as operators on $\\mathbf{y}$ on both sides (focusing on no-effect and effect-on-both-sides arrows). Moreover, these rules are complete and $\\exists$ an algorithm to find all identifiable intervention distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e533e6e-4f4b-4a00-8c1b-835e4a7b9860",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We assign two potential outcomes to each patient $u$. For example, $B_{u}(t=1)$  indicates whether the patient would go blind  ($B=1$)  or get cured  ($B=0$)  if she receives treatment  ($T=1$) . Analogously, we clarify $B_{u}(t=0)$. Then we write\n",
    "\n",
    "> the counterfactual SCM  $\\mathfrak{C}$  satisfies  $T=N_{T}$  and  $B=T \\cdot N_{B}+(1-T) \\cdot\\left(1-N_{B}\\right)$ s.t. $t=0/1$  correspond to interventions on $T$. \n",
    "\n",
    "Summarizing, we have that\n",
    "\n",
    "$$\\underbrace{B_{u}(t=\\tilde{t})}_{\\text {potential outcome }}=\\underbrace{B \\text { in the } \\mathrm{SCM} \\mid \\mathbf{N}=\\mathbf{n}_{u} ; d o(T:=\\tilde{t})}_{\\text {counterfactual SCM }}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b584e-2eb3-462f-9b7d-71ece44d939c",
   "metadata": {},
   "source": [
    "Since $N_T, N_B$ are deterministic, the entailed distribution of $B$ is degenerate and $B$ is deterministic as required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e7bfd-05d9-4727-9d73-037d1a825413",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d62d2f-01ab-4b47-baac-333299e715e3",
   "metadata": {},
   "source": [
    "Two models are called\n",
    "$\\{$ probabilistically/interventionally/counterfactually $\\}$ equivalent\n",
    "if they entail the same $\\{$ obs./obs. and int./obs., int., and counterf. $\\}$ distributions.\n",
    "\n",
    "> [Identifiability of Markov equivalence class] Assume that  $P_{\\mathbf{X}}$  is Markovian and faithful with respect to  $\\mathcal{G}^{0}$. Then, $\\forall \\mathcal{G} \\in \\operatorname{CPDAG}\\left(\\mathcal{G}^{0}\\right)$, $\\exists$ SCM which entails $P_{\\mathbf{X}}$. \n",
    ">\n",
    "\n",
    "**Proof.**\n",
    "Let  $N_{1}, ..., N_{d}\\stackrel{i.i.d.}{\\sim} U[0,1]$. It suffices that we define $X_{j}:=f_{j}\\left(X_{\\mathbf{P A}_{j}}, N_{j}\\right)$  with\n",
    "$$ f_{j}\\left(x, n_{j}\\right):=F_{X_{j} \\mid X_{\\mathbf{P A}_{j}}= x}^{-1}\\left(n_{j}\\right) $$\n",
    "where $F_{Y}^{-1}(a):=\\inf \\left\\{y \\in \\mathbb{R}: F_{Y}(y) \\geq a\\right\\}$. From the Inverse CDF method, this recovers the distribution of $X_{j} \\mid X_{\\mathbf{P A}_{j}}=x$ and then guarantees that $\\operatorname{CPDAG}\\left(\\mathcal{G}^{0}\\right)$ is identifiable from $P_{\\mathbf{X}}$.\n",
    "\n",
    "This implies that for each probabilistic model, there is an observationally equivalent SCM.\n",
    "\n",
    "---\n",
    "\n",
    "Next, we study hidden variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e08601-f986-4c4d-9618-10dad4afade3",
   "metadata": {},
   "source": [
    "> $X$ is **causally sufficient** if there is no hidden common cause $C \\notin X$ that is causing more than one variable in $X$.\n",
    ">\n",
    "> $X$ is **Interventional sufficiency** if $\\exists$ SCM over $X$ that cannot be falsified as an\n",
    "interventional model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b02ca-64d5-4a52-97f1-cf6e36c9684d",
   "metadata": {},
   "source": [
    "Then we know\n",
    "\n",
    "> (**Interventional sufficiency and causal sufficiency**) Let  $\\mathfrak{C}$  be an SCM for the variables  $\\mathbf{X}$  that cannot be falsified as an interventional model.\n",
    "> 1. $\\mathbf{O} \\subseteq \\mathbf{X}$ causally sufficient $\\implies$ interventionally sufficient.\n",
    ">\n",
    "> 2. $\\exists$ exmples which is interventionally sufficient but not causally sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02145cf-bcf8-48b0-b977-c2bd90a20223",
   "metadata": {},
   "source": [
    "**Proof.** Consider **marginalizations of SCMs [Bongers et al. [2016]]**: start with the original SCM and consider only the structural assignments of the observed variables. One then repeatedly plugs in the assignments of the hidden variables whenever they appear on the right-hand side.\n",
    "\n",
    "Consider assignment for $O \\in \\mathbf{O}$ which contains a multivariate error variable $\\mathbf{N}_{O}$. From causal sufficiency, $\\mathbf{N}_{O}$ are jointly independent which proves 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074a7da-7396-443b-9b20-4e5b04b55316",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab8b40-87d1-404a-8419-aa77c84ed88d",
   "metadata": {},
   "source": [
    "Next, we study graph structure which can be recovered from the joint distribution. Consider additive noise models (ANMs), $Y=f_{Y}(X)+N_{Y}, \\quad N_{Y} \\perp X$.\n",
    "\n",
    "> (Identifiability of ANMs) Suppose the ANM is smooth enough, then $\\left(\\log p_{N_{Y}}\\right)^{\\prime \\prime}\\left(y-f_{Y}(x)\\right) f_{Y}^{\\prime}(x) \\neq 0$ implies $\\log p_{X}$  for which the obtained joint distribution  $P_{X, Y} $ admits a smooth ANM from  $Y$  to  $X$  is contained in a 3-dimensional affine space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20394e74-71f7-4b74-bcb0-9c3897f4a9a1",
   "metadata": {},
   "source": [
    "We know that \n",
    "\n",
    "$$p(x, y)=p_{Y}(y) p_{N_{X}}\\left(x-f_{X}(y)\\right) \\implies \\log p(x, y)=\\log p_{Y}(y)+\\log p_{N_{X}}\\left(x-f_{X}(y)\\right),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9bb5a8-93b4-42b7-b186-ce1acd207cd4",
   "metadata": {},
   "source": [
    "which follows that\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial x}\\left(\\frac{\\partial^{2} \\log p(x, y) / \\partial x^{2}}{\\partial^{2} \\log p(x, y) /(\\partial x \\partial y)}\\right)=0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625c2b9-e8c3-416b-bca1-71cf6b7d2d5d",
   "metadata": {},
   "source": [
    "Also, we know that $\\log p(x, y)=\\log p_{X}(x)+\\log p_{N_{Y}}\\left(y-f_{Y}(x)\\right)$, which follows by taking in the differntial equation yields\n",
    "\n",
    "> a differential equation for the third derivative of $\\log p_{X}$ in terms of (first, second, and third) derivatives of $f_{Y}$ and $\\log p_{N_{Y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babdfc23-105a-488f-93fd-eac557df860f",
   "metadata": {},
   "source": [
    "Therefore, $f_{Y}$ and $\\log p_{N_{Y}}$ determine $\\log p_{X}$ up to the\n",
    "three free parameters, which follows the conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50743c-9f36-4050-8531-647030fb2cab",
   "metadata": {},
   "source": [
    "> **Principle.**  $P_{X}$ and $P_{Y \\mid X}$ contain no information about each other.\n",
    "\n",
    "\n",
    "We then introduce **Kolmogorov complexity** from Algorithmic Information Theory to check the algorithmic dependence between $P_{X}$ and $P_{Y \\mid X}$.\n",
    "\n",
    "> For any binary string $s$, $K(s)$ = length of the shortest program $=\\left|s^{*}\\right|$ where $s\n",
    "^∗$ is the shortest compression of $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eebc3f9-ec1b-4cdd-a72c-f355615802b1",
   "metadata": {},
   "source": [
    "Although $K(s)$ is uncomputable, it's useful to formalize conceptual ideas below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41258595-49f5-47ea-81da-8521d4ab7bb2",
   "metadata": {},
   "source": [
    "> Mutual information: $I(s: t)=K(s)-K\\left(s \\mid t^{*}\\right) \\stackrel{ + }{=} K(s)+K(t)-K(s, t)$.\n",
    ">\n",
    "> **Principle**: $P_{C}$  and  $P_{E \\mid C}$  are algorithmically independent, i.e.\n",
    ">\n",
    "> $$I\\left(P_{C}: P_{E \\mid C}\\right) \\stackrel{ + }{=} 0 \\Leftrightarrow K\\left(P_{C, E}\\right) \\stackrel{ + }{=} K\\left(P_{C}\\right)+K\\left(P_{E \\mid C}\\right). $$\n",
    "\n",
    "Janzing and Steudel [2010] argue that \n",
    "\n",
    "> $Y:=f_{Y}(X)+N_{Y}$  implies that the second derivative of  $y \\mapsto \\log p(y)$  is determined by partial derivatives of  $(x, y) \\mapsto \\log p(x \\mid y)$. \n",
    "\n",
    "Hence, knowing  $P_{X \\mid Y}$  admits a short description of  $P_{Y}$  (up to some accuracy). Whenever  $K\\left(P_{Y}\\right)$  is larger than this small amount of information, Janzing and Steudel [2010] conclude that  $Y \\rightarrow X$  should be rejected because  $P_{Y}$  and  $P_{X \\mid Y} $ are algorithmically dependent. This is based on the principle of ANMs to a large set of different distributions since we cannot guarantee that $K\\left(P_{Y}\\right)$  is large enough to reject  $Y \\rightarrow X$  just because there is an ANM from  $Y$  to  $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d40d3-c922-4304-a1f0-836cb3a8650c",
   "metadata": {},
   "source": [
    "Note that the following is true\n",
    "\n",
    "> $K\\left(P_{X}\\right)+K\\left(P_{Y \\mid X}\\right) \\stackrel{ \\pm}{=} K\\left(P_{X, Y}\\right) \\stackrel{+}{\\leq} K\\left(P_{Y}\\right)+K\\left(P_{X \\mid Y}\\right)$\n",
    "\n",
    "which reflects the spirit of Occam’s razor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f423a-b8e3-40ce-b565-463a5fdbdd54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> [ **Thomas Richardson. Ancestral Graph Markov Models. Appendix.** ] Let  $\\mathcal{E}=\\{-, \\leftarrow, \\rightarrow, \\leftrightarrow\\}$  be the set of edges.  A mixed graph is a pair $\\mathscr{g}=(V, E)$  with $E: V \\times V \\rightarrow \\mathcal{P}(\\mathcal{E})$ subject to \n",
    "> $$\\begin{aligned}\n",
    "E(\\alpha, \\alpha)=\\varnothing, & \\quad -\\in E(\\alpha, \\beta) \\Longleftrightarrow-\\in E(\\beta, \\alpha), \\\\\n",
    "\\leftarrow \\in E(\\alpha, \\beta) \\Longleftrightarrow \\rightarrow \\in E(\\beta, \\alpha), & \\quad   \\leftrightarrow \\in E(\\alpha, \\beta) \\Longleftrightarrow \\leftrightarrow \\in E(\\beta, \\alpha) .\n",
    "\\end{aligned}$$\n",
    ">\n",
    "> An **ADMG** is a mixed graph containing no directed cycles, which can be viewed as a DAG adding  bidirected arrows ($\\leftrightarrow$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2435c-f61c-4290-aaf9-758fff79cb95",
   "metadata": {},
   "source": [
    "If $a \\leftrightarrow b$, then $a$ is a sibling of $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19cccb-d422-4572-b709-d4dc470b6659",
   "metadata": {},
   "source": [
    "> Start with the original DAG containing hidden variables, then apply **latent projection of a DAG**. The resulting graph structure is an ADMG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4e08a-dbe4-44ea-ac2f-15b690a4f22a",
   "metadata": {},
   "source": [
    "Then we find\n",
    "\n",
    "> a **nested Markov property**  s.t. a distribution is nested Markovian with respect to an ADMG if \n",
    ">\n",
    "> $$\\text{conditional independences （implied by the graph structure） hold} \\quad  \\& \\quad \\text{some other constraints hold}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd53c1a-7b00-4471-90e9-65cd0ae3c359",
   "metadata": {},
   "source": [
    "We therefore have\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\left\\{P_{\\mathbf{O}}: P_{\\mathbf{O}, \\mathrm{v}}\\right.  \\text{ induced by a DAG  \\mathcal{G}  with latent variables }  \\} \n",
    "& \\subseteq\\left\\{P_{\\mathbf{O}}: P_{\\mathbf{O}}\\right.  \\text{ is nested Markovian with respect to corresponding ADMG } \\}  \\\\\n",
    "& \\subseteq\\left\\{P_{\\mathbf{O}}: P_{\\mathbf{O}}\\right.  \\text{ is Markovian with respect to corresponding ADMG } \\} .\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "3. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "4. Geoff K. Nicholls. Bayes Methods.\n",
    "5. Patrick J. Laub. Hawkes Processes.\n",
    "6. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View.\n",
    "7. Jonas Peters. Elements of Causal Inference.\n",
    "8. Alessio Zanga. A Survey on Causal Discovery.\n",
    "9. Qing Zhou. Directed Mixed Graphs for Latent Variables.\n",
    "10. Peter Spirtes. Causation, Prediction, and Search. \n",
    "11. Thomas Richardson. Ancestral Graph Markov Models.\n",
    "12. Jonas Peters. Causal Inference Using Invariant Prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
