{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Feb 22] Causal Inference and Transfer Learning II\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f2a81-37b9-435e-8710-90c1034d79c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Examples of Counting Process\n",
    "\n",
    "> A **counting process** is an a.s. finite $\\big($ i.e. $\\mathbb{P}(\\bigcup_t N(t)<\\infty )=1$ $\\big)$ stochastic process  $(N(t): t \\geqslant 0)$  taking values in  $\\mathbb{N}_{0}$  s.t.  $N(0)=0$, and is a right-continuous step function with increments of size +1.\n",
    "\n",
    "It can be written as a **point process**: $N(t)=\\sum_{i \\geq 1} I\\left\\{T_{i} \\leq t\\right\\}$ where $ \\mathbb{P}\\left(0 \\leqslant T_{1} \\leqslant T_{2} \\leqslant \\ldots < \\infty \\right)=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa72a71-f5d9-4645-bb3a-39eaf1c8926f",
   "metadata": {},
   "source": [
    "> For all choices of  $0\\leq t_{1}<t_{2}<\\cdots<t_{n}$, a process has **independent** increments if $N\\left(t_{i}\\right)-N\\left(t_{i-1}\\right)$, $i \\leq n$, are independent; a process has **stationary** increments if $N\\left(t+h\\right)-N\\left(t\\right)$ depends only on $h$, $\\forall t \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade39c2b-e1df-47b1-9d28-9240570c82f0",
   "metadata": {},
   "source": [
    "Now we consider \n",
    "\n",
    "> ( **Poisson Process** ) A counting process  $(N(t): t \\geqslant 0)$  is a homogeneous Poisson process with rate  $\\lambda>0$  if \n",
    ">\n",
    "> 1. $N(I) \\sim \\operatorname{Poi}(\\lambda|I|)$\n",
    ">\n",
    "> 2. $N\\left(I_{1}\\right)$, $ N\\left(I_{2}\\right), \\ldots, N\\left(I_{n}\\right)$  are independent, $\\forall$ disjoint intervals  $I_{1}, I_{2}, \\ldots, I_{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fb0f0-87ab-4e25-911d-f6b387a1b548",
   "metadata": {},
   "source": [
    "It follows that \n",
    "\n",
    "$$ \\mathbb{P}(N(t)=n)=\\frac{(\\lambda t)^{n}}{n !} \\mathrm{e}^{-\\lambda t}, \\quad n=0,1,2, \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3203432-1e67-4f94-98e8-69bfdd632179",
   "metadata": {},
   "source": [
    "$\\exists$ a connection between Bernoulli and Poisson processes. Fixing some $h>0$, partition time into disjoint intervals  $J_{1}^{(h)}=(0, h], J_{2}^{(h)}=(h, 2 h], \\ldots$\n",
    "\n",
    "Set $X_{n}$ = # arrivals in  $J_{n}^{(h)} \\sim \\operatorname{Ber}(p) = \\operatorname{Ber}(\\lambda h)$. Defining  $Y_{n}=   X_{1}+\\cdots+X_{n}, Y_{n}$ = # arrivals in the interval  $(0, n h]$.\n",
    "\n",
    "> **Prop.** $ N(t)$  and  $Y_{n}$  have **approximately** the same distribution.\n",
    "\n",
    "**Proof.** Now consider a fixed time  $t>0$, which falls in the interval  $n=\\lfloor t / h\\rfloor$. Then,  $Y_{n} \\sim \\operatorname{Bin}(n, p)$  and\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}\\left(Y_{n}=m\\right) & =\\left(\\begin{array}{l}\n",
    "n \\\\\n",
    "m\n",
    "\\end{array}\\right)(\\lambda h)^{m}(1-\\lambda h)^{n-m} \\approx\\left(\\begin{array}{l}\n",
    "n \\\\\n",
    "m\n",
    "\\end{array}\\right)(\\lambda t / n)^{m}(1-\\lambda t / n)^{n-m} \\\\\n",
    "& =\\frac{n !}{(n-m) ! n^{m}} \\frac{(\\lambda t)^{m}}{m !}\\left(1-\\frac{\\lambda t}{n}\\right)^{n-m}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "As $ n \\rightarrow \\infty$, $\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left(Y_{n}=m\\right)=\\frac{(\\lambda t)^{m}}{m !} \\mathrm{e}^{-\\lambda t}$.\n",
    "\n",
    "> **Thm I.** The interarrival times of the Bernoulli (Poisson) process are independent and geometric (exponential).\n",
    ">\n",
    "> **Thm II.** Bernoulli (Poisson) processes have superposition and thinning property.\n",
    "\n",
    "**Proof.** We only prove thm I for Poisson process. Define $\\mathbb{P}(N(t_2 - t_1) = k) = P(k; t_2 - t_1)$. Computation shows that for $\\Delta T_k = T_k - T_{k-1}$ where $T_k$'s are the stopping times,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{P}\\left(t_{1} \\leq \\Delta T_{1} \\leq t_{1}+\\delta,\\right. & \\left.t_{2} \\leq \\Delta T_{2} \\leq t_{2}+\\delta\\right) \\\\\n",
    "\\approx & P\\left(0 ; t_{1}\\right) \\cdot P(1 ; \\delta) \\cdot P\\left(0 ; t_{2}-t_{1}-\\delta\\right) \\cdot P(1 ; \\delta) \\\\\n",
    "= & e^{-\\lambda t_{1}} \\lambda \\delta e^{-\\lambda\\left(t_{2}-\\delta\\right)} \\lambda \\delta.\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "We divide both sides by  $\\delta^{2}$, and take the limit as  $\\delta \\downarrow 0$, to obtain\n",
    "\n",
    "$$f_{\\Delta T_{1}, \\Delta T_{2}}\\left(t_{1}, t_{2}\\right)=\\lambda e^{-\\lambda t_{1}} \\lambda e^{-\\lambda t_{2}} . \\quad t_{1}, t_{2}>0.$$\n",
    "\n",
    "\n",
    "This shows that  $\\Delta T_{2}$  is independent of  $\\Delta T_{1}$ and has the same exponential distribution, which can easily generalize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b787cd9-4143-4104-bf63-e3676b5e398c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20cb64e-9d9c-4152-9d81-a21d5d78fede",
   "metadata": {},
   "source": [
    "### 2. Stochastic Intensity from a Martingale Viewpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6a201-e8e1-422e-868e-66104a1f94c2",
   "metadata": {},
   "source": [
    "> $A \\in \\mathcal{V}_{T}$ (of finite variation) if the process is cadlag adapted and $\\int_{0}^{T}\\left|d A_{t}\\right|<\\infty$.\n",
    ">\n",
    "> $A \\in \\mathcal{A}_{T}$ (of integrable variation) if $A \\in  \\mathcal{V}_{T}$ s.t. $E\\left[\\int_{0}^{T}\\left|d A_{t}\\right|\\right]<\\infty$.\n",
    ">\n",
    "> We set $\\mathcal{V} =  \\bigcup_T \\mathcal{V}_{T}$ and $\\mathcal{A} =  \\bigcup_T \\mathcal{A}_{T}$.\n",
    "\n",
    "The optional  $\\sigma$-algebra on $ R_{+} \\times \\Omega$  is generated by all $Y_{t}(\\omega)=Z(\\omega) I\\{r \\leq t<s\\}$ where $Z$ is $\\mathcal{F}_{r}$-measurable. The predictable  $\\sigma$-algebra $\\Sigma_{P}$ on $ R_{+} \\times \\Omega$  is generated by all $Y_{t}(\\omega)=Z(\\omega) I\\{r < t \\leq s\\}$ where $Z$ is $\\mathcal{F}_{r}$-measurable. Then in particular every $A \\in \\mathcal{V}_{T}$ is optional. And for adapted cadlag $X$,  $Y_{t}=X_{t-}\\in\\mathcal{F}_{t-}$ is adapted left-continuous, and thus certainly predictable.\n",
    "\n",
    "> 1. The optional $\\sigma$-algebra is generated by cadlag adapted processes.\n",
    ">\n",
    "> 2. The predictable $\\sigma$-algebra is generated by adapted left continuous processes.\n",
    ">\n",
    "> 3. Predictable $\\implies$ Optional.\n",
    "\n",
    "Those can be proved with some $X^{n} \\rightarrow X$ pointwise. For example, we set $X^{n}=\\sum_{k \\in \\mathbb{N}} X_{k / 2^{n}} I\\{ k / 2^{n} \\leq t < (k+1) / 2^{n}\\}$. Also based on 2, for adapted left-continuous $X$, \n",
    "\n",
    "$$X_{t}^{n}=n \\int_{t-1 / n}^{t} I \\left\\{\\left|X_{s \\vee 0}\\right| \\leq n\\right\\} X_{s \\vee 0} d s \\to X_t $$\n",
    "\n",
    "pointwise implies that predictable $\\sigma$-algebra is also generated by adapted continuous processes.\n",
    "\n",
    "> Practically, **one may without great risk interpret “optional” as “adapted” and  \"predictable\" as either \"adapted and left continuous\" or as \"$\\mathcal{F}_{t-}$  adapted\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16588a82-6eea-4b8d-b168-f088ab16b9ba",
   "metadata": {},
   "source": [
    "Then by a (possibily non-trivial) approximation argument\n",
    "\n",
    "> 1. $A\\in \\mathcal{V}_{T}$  & $h$  is optional s.t. $E\\left[\\int_{0}^{t}\\left|h_{s}\\right|\\left|d A_{s}\\right|\\right]<\\infty \\implies X_{t}=\\int_{0}^{t} h_{s} d A_{s}$ is cadlag adapted.\n",
    ">\n",
    "> 2. $M\\in \\mathcal{V}_{T}$  is predictable & $h$  is predictable s.t. $E\\left[\\int_{0}^{t}\\left|h_{s}\\right|\\left|d M_{s}\\right|\\right]<\\infty \\implies X_{t}=\\int_{0}^{t} h_{s} d M_{s}$ is predictable.\n",
    ">\n",
    "> 3. $M\\in \\mathcal{V}_{T}$  is a martingale & $h$  is predictable s.t. $E\\left[\\int_{0}^{t}\\left|h_{s}\\right|\\left|d M_{s}\\right|\\right]<\\infty \\implies X_{t}=\\int_{0}^{t} h_{s} d M_{s}$ is a martingale.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee9cef-7b2c-471b-a898-7e17c59bc041",
   "metadata": {},
   "source": [
    "**Proof.** Only 3 requires non-trivial approximation argument. It's known that from [Limit Theorems for Stochastic Processes, P16] that \n",
    "\n",
    "> If $h$ is predictable, $h_{t}$ is $\\mathcal{F}_{t-}$-measurable.\n",
    "\n",
    "It suffices $E\\left[d X_{t} \\mid \\mathcal{F}_{t-}\\right]=0$. From $d X_{t}=h_{t} d M_{t}$, so \n",
    "\n",
    "$$E\\left[d X_{t} \\mid \\mathcal{F}_{t-}\\right]=E\\left[h_{t} d M_{t} \\mid \\mathcal{F}_{t-}\\right]=h_{t} E\\left[d M_{t} \\mid \\mathcal{F}_{t-}\\right]=0.\n",
    "$$\n",
    "\n",
    "where we use $h_{t} \\in \\mathcal{F}_{t-}$ to pull this term outside the expectation. (The argument can be made rigorous by considering $P$-null sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cbec9-8483-4a7f-8f5d-e94eb4114327",
   "metadata": {},
   "source": [
    "> Assume that  $X$  is optional and cadlag, and set $Y_{t}=X_{t-}$. Then  Y  is predictable and then\n",
    ">\n",
    "> $$\\int_{0}^{t} h_{s} X_{s} d s=\\int_{0}^{t} h_{s} Y_{s} d s. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf948568-f773-4680-985d-45c29d07fdf1",
   "metadata": {},
   "source": [
    "Consider  $d X_{t}=\\mu_{t} d t+\\sigma_{t} d W_{t}+h_{t} d N_{t}$ where  $\\mu$  and $\\sigma$ are predictable and  $N$  is a counting process. \n",
    "\n",
    "\n",
    "Between the jumps of  $N$,  the process  $X$ enjoys $d X_{t}=\\mu_{t} d t+\\sigma_{t} d W_{t}$. From standard Itô formula ($(\\mathrm{d} z)^{2} = \\mathrm{d}t, \\mathrm{~d}z\\mathrm{~d}t = 0, (\\mathrm{~d}t)^{2} = 0$ where $z$ denote Wiener-Brownian motion),\n",
    "\n",
    "$$ d F\\left(t, X_{t}\\right)=\\left\\{\\frac{\\partial F}{\\partial t}\\left(t, X_{t}\\right)+\\mu_{t} \\frac{\\partial F}{\\partial x}\\left(t, X_{t}\\right)+\\frac{1}{2} \\sigma_{t}^{2} \\frac{\\partial^{2} F}{\\partial x^{2}}\\left(t, X_{t}\\right)\\right\\} d t+\\sigma_{t} \\frac{\\partial F}{\\partial x}\\left(t, X_{t}\\right) d W_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bbd51-28ed-48a0-a570-b98cd077634b",
   "metadata": {},
   "source": [
    "Otherwise at jump time $t$, $\\Delta X_{t}=h_{t} \\Delta N_{t}=h_{t}$ implies that \n",
    "\n",
    "$$ \\Delta Z_{t}=F\\left(t, X_{t}\\right)-F\\left(t-, X_{t-}\\right) = \\Delta Z_{t}=F\\left(t, X_{t-}+h_{t}\\right)-F\\left(t-, X_{t-}\\right). $$\n",
    "\n",
    "Therefore, we can generalize above to the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc08525-a6dd-4a1f-b4b9-57dc65e32002",
   "metadata": {},
   "source": [
    "> **Thm.** (**Itô formula**) Assume $X$  has dynamics $d X_{t}=\\mu_{t} d t+\\sigma_{t} d W_{t}+h_{t} d N_{t}$, then \n",
    ">\n",
    "> $$ \\begin{aligned}\n",
    "d F\\left(t, X_{t}\\right) & =\\left\\{\\frac{\\partial F}{\\partial t}\\left(t, X_{t}\\right)+\\mu_{t} \\frac{\\partial F}{\\partial x}\\left(t, X_{t}\\right)+\\frac{1}{2} \\sigma_{t}^{2} \\frac{\\partial^{2} F}{\\partial x^{2}}\\left(t, X_{t}\\right)\\right\\} d t +\\sigma_{t} \\frac{\\partial F}{\\partial x}\\left(t, X_{t}\\right) d W_{t} \\\\\n",
    "& +\\left\\{F\\left(t, X_{t-}+h_{t}\\right)-F\\left(t, X_{t-}\\right)\\right\\} d N_{t}.\n",
    "\\end{aligned} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a507a9-3391-456b-bf7e-03fa616eca34",
   "metadata": {},
   "source": [
    "After that we consider SDE of counting process, where non-causal dynamic is avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ade3b-63aa-4fe5-a1c4-50cbb47ef6c2",
   "metadata": {},
   "source": [
    "> Assume that $\\forall x_{0}$, the ODE $$\\left\\{\\begin{aligned}\n",
    "\\frac{d X_{t}}{d t} & =\\mu\\left(X_{t}\\right), \\\\\n",
    "X_{0} & =x_{0},\n",
    "\\end{aligned}\\right.$$\n",
    ">\n",
    "> has a unique global solution.  Then $\\forall \\beta: R \\rightarrow R$, the SDE\n",
    ">\n",
    "> $$\\left\\{\\begin{aligned}\n",
    "d X_{t} & =\\mu\\left(X_{t-}\\right) d t+\\beta\\left(X_{t-}\\right) d N_{t}, \\\\\n",
    "X_{0} & =x_{0},\n",
    "\\end{aligned}\\right.$$\n",
    ">\n",
    "> has a unique global solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084467f4-c077-41f0-b006-ae6374095c63",
   "metadata": {},
   "source": [
    "**Proof.** We have the following concrete algorithm.\n",
    "\n",
    "1. Denote the jump times of  $N$  by  $T_{1}, T_{2}, \\ldots $\n",
    "2. $\\forall \\omega $, solve the ODE\n",
    "\n",
    "$$\\left\\{\\begin{aligned}\n",
    "\\frac{d X_{t}}{d t} & =\\mu\\left(X_{t}\\right), \\\\\n",
    "X_{0} & =x_{0}\n",
    "\\end{aligned}\\right.$$\n",
    "\n",
    "on $\\left[0, T_{1}\\right)$ to determined $X_{T_{1}-}$.\n",
    "\n",
    "3. Calculate $X_{T_{1}}$  by $X_{T_{1}}=X_{T_{1}-}+\\beta\\left(X_{T_{1}-}\\right)$.\n",
    "\n",
    "4. Given  $X_{T_{1}}$  from 3, solve the ODE $\\frac{d X_{t}}{d t}=\\mu\\left(X_{t}\\right)$ on  $\\left[T_{1}, T_{2}\\right)$, which gives us  $X_{T_{2}-}$.\n",
    "\n",
    "5. Compute  $X_{T_{2}}$  by $X_{T_{2}}=X_{T_{2}-}+\\beta\\left(X_{T_{2}-}\\right)$.\n",
    "\n",
    "6. Continue by induction.\n",
    "\n",
    "Finally, we discuss **stochastic intensity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e25b7-18b2-456a-b658-be82fd849423",
   "metadata": {},
   "source": [
    "> Given an optional counting process  $N$. Let  $\\lambda \\geq 0$  be optional s.t. $\\int_{0}^{t} \\lambda_{s} d s<\\infty$. If \n",
    ">\n",
    "> $$E\\left[\\int_{0}^{\\infty} h_{t} \\lambda_{t} d t\\right]=E\\left[\\int_{0}^{\\infty} h_{t} d N_{t}\\right]$$\n",
    ">\n",
    "> $\\forall$ predictable $h$, then we say $N$  has (stochastic) intensity  $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50a16d-8129-4349-be03-b53112472e3c",
   "metadata": {},
   "source": [
    "Then we know if $E\\left[N_{t}\\right]<\\infty$, then $M_{t}=N_{t}-\\int_{0}^{t} \\lambda_{s} d s$ is a martingale, which can be seen from\n",
    "\n",
    "$$ \\text{ set } h_{u}(\\omega)=I_{A}(\\omega) I\\{s<u \\leq t\\} \\implies E\\left[I_{A} \\int_{s}^{t} \\lambda_{u} d u\\right]=E\\left[I_{A}\\left(N_{t}-N_{s}\\right)\\right] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3795ecf-5f5b-4217-9544-a3e7af06617c",
   "metadata": {},
   "source": [
    "$A_{t}=\\int_{0}^{t} \\lambda_{s} d s$ is the compensator process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56087a4f-16ce-40fa-adff-62092d8c1098",
   "metadata": {},
   "source": [
    "> Assume that  $N$  has intensity  $\\lambda^{*}$. Then it assumes  predictable intensity  $\\lambda$ and is **unique**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ee761-c3d4-48fc-ad43-5004d8cf5953",
   "metadata": {},
   "source": [
    "This can be seen directly from \n",
    "\n",
    "$$ \\mu_{t}=\\lambda_{t-} \\implies E\\left[\\int_{0}^{\\infty} h_{t} \\lambda_{t} d t\\right]=E\\left[\\int_{0}^{\\infty} h_{t} \\mu_{t} d t\\right]. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fb65f-2362-4d77-b823-c8867649aa7a",
   "metadata": {},
   "source": [
    "Assume that  $N$  has the predictable intensity process  $\\lambda$. Modulo integrability, this implies that $d N_{t}-\\lambda d t$ is a martingale increment, and heuristically \n",
    "\n",
    "$$E\\left[d N_{t}-\\lambda d t \\mid \\mathcal{F}_{t-}\\right]=0 \\implies E\\left[d N_{t} \\mid \\mathcal{F}_{t-}\\right]=\\lambda_{t} d t .$$\n",
    "\n",
    "Thus, $\\lambda$  has the interpretation that  **$\\lambda_{t}$  is the conditionally expected number of jumps per unit of time**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b92552-c93c-4ed4-9eeb-1f7f95dbcf46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb83bd-a20c-4fbb-b597-4eef2ef6e9fe",
   "metadata": {},
   "source": [
    "We shall give a connection of stochastic intensity function and conditional intensity function as is defined below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69058b-b8db-4c39-abd6-5c46f876ea85",
   "metadata": {},
   "source": [
    "\n",
    "> Set $\\mathcal{H}_{t}$ = the list of time events  $\\left\\{t_{1}, \\ldots, t_{n}\\right\\}$  up to time  $t$.\n",
    ">\n",
    "> For counting process $N(t)$,  the **conditional intensity function** is \n",
    ">\n",
    "> $$ \\lambda^{*}(t) = \\lambda^{*}(t, \\omega) = \\lim _{h \\searrow 0} \\frac{\\mathbb{E}[N(t+h)-N(t) \\mid \\mathcal{H}(t)]}{h}.$$\n",
    "\n",
    "An alternative way of definition is to use CDF. Set distribution function of $T_{k+1}$ given the history up to $\\mathcal{H}_{u}$ where last jump  $k$  taken at time  $u$ as\n",
    "\n",
    "$$F^{*}\\left(t \\mid \\mathcal{H}_{u}\\right)=\\int_{u}^{t} \\mathbb{P}\\left[s \\leq T_{k+1} \\leq s+d s \\mid \\mathcal{H}_{u}\\right] d s=\\int_{u}^{t} f^{*}\\left(s \\mid \\mathcal{H}_{u}\\right) d s.$$\n",
    "\n",
    "> We have the decomposition $f\\left(t_{1}, t_{2}, \\ldots, t_{k}\\right)=\\prod_{i=1}^{k} f^{*}\\left(t_{i} \\mid \\mathcal{H}_{u}\\right)$.\n",
    "\n",
    "Then we know \n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\lambda^{*}(t) = & E\\left[N(d t) \\mid \\mathcal{H}_{t}\\right] = \\mathbb{P}\\left(\\text { point in } \\mathrm{dt} \\mid \\mathcal{H}_{t}\\right) = \\mathbb{P}\\left(\\text { point in } \\mathrm{dt} \\mid \\text { point not before } \\mathrm{t}, \\mathcal{H}_{t}\\right) \\\\\n",
    "= & \\frac{\\mathbb{P}\\left(\\text { point in } \\mathrm{dt} \\text {, point not before } \\mathrm{t} \\mid \\mathcal{H}_{t}\\right)}{\\mathbb{P}\\left(\\text { point not before } \\mathrm{t} \\mid \\mathcal{H}_{t}\\right)} = \\frac{\\mathbb{P}\\left(\\text { point in } \\mathrm{dt} \\mid \\mathcal{H}_{t}\\right)}{\\mathbb{P}\\left(\\text { point not before } \\mathrm{t} \\mid \\mathcal{H}_{t}\\right)} = \\frac{f^{*}\\left(t \\mid \\mathcal{H}_{t}\\right)}{1-F^{*}\\left(t \\mid \\mathcal{H}_{t}\\right)}.\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "> Therefore, we have the second definition:\n",
    ">\n",
    "> $$\\lambda^{*}\\left(t \\mid \\mathcal{H}_{t}\\right)=\\frac{f^{*}\\left(t \\mid \\mathcal{H}_{t}\\right)}{1-F^{*}\\left(t \\mid \\mathcal{H}_{t}\\right)} = -\\frac{\\dot{S}^{*}\\left(t \\mid \\mathcal{H}_{t}\\right)}{S^{*}\\left(t \\mid \\mathcal{H}_{t}\\right)}=-\\frac{d}{d t} \\ln \\left(S^{*}\\left(t \\mid \\mathcal{H}_{t}\\right)\\right)$$ \n",
    ">\n",
    "> where we give it an interpretation from **survial analysis**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0418c-4362-4b47-b7c8-7361b686b478",
   "metadata": {},
   "source": [
    "Usually we interpret it as its left-continuous modification $\\lambda^{*}\\left(t^{-}\\right)$, which equals the **stochastic Intensity**:\n",
    "\n",
    "$$ \\lambda^{*}\\left(t^{-}\\right) = \\lim _{h \\searrow 0} \\frac{\\mathbb{E}[N(t)-N(t-h) \\mid \\mathcal{H}(t-h)]}{h} = \\frac{E\\left[d N_{t} \\mid \\mathcal{F}_{t-}\\right]}{d t}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7165565-c7a5-425e-aa4c-4393fde858e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d60b25-d230-45ca-a904-c57e7bcf1f4a",
   "metadata": {},
   "source": [
    "### 3. Hawkes Process\n",
    "\n",
    "A **Hawkes Process** is a generalization of Poission process.\n",
    "\n",
    "> A **Hawkes Process** is   a counting process $(N(t): t \\geqslant 0)$, with associated history  $(\\mathcal{H}(t): t \\geqslant 0)$, s.t.\n",
    ">\n",
    "> $$\n",
    "\\mathbb{P}(N(t+h)-N(t)=m \\mid \\mathcal{H}(t))=\\left\\{\\begin{array}{ll}\n",
    "1-\\lambda^{*}(t) h+\\mathrm{o}(h), & m=0 \\\\\n",
    "\\lambda^{*}(t) h+\\mathrm{o}(h), & m=1 \\\\\n",
    "\\mathrm{o}(h), & m>1.\n",
    "\\end{array}\\right. $$\n",
    "\n",
    "Here we supppose $\\lambda^{*}(t)=\\lambda+\\int_{0}^{t} \\mu(t-u) \\mathrm{d} N(u)$ where  $\\lambda>0$  and  $\\mu:(0, \\infty) \\rightarrow[0, \\infty)$ are **background intensity** and **excitation function** respectively.  $\\mu(\\cdot) = 0 $ refers to a homogeneous Poisson process, which is the case we shall not study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe51519-47c3-4db1-8620-993679b9893b",
   "metadata": {},
   "source": [
    "> **Thm.** The likelihood $L$ of regular $N(\\cdot)$ is expressible in the form $L=\\left[\\prod_{i=1}^{k} \\lambda^{*}\\left(t_{i}\\right)\\right] \\exp \\left(-\\int_{0}^{T} \\lambda^{*}(u) \\mathrm{d} u\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e8744-cd8b-4851-a15c-90da9649b3d7",
   "metadata": {},
   "source": [
    "**Proof.** From definition,\n",
    "\n",
    "$$ \\lambda^{*}(t) = -\\frac{\\mathrm{d} \\log \\left(1-F^{*}(t)\\right)}{\\mathrm{d} t} \\implies -\\int_{t_{k}}^{t} \\lambda^{*}(u) \\mathrm{d} u=\\log \\left(1-F^{*}(t)\\right)-\\log \\left(1-F^{*}\\left(t_{k}\\right)\\right) .\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd19eb-a592-4d76-b8ff-96d3b51d2d2d",
   "metadata": {},
   "source": [
    "Since we know $F^{*}\\left(t_{k}\\right)=0$  as  $T_{k+1}>t_{k}$, then $-\\int_{t_{k}}^{t} \\lambda^{*}(u) \\mathrm{d} u=\\log \\left(1-F^{*}(t)\\right)$. Further rearranging yields\n",
    "\n",
    "$$F^{*}(t)=1-\\exp \\left(-\\int_{t_{k}}^{t} \\lambda^{*}(u) \\mathrm{d} u\\right), \\quad f^{*}(t)=\\lambda^{*}(t) \\exp \\left(-\\int_{t_{k}}^{t} \\lambda^{*}(u) \\mathrm{d} u\\right) .$$\n",
    "\n",
    "Then we know \n",
    "\n",
    "$$ \\begin{aligned}\n",
    "L & = \\left[\\prod_{i=1}^{k} f^{*}\\left(t_{i}\\right)\\right]\\left(1-F^{*}(T)\\right) = \\prod_{i=1}^{k} \\lambda^{*}\\left(t_{i}\\right) \\exp \\left(-\\int_{t_{i-1}}^{t_{i}} \\lambda^{*}(u) \\mathrm{d} u\\right) \\exp \\left(-\\int_{t_{k}}^{T} \\lambda^{*}(u) \\mathrm{d} u\\right) \\\\\n",
    "& = \\left[\\prod_{i=1}^{k} \\lambda^{*}\\left(t_{i}\\right)\\right] \\exp \\left(-\\int_{0}^{T} \\lambda^{*}(u) \\mathrm{d} u\\right).\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1f421-858f-40af-ba62-bc8bed5919a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd52f9-e625-40a9-af67-784ecfd0cacf",
   "metadata": {},
   "source": [
    "We consider **exponentially decaying intensity** subsequently where $\\lambda^{*}(t)=\\lambda+\\int_{0}^{t} \\alpha \\mathrm{e}^{-\\beta(t-s)} \\mathrm{d} N(s)=\\lambda+\\sum_{t_{i}<t} \\alpha \\mathrm{e}^{-\\beta\\left(t-t_{i}\\right)}$.\n",
    "\n",
    "A generalization is to consider $\\mathrm{d} \\lambda^{*}(t)=\\beta\\left(\\lambda-\\lambda^{*}(t)\\right) \\mathrm{d} t+\\alpha \\mathrm{d} N(t)$, calculation yields\n",
    "\n",
    "$$\\lambda^{*}(t)=\\mathrm{e}^{-\\beta t}\\left(\\lambda_{0}-\\lambda\\right)+\\lambda+\\int_{0}^{t} \\alpha \\mathrm{e}^{\\beta(t-s)} \\mathrm{d} N(s), \\quad t \\geq 0.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2102854-703a-4a17-9140-57de8358bbc9",
   "metadata": {},
   "source": [
    "$\\lambda^{*}(t) = \\lambda+\\sum_{t_{i}<t} \\alpha \\mathrm{e}^{-\\beta\\left(t-t_{i}\\right)}$ admits a **immigration–birth representation (branching process)**.\n",
    "\n",
    "> Let  $Z_{i}$  = # offspring in the  $i$-th generation with  $Z_{0}=1$. \n",
    "\n",
    "Set  $Z_{1} \\sim \\operatorname{Poi}(n)$  where the mean  $n=\\int_{0}^{\\infty} \\alpha \\mathrm{e}^{-\\beta s} \\mathrm{~d} s=\\frac{\\alpha}{\\beta}$  is known as the **branching ratio**. The value of $n$ determines whether or not the Hawkes process explodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf902d25-5ec1-49af-864e-790dea40e3e8",
   "metadata": {},
   "source": [
    "> $$g(s)=\\mathbb{E}\\left[\\lambda^{*}(s)\\right]=\\frac{\\mathbb{E}[\\mathbb{E}[N_s(ds) \\mid \\mathcal{H}(s)]]}{\\mathrm{d} s}=\\frac{\\mathbb{E}[N_s(ds)]}{\\mathrm{d} s} \\implies \\mathbb{E}[N_s(ds)]=g(s) \\mathrm{d} s.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8df3d-565c-43ba-aa9f-7b32a171b4db",
   "metadata": {},
   "source": [
    "Note that $N_{s}(t)=N(s+t)-N(s)$. From above, $$g(t)=\\mathbb{E}\\left[\\lambda^{*}(t)\\right]=\\mathbb{E}\\left[\\lambda+\\int_{0}^{t} \\mu(t-s) \\mathrm{d} N(s)\\right] = \\lambda+\\int_{0}^{t} \\mu(t-s) N_s(ds),$$\n",
    "\n",
    "which follows that $g(t)=\\lambda+\\int_{0}^{t} \\mu(t-s) g(s) \\mathrm{d} s=\\lambda+\\int_{0}^{t} g(t-s) \\mu(s) \\mathrm{d} s$, i.e. we obtained the renewal–type equation $g=\\lambda+g \\star \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb50b4-92d0-4e8f-8969-73d6ce600fee",
   "metadata": {},
   "source": [
    "> For the defective case $n<1$, $g(t)=\\mathbb{E}\\left[\\lambda^{*}(t)\\right] \\rightarrow \\frac{\\lambda}{1-n}$.\n",
    ">\n",
    "> For the excessive case $n>1$, $\\lambda^{*}(t) \\rightarrow \\infty$ exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099220ee-bddc-4870-bd56-2ad1092170cf",
   "metadata": {},
   "source": [
    "Viewed as a branching process, this can be seen clearly. (using $\\mathbb{E}\\left[Z_{i}\\right]=n^{i}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f3036-c521-430e-91a4-c938b248f1ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Simulation Method\n",
    "\n",
    "We focus on the typical simulation method below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32648297-d9ec-41f7-9093-7d4c771663e2",
   "metadata": {},
   "source": [
    "> For general point processes, a simulation algorithm is suggested by **the converse of the random time\n",
    "change theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56bd487-1801-4421-aa22-7fca5c36b602",
   "metadata": {},
   "source": [
    "Specifically for HPs, **the inverse compensator method** iteratively solves \n",
    "\n",
    "$$t_{1}^{*}=\\int_{0}^{t_{1}} \\lambda^{*}(s) \\mathrm{d} s, \\quad t_{k+1}^{*}-t_{k}^{*}=\\int_{t_{k}}^{t_{k+1}} \\lambda^{*}(s) \\mathrm{d} s.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce001e10-355c-482e-907e-41f03599a747",
   "metadata": {},
   "source": [
    "Since \n",
    "$$\\int_{t_{k}}^{t} \\lambda^{*}(u) \\mathrm{d} u=-\\log \\left(1-F^{*}(t)\\right) \\implies \\int_{t_{k}}^{t_{k+1}} \\lambda^{*}(u) \\mathrm{d} u=-\\log (U),$$\n",
    "where $U \\sim \\operatorname{Unif}[0,1]$. For an exponentially decaying intensity the equation becomes\n",
    "\n",
    "$$\\log (U)+\\lambda\\left(t_{k+1}-t_{k}\\right)-\\frac{\\alpha}{\\beta}\\left(\\sum_{i=1}^{k} \\mathrm{e}^{\\beta\\left(t_{k-1}-t_{i}\\right)}-\\sum_{i=1}^{k} \\mathrm{e}^{-\\beta\\left(t_{k}-t_{i}\\right)}\\right)=0 .$$\n",
    "\n",
    "Solving for $t_{k+1}$ can be achieved in linear time using the recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "2. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "3. Geoff K. Nicholls. Bayes Methods.\n",
    "4. Patrick J. Laub. Hawkes Processes.\n",
    "5. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
