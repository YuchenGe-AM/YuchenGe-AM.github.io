{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Feb 23] Causal Inference and Transfer Learning III\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c723a98-16e1-4224-997f-2b80431b4d02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.  Basic Aspects of Bayesian Method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42668b62-7326-44b1-8d4e-ee3d2dcc4664",
   "metadata": {},
   "source": [
    "In a Bayesian setting, we have some unknown real-world quantity  $\\Theta$  takes values in a parameter space  $\\Omega$. Typically  $\\Omega \\subset R^{p}$. \n",
    "\n",
    "> Let  $S \\subseteq \\Omega$, is  $\\Theta$  in  $S$  ?\n",
    "\n",
    "If knowledge of the world is coherent (as is shown below), then $\\exists$ a unique **prior distribution** with density  $\\pi(\\theta)$  on  $\\Omega$ and \n",
    "\n",
    "> $$\\int_{S} \\pi(\\theta) d \\theta=\\operatorname{Pr}(\\Theta \\in S). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fab03-631b-401a-9ce7-15217dab004f",
   "metadata": {},
   "source": [
    "Observations  $Y=\\left(Y_{1}, \\ldots, Y_{n}\\right)$, $Y \\in \\mathcal{Y}$  are distributed according to an observation model with probability density  $p(y \\mid \\theta)$  for realisation  $Y=y$  with  $y=\\left(y_{1}, \\ldots, y_{n}\\right)$  given  $\\Theta=\\theta$. If the observations are iid then we have an **observation model** with probability density\n",
    "\n",
    "$$p(y \\mid \\theta)=\\prod_{i=1}^{n} p\\left(y_{i} \\mid \\theta\\right).$$\n",
    "\n",
    "Therefore, our beliefs about $\\Theta \\in S$ change with the **posterior density** of the posterior distribution being\n",
    "\n",
    "$$\\pi(S \\mid y)=\\operatorname{Pr}(\\Theta \\in S \\mid Y=y) = \\int_S \\pi(\\theta \\mid y) = \\int_S \\frac{p(y \\mid \\theta) \\pi(\\theta)}{p(y)}$$\n",
    "\n",
    "where $p(y)=\\int_{\\Omega} p(y \\mid \\theta) \\pi(\\theta) d \\theta$ is the normalising marginal likelihood (also, **the prior predictive distribution**). Answers to questions about  $\\Theta$  can be given in terms of  $\\pi(\\theta \\mid y)$ via $\\operatorname{Pr}(\\Theta \\in S \\mid Y=y)=\\int_{S} \\pi(\\theta \\mid y) d \\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efe07c-5315-4acf-a954-496a1226ed19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561ad4e-6763-4e90-afac-6cd06e024415",
   "metadata": {},
   "source": [
    "Given observation model  $Y \\sim p(\\cdot \\mid \\theta)$, the  $\\Theta$-estimator $\\delta: \\mathcal{Y} \\rightarrow \\mathbb{R}^{p}$, has risk  $\\mathcal{R}(\\theta, \\delta)$  at $\\Theta=\\theta$  given by\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathcal{R}(\\theta, \\delta) & =E_{Y \\mid \\Theta=\\theta}(L(\\theta, \\delta(Y))) \\\\\n",
    "& =\\int_{\\mathcal{Y}} L(\\theta, \\delta(y)) p(y \\mid \\theta) d y .\n",
    "\\end{aligned}$$\n",
    "\n",
    ">  The Bayes risk $\\rho(\\pi, \\delta)$ is the risk averaged over the prior,\n",
    ">\n",
    "> $$\\begin{aligned}\n",
    "\\rho(\\pi, \\delta) & =E_{\\Theta}(\\mathcal{R}(\\theta, \\delta)) =E_{\\Theta, Y}(L(\\Theta, \\delta(Y))) \\\\\n",
    "& =\\int_{\\Omega} \\int_{\\mathcal{Y}} L(\\theta, \\delta(y)) p(y \\mid \\theta) \\pi(\\theta) d y d \\theta .\n",
    "\\end{aligned}$$\n",
    "> \n",
    "> where we have a prior  $\\pi(\\theta)$, posterior  $\\pi(\\theta \\mid y)$  and marginal likelihood  $p(y)$.\n",
    "\n",
    "\n",
    "A Bayes estimator  $\\delta^{\\pi}$  for  $\\theta$  minimises the Bayes risk $\\delta^{\\pi}=\\arg \\min _{\\delta} \\rho(\\pi, \\delta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ae5b2-d098-4a35-9e81-ed2971f7f08c",
   "metadata": {},
   "source": [
    "This is not straightforward as $\\delta$ is a function, then we consider the **expected posterior\n",
    "loss**\n",
    "$$\\rho(\\pi, \\delta \\mid y) = E_{\\Theta \\mid Y=y}(L(\\Theta, \\delta(y))) =\\int_{\\Omega} L(\\theta, \\delta(y)) \\pi(\\theta \\mid y) d \\theta \\implies \\rho(\\pi, \\delta)=\\int_{\\mathcal{Y}} \\rho(\\pi, \\delta \\mid y) p(y) d y.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112e472-c571-49bb-b891-72f163b6f229",
   "metadata": {},
   "source": [
    "> $\\delta^{\\pi}(y)=\\arg \\min _{\\delta} \\rho(\\pi, \\delta \\mid y)$ minimizes the Bayes risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ffcf6-0cb9-48ed-9e25-0d7a53329f66",
   "metadata": {},
   "source": [
    "For example, let $L(\\Theta, \\delta(Y))=(\\Theta-\\delta(Y))^{2}$, in this case $ E_{\\Theta \\mid y}(L(\\Theta, \\delta)) $ is minimised over actions by the posterior mean  $\\delta^{*}=E_{\\Theta \\mid y}(\\Theta)$  (just differentiate wrt  $\\delta(y)$  at fixed  $y$  ). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4764d-1a1b-42e5-9751-f24296202324",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Suppose we wish to estimate the expectation w.r.t. $f(\\theta)$. If the loss is the square error then we estimate  $f(\\Theta)$  with the posterior mean $ E_{\\Theta \\mid Y=y}(f(\\Theta))$. Simulate  $\\theta^{(t)} \\sim \\pi(\\cdot \\mid y)$, $t=1, \\ldots, T$  and compute\n",
    "$$\\hat{f}=\\frac{1}{T} \\sum_{t=1}^{T} f\\left(\\theta^{(t)}\\right) .$$\n",
    "\n",
    "> For example, if  $S \\in \\mathcal{B}_{\\Omega}$  and  $f(\\theta)=\\mathbb{I}_{\\theta \\in S}$  then  $\\hat{f}$  estimates  $\\pi(S \\mid y)$.\n",
    "\n",
    "We also commonly report posterior credible sets in order to quantify uncertainty. A level-$\\alpha$  Highest Posterior Density (HPD) credible set  $C_{\\alpha}$  satisfies\n",
    "\n",
    "$$\\int_{\\Omega \\cap C_{\\alpha}} \\pi(\\theta \\mid y) d \\theta=1-\\alpha, \\text { and } \\theta \\in C_{\\alpha}, \\theta^{\\prime} \\in \\Omega \\backslash C_{\\alpha} \\Rightarrow \\pi(\\theta \\mid y) \\geq \\pi\\left(\\theta^{\\prime} \\mid y\\right) .$$\n",
    "\n",
    "An HPD set (or general credible set with fixed posterior probability mass) is qualitatively\n",
    "different in meaning from a frequentist confidence interval since it involves the **the probability of parameters**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8a3cb-17a4-4057-a0dc-aa3ab2e2f77d",
   "metadata": {},
   "source": [
    "> The HPD set can be estimated from Monte Carlo samples  $\\theta^{(t)} \\sim \\pi(\\cdot \\mid y)$. Specifically, the HPD set is a Bayes estimator by supposing the action space is  $\\delta \\in \\Delta = \\left\\{A \\in \\mathcal{B}_{\\Omega}: \\pi(A \\mid y)=1-\\alpha\\right\\}$ and the loss  $L(\\Theta, \\delta)=\\mathbb{I}_{\\Theta \\notin \\delta}+|\\delta|$  where  $|\\delta|=\\int_{\\delta} d \\theta$. \n",
    ">\n",
    "> The expected posterior loss is minimised over the action space by  $\\delta^{*}=C_{\\alpha}$,  an HPD set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2462fb9-6e78-42f1-b238-e0ecb7ce6aca",
   "metadata": {},
   "source": [
    "After that, we obtain the **posterior predictive distribution** of the data $p\\left(y^{\\prime} \\mid y\\right)=\\int_{\\Omega} p\\left(y^{\\prime} \\mid \\theta\\right) \\pi(\\theta \\mid y) d \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da905d-23c8-4537-955e-87c162ee0215",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00c2d1-f293-48c9-b0ea-c2985a776320",
   "metadata": {},
   "source": [
    "For model  $m$, we have a parameter prior  $\\Theta \\sim   \\pi(\\theta \\mid m)$ and an observation model  $Y \\sim p(y \\mid \\theta, m)$. The parameter space may vary from model to model. \n",
    "\n",
    "> The \"model\" is the joint model for the \"generative process\" for  $\\Theta, Y$, with joint density  $\\pi(\\theta \\mid m) p(y \\mid \\theta, m)$  and state space  $\\Omega_{m} \\times \\mathcal{Y}$. \n",
    "\n",
    "Conditioning on  $Y=y$  we get the posterior under model  $M=m$,\n",
    "\n",
    "$$\\pi(\\theta \\mid y, m)=\\frac{p(y \\mid \\theta, m) \\pi(\\theta \\mid m)}{p(y \\mid m)}$$\n",
    "\n",
    "with $p(y \\mid m)=\\int_{\\Omega_{m}} p(y \\mid \\theta, m) \\pi(\\theta \\mid m) d \\theta$. Similarly, the posterior model probability is $\\pi(m \\mid y)= p(y \\mid m) \\pi_{M}(m) /p(y)$ where  $\\pi_{M}(m)$  is our prior probability and $p(y)=\\sum_{m \\in \\mathcal{M}} p(y \\mid m) \\pi_{M}(m)$.\n",
    "\n",
    "\n",
    "> Under the  $0-1$  loss function with truth  $M$  and action  $\\delta \\in \\mathcal{M}$  the loss is  $L(M, \\delta)=\\mathbb{I}_{M \\neq \\delta}$.  The Bayes estimator is the\n",
    "maximum a posteriori (MAP) model.\n",
    "\n",
    "This can be seen that the expected posterior loss  $E_{M \\mid y}(L(M, \\delta))=1-\\pi(\\delta \\mid y)$  and this is minimised by the choice  $\\delta=m^{*}$  the most probable model a posteriori.\n",
    "\n",
    "> The model averaged posterior which allows for uncertainty is $\\pi(\\theta \\mid y)=\\sum_{m \\in \\mathcal{M}} \\pi(\\theta \\mid y, m) \\pi_{M}(m \\mid y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873edb61-3dab-4070-81d7-b609a3c61146",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42d1c6-1c02-4ae4-a471-b6ac9837eb4f",
   "metadata": {},
   "source": [
    "### 2. MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cecd1ea-e004-47a9-bf58-2e3aaa0d8885",
   "metadata": {},
   "source": [
    "MCMC is a family of algorithms for simulating $X_{t} \\xrightarrow{D} p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50f2d9-5420-43c2-95dd-0a5be96e80c9",
   "metadata": {},
   "source": [
    "Write $P_{i, j}=\\mathbb{P}\\left(X_{t+1}=j \\mid X_{t}=i\\right)$ for a homogeneous Markov chain on $\\Omega$. \n",
    "\n",
    "> $p_i$ is **detailed balance** if $p_{i} P_{i, j}=p_{j} P_{j, i} \\text { holds for all } i, j \\in \\Omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0869e5e-a907-4cb3-8019-fe7668bb4a92",
   "metadata": {},
   "source": [
    "As a consequence, $p$ is stationary for $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891b2bd-11b5-48c3-81d1-6453d58a9a4f",
   "metadata": {},
   "source": [
    "> Suppose $\\left\\{X_{t}\\right\\}_{t=0}^{\\infty}$ is irreducible and aperiodic on $\\Omega$ satisfying detailed balance w.r.t. $p$. Then \n",
    ">\n",
    "> $$ \\hat{f}_{T} = T^{-1} \\sum_{t} f\\left(X_{t}\\right) \\xrightarrow{\\text { a.s. }} E_{X \\sim p}(f(X)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5011f5-58a0-4e3d-b04b-696156977fca",
   "metadata": {},
   "source": [
    "Then we propose MCMC.\n",
    "\n",
    "> Given a proposal $Q$ s.t. $q(j \\mid i)>0 \\Leftrightarrow q(i \\mid j)>0$.\n",
    ">\n",
    "> Let  $X_{t}=i$. The next state  $X_{t+1}$  is realised below\n",
    "> 1. Draw  $j \\sim q(\\cdot \\mid i)$  and  $u \\sim U[0,1]$.\n",
    "> 2. If  $u \\leq \\alpha(j \\mid i)$  where $\\alpha(j \\mid i)=\\min \\left\\{1, \\frac{p_{j} q(i \\mid j)}{p_{i} q(j \\mid i)}\\right\\}$, then set  $X_{t+1}=j$ , otherwise set  $X_{t+1}=i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e96d3a-b2b7-4706-a1b6-d5005d987116",
   "metadata": {},
   "source": [
    "Seeing that $P_{i, j}=P\\left(X_{t+1}=j \\mid X_{t}=i\\right)=q(j \\mid i) \\alpha(j \\mid i)$. Then \n",
    "\n",
    "$$\\begin{aligned}\n",
    "p_{i} P_{i, j} & =p_{i} q(j \\mid i) \\alpha(j \\mid i) = p_{i} q(j \\mid i) \\min \\left\\{1, \\frac{p_{j} q(i \\mid j)}{p_{i} q(j \\mid i)}\\right\\} = \\min \\left\\{p_{i} q(j \\mid i), p_{j} q(i \\mid j)\\right\\} \\\\\n",
    "& \\left.=p_{j} q(i \\mid j) \\min \\left\\{\\frac{p_{i} q(j \\mid i)}{p_{j} q(i \\mid j)}, 1\\right)\\right\\} =p_{j} q(i \\mid j) \\alpha(i \\mid j) =p_{j} P_{j, i}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fdbbd0-1fb4-4481-98f0-1ec0dad961d9",
   "metadata": {},
   "source": [
    "and we are done. In continuous case,\n",
    "\n",
    "> The transition kernel for Metropolis-Hasting MCMC is\n",
    ">\n",
    "> $$K\\left(\\theta, d \\theta^{\\prime}\\right)=\\alpha\\left(\\theta^{\\prime} \\mid \\theta\\right) q\\left(d \\theta^{\\prime} \\mid \\theta\\right)+c(\\theta) \\delta_{\\theta}\\left(d \\theta^{\\prime}\\right),$$\n",
    ">\n",
    "> where $\\alpha\\left(\\theta^{\\prime} \\mid \\theta\\right)=\\min \\left\\{1, \\frac{p\\left(\\theta^{\\prime}\\right) q\\left(\\theta \\mid \\theta^{\\prime}\\right)}{p\\left(\\theta^{\\prime}\\right) q\\left(\\theta^{\\prime} \\mid \\theta\\right)}\\right\\}$, $c(\\theta)=1-\\int_{\\Omega} \\alpha\\left(\\theta^{\\prime} \\mid \\theta\\right) q\\left(d \\theta^{\\prime} \\mid \\theta\\right)$, and $\\int_{A} \\delta_{\\theta}\\left(d \\theta^{\\prime}\\right)=\\mathbb{I}_{\\theta^{\\prime} \\in A}$.\n",
    ">\n",
    "> The detailed balance is satisfied when $p\\left(d \\theta^{\\prime}\\right) K\\left(\\theta^{\\prime}, d \\theta\\right)=p(d \\theta) K\\left(\\theta, d \\theta^{\\prime}\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce5165-2cb3-41b4-a2bc-f8506eb28ec7",
   "metadata": {},
   "source": [
    "For $\\theta=\\left(\\theta_{1}, \\ldots, \\theta_{p}\\right)$ with $\\geq$ one dimension, choose a kernel at random from  $K_{1}, \\ldots, K_{N}$ where\n",
    "\n",
    "$$K_{k}\\left(\\theta, d \\theta^{\\prime}\\right)=\\alpha_{k}\\left(\\theta^{\\prime} \\mid \\theta\\right) q_{k}\\left(d \\theta^{\\prime} \\mid \\theta\\right)+c_{k}(\\theta) \\delta_{\\theta}\\left(d \\theta^{\\prime}\\right)$$\n",
    "\n",
    "The overall kernel is $K\\left(\\theta, d \\theta^{\\prime}\\right)=\\sum_{k=1}^{N} \\xi_{k} K_{k}\\left(\\theta, d \\theta^{\\prime}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f7866-c2f4-46d4-9edc-122c4970d213",
   "metadata": {},
   "source": [
    "\n",
    "> The Gibbs sampler is a special case of the multi-dim case where $\\theta_{i}^{\\prime} \\sim \\pi\\left(\\cdot \\mid \\theta_{-i}\\right)$ and $\\theta_{-i}^{\\prime}=\\theta_{-i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a23217-a429-4eda-995f-315f9908e201",
   "metadata": {},
   "source": [
    "Important applications of the Gibbs sampler arise for missing data. Suppose the observation process is\n",
    "\n",
    "$$z \\sim p(z \\mid \\theta), \\quad y \\sim p(y \\mid z, \\theta)$$\n",
    "\n",
    "The posterior  $\\pi(\\theta \\mid y)$  is awkward as $\\pi(\\theta \\mid y) \\propto \\pi(\\theta) \\int p(y \\mid z, \\theta) p(z \\mid \\theta) d z$. \n",
    "\n",
    "> In data augmentation we work with $p(\\theta, z \\mid y)$, thinking of missing data as another parameter: the posterior is simply\n",
    ">\n",
    "> $$p(\\theta, z \\mid y) \\propto p(y \\mid z, \\theta) p(z \\mid \\theta) p(\\theta) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe23b3-6a65-40fb-812c-db478f9f2412",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df08e7-576a-4bfd-a157-eef7ee4c1cf7",
   "metadata": {},
   "source": [
    "### 3. Exchangibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb2697-08a6-4271-8bc7-65a108c3fbb8",
   "metadata": {},
   "source": [
    "> An infinite exchangeable sequence of $\\left\\{X_{i}\\right\\}_{i=1}^{\\infty}$  is an infinite sequence s.t. $X_{1}, X_{2}, \\ldots, X_{n}$  are exchangeable for every  $n \\geq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6a533-4a51-41f3-941d-ad6ff15a8369",
   "metadata": {},
   "source": [
    "Suppose the data forms an exchangeable sequence, then $\\exists$ a generative model\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Theta & \\sim F \\\\\n",
    "X_{i} \\mid \\Theta=\\theta & \\sim p(\\cdot \\mid \\theta), \\quad \\text { iid for } i=1, \\ldots, n\n",
    "\\end{aligned}$$\n",
    "\n",
    "for the data. These distributions all exist by **de Finetti**. If  $x=\\left(x_{1}, \\ldots, x_{n}\\right)$, the likelihood is\n",
    "\n",
    "$$p(x \\mid \\theta)=\\prod_{i} p\\left(x_{i} \\mid \\theta\\right) .$$\n",
    "\n",
    "\n",
    "Here  F  is \"nature's prior\", which may not coincide with our own prior  $\\pi$. However, de Finetti gives the form for the prior predictive distribution of the data\n",
    "\n",
    "$$p_{1: n}\\left(x_{1}, \\ldots, x_{n}\\right)=\\int p\\left(x_{1}, \\ldots, x_{n} \\mid \\theta\\right) d F(\\theta) .$$\n",
    "\n",
    "This is of course the marginal likelihood. Suppose we have some priors  $\\pi(\\theta \\mid m), m \\in \\mathcal{M}$  and we carry out model selection using the marginal likelihood\n",
    "\n",
    "$$p(x \\mid M=m)=\\int p(x \\mid \\theta) \\pi(\\theta \\mid M=m) d \\theta .$$\n",
    "\n",
    "When we choose a model  $M=m$, we are forming an estimate  $\\pi(\\theta \\mid M=m)$  for  $F$ the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470de4e4-c9dc-4022-a7c1-254d0c6ce90d",
   "metadata": {},
   "source": [
    "We also get an expression for the posterior in terms of  $F$. Given $x_{1: m}=\\left(x_{1}, \\ldots, x_{m}\\right)$  and we wish to predict  $x_{m+1: n}=\\left(x_{m+1}, \\ldots, x_{n}\\right)$. The posterior predictive distribution is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "p\\left(x_{m+1: n} \\mid x_{1: m}\\right) & =p\\left(x_{1: n}\\right) / p\\left(x_{1: m}\\right) \\\\\n",
    "& =\\int p\\left(x_{m+1: n} \\mid \\theta\\right) \\frac{p\\left(x_{1: m} \\mid \\theta\\right) d F(\\theta)}{p\\left(x_{1: m}\\right)} \\\\\n",
    "& =\\int p\\left(x_{m+1: n} \\mid \\theta\\right) d \\tilde{F}(\\theta) .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "After $X_{1}=x_{1}, \\ldots X_{m}=x_{m}$, $\\exists$ a generative model\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Theta \\mid X_{1: m} & \\sim \\tilde{F}(\\theta) \\\\\n",
    "X_{i} \\mid \\Theta=\\theta & \\sim p(\\cdot \\mid \\theta), \\quad \\text { iid for } i=m+1, \\ldots, n .\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Here $d \\tilde{F}(\\theta) \\propto p\\left(x_{1}, \\ldots, x_{m} \\mid \\theta\\right) d F(\\theta)$ is the updated true generative model for the parameter  $\\Theta \\mid X_{1: m}$, or in other words, the posterior. **de Finetti** tells us that Bayesian inference is possible in this exchangeable setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70882d72-f204-4184-9c97-5285cfcc9801",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Model Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b8b0d1-0ed6-4147-afeb-9739b8d17193",
   "metadata": {},
   "source": [
    "> The model-averaged posterior  is $\\pi(\\theta \\mid y)=\\sum_{m \\in \\mathcal{M}} \\pi(\\theta, m \\mid y), \\theta \\in \\Omega$ where $\\Omega=\\bigcup_{m \\in \\mathcal{M}} \\Omega_{m}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc1b4b-e6f8-46ff-bb57-794ea3b90903",
   "metadata": {},
   "source": [
    "Then we claim that model averaging is preferred to inference after model selection. Consider estimating $h(\\theta)$. Then we have the posterior mean and the single-model posterior mean\n",
    "\n",
    "$$E_{\\theta, m \\mid y}(h(\\theta))=\\sum_{m \\in \\mathcal{M}} \\int_{\\Omega_{m}} h(\\theta) \\pi(\\theta, m \\mid y) d \\theta, \\quad E_{\\theta \\mid y, m^{*}}(h(\\theta))=\\int_{\\Omega_{m^{*}}} h(\\theta) \\pi\\left(\\theta \\mid y, m^{*}\\right) d \\theta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880f121-2cc6-42be-a22f-ea22b83af595",
   "metadata": {},
   "source": [
    "\n",
    "If the loss for estimating $\\delta$  when the truth is  $h$  is  $(h-\\delta)^{2}$,  then the Bayes risk  $\\rho(\\pi, \\delta(y))$  allowing for model and parameter uncertainty is minimised by  $E_{\\theta, m \\mid y}(h(\\theta))$  and  $\\rho\\left(\\pi, E_{\\theta \\mid y, m}(h)\\right) \\geq \\rho\\left(\\pi, E_{\\theta, m \\mid y}(h)\\right)$  for every  $m \\in \\mathcal{M}$.\n",
    "\n",
    "**Proof.** Recall that the Bayes risk is minimised by the estimator minimising the expected posterior loss  $\\rho(\\pi, \\delta \\mid y)$  at every $ y \\in \\mathcal{Y}$. This is\n",
    "\n",
    "$$\\rho(\\pi, \\delta \\mid y)=\\sum_{m \\in \\mathcal{M}} \\int_{\\Omega_{m}}(\\delta-h(\\theta))^{2} \\pi(\\theta, m \\mid y) d \\theta$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3227bd24-749b-43a6-9500-1c361b25960d",
   "metadata": {},
   "source": [
    "Differentiation gives \n",
    "\n",
    "$$\\frac{\\partial \\rho}{\\partial \\delta}=\\sum_{m \\in \\mathcal{M}} \\int_{\\Omega_{m}}(2 \\delta-2 h(\\theta)) \\pi(\\theta, m \\mid y) d \\theta$$\n",
    "\n",
    "and this is $0$ when $\\delta(y)=E_{\\theta, m \\mid y}(h(\\theta))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "2. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "3. Geoff K. Nicholls. Bayes Methods.\n",
    "4. Patrick J. Laub. Hawkes Processes.\n",
    "5. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
