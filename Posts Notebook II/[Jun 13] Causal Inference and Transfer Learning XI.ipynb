{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ecbc3a",
   "metadata": {},
   "source": [
    "## [Jun 13] Causal Inference and Transfer Learning XI\n",
    "\n",
    "Presenter: Yuchen Ge  \n",
    "Affiliation: University of Oxford  \n",
    "Contact Email: gycdwwd@gmail.com  \n",
    "Website: https://yuchenge-am.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd53c1a-7b00-4471-90e9-65cd0ae3c359",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f8e0b-fb71-4da4-bd7d-511c1ea437e1",
   "metadata": {},
   "source": [
    "We follow the framework in $\\textit{Toward Better Practice of Covariate Adjustment\n",
    "in Analyzing Randomized Clinical Trials}$. First **observe** i.i.d. data for each patient $i\\in [n]$\n",
    "\n",
    "> $\\big($**potential responses**, **covariates for randomization** , **covariates**$\\big)$ = $\\left(Y_{i}^{(1)}, \\ldots, Y_{i}^{(k)}, Z_{i}, X_{i}\\right)$ with corresponding $k$-dim **treatment indicator** $A_{i}$, $i=1, \\ldots, n$ \n",
    "\n",
    "satisfying the following conditions\n",
    "\n",
    "1. (**consistency**) $Y_{i}=Y_{i}^{(t)}$ if and only if $A_{i}=a_{t}$;\n",
    "\n",
    "2. (**regular**) $\\Sigma_{X}=\\operatorname{var}\\left(X_{i}\\right)$ is positive definite;\n",
    "\n",
    "3. (**randomization**) $ \\left\\{A_{i} \\right\\} \\perp  \\left\\{\\left(Y_{i}^{(1)}, \\ldots, Y_{i}^{(k)}, X_{i}\\right)\\right\\} \\mid \\left\\{Z_{i}\\right\\}$; $n_{t}(z) / n(z) \\rightarrow \\pi_{t}$  almost surely.\n",
    "\n",
    "Here $n(z)$ = # patients with  $Z=z$  and  $n_{t}(z)$ = # patients with  $Z=z$  and treatment $ t $. Then we set  pre-specified treatment assignment proportions as $P\\left(A_{i}=a_{t} \\mid Z_{1}, \\ldots, Z_{n}\\right)=\\pi_{t}$ s.t. $\\sum_{t=1}^{k} \\pi_{t}=1$ and $\\pi_t \\in (0,1)$, $t=1, \\ldots, k$. We are interested in "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c5e81-e2da-48a0-9e77-105d3449f57a",
   "metadata": {},
   "source": [
    "> given functions $\\theta_{t}-\\theta_{s}$, $\\theta_{t} / \\theta_{s}$, or $\\left\\{\\theta_{t} /\\left(1-\\theta_{t}\\right)\\right\\} /\\left\\{\\theta_{s} /\\left(1-\\theta_{s}\\right)\\right\\}$ where $\\theta_{t}=E\\left(Y^{(t)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb6944-a657-4534-9a6a-c562623081de",
   "metadata": {},
   "source": [
    "Let  $\\theta=\\left(E\\left(Y^{(1)}, \\ldots, E\\left(Y^{(k)}\\right)\\right)^{\\top}\\right.$  and  $\\beta=\\sum_{t=1}^{k} \\pi_{t} \\beta_{t}$ where $\\beta_{t}=\\Sigma_{X}^{-1} \\operatorname{cov}\\left(X_{i}, Y_{i}^{(t)}\\right), t=1, \\ldots, k$. Then (from lemma 2)\n",
    "\n",
    "$$(\\theta, \\beta)=\\arg \\min _{(\\vartheta, \\not \\beta)} E\\left[\\left\\{Y_{i}-\\vartheta^{\\top} A_{i}-\\not \\beta^{\\top}\\left(X_{i}-\\mu_{X}\\right)\\right\\}^{2}\\right] \\quad \\& \\quad \\left(\\theta, \\beta_{1}, \\ldots, \\beta_{k}\\right)=\\arg \\min _{\\left(\\vartheta, \\not \\beta_{1}, \\ldots, \\not \\beta_{k}\\right)} E\\left[\\left\\{Y_{i}-\\vartheta^{\\top} A_{i}-\\sum_{t=1}^{k} \\not \\beta_{t}^{\\top}\\left(X_{i}-\\mu_{X}\\right) I\\left(A_{i}=a_{t}\\right)\\right\\}^{2}\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f3e43-f00e-4f35-9e4a-9b413569e4a5",
   "metadata": {},
   "source": [
    "Both can be proved via checking the optimality points $(\\theta, \\beta)$. This justifies that we use the following working models\n",
    "\n",
    "> 1. (**homogeneous**) $E\\left(Y_{i} \\mid A_{i}, X_{i}\\right)=\\vartheta^{T} A_{i}+\\not \\beta^{T}\\left(X_{i}-\\mu_{X}\\right)$;\n",
    ">\n",
    "> 2. (**heterogeneous**) $E\\left(Y_{i} \\mid A_{i}, X_{i}\\right)=\\vartheta^{T} A_{i}+\\sum_{t=1}^{k} \\not \\beta_{t}^{T}\\left(X_{i}-\\mu_{X}\\right) I\\left(A_{i}=a_{t}\\right)$.\n",
    "\n",
    "The paper then investigates **asypototic behavior** of a general class of estimators\n",
    "of the form\n",
    "\n",
    "$$  \\hat{\\theta}\\left(\\hat{b}_{1}, \\ldots, \\hat{b}_{k}\\right)=\\left(\\bar{Y}_{1}-\\hat{b}_{1}^{T}\\left(\\bar{X}_{1}-\\bar{X}\\right) \\ldots, \\bar{Y}_{k}-\\hat{b}_{k}^{T}\\left(\\bar{X}_{k}-\\bar{X}\\right)\\right)^{T} $$\n",
    "\n",
    "where (fixed or dependent) $\\hat{b}_{t}$'s have the same dimension as  $X$  and $\\bar{X}_{t} = $ sample mean of  $X_{i}$'s under treatment $t$. This is a **generalization of least\n",
    "squares estimators**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Consider **another** trial \n",
    "\n",
    "> $\\big($**potential responses**, **covariates for randomization** , **covariates**$\\big)$ = $\\left(Y_{i}^{(1)}, \\ldots, Y_{i}^{(k)}, \\tilde{Z}_{i}, \\tilde{X}_{i}\\right)$ with corresponding $k$-dim **treatment indicator** $\\tilde{A}_{i}$, $i=1, \\ldots, \\tilde{n}$.\n",
    "\n",
    "Fix some $C_1, C_2, C_3, C_4>0$, then consider \n",
    "\n",
    "$$\\arg \\min_{(\\vartheta, \\not \\beta, \\tilde{\\vartheta}, \\tilde{\\not \\beta})} E\\left[\\left\\{Y_{i}- C_1 \\vartheta^{\\top} A_{i} - C_2 \\not \\beta^{\\top}\\left(X_{i}-\\mu_{X}\\right) - C_3 \\tilde{\\vartheta}^{\\top} \\tilde{A}_{i} - C_4 \\tilde{\\not \\beta}^{\\top}\\left(\\tilde{X}_{i}-\\mu_{\\tilde{X}}\\right) \\right\\}^{2}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e77f4a-cd61-4d2c-9f07-bcbaf2655cdc",
   "metadata": {},
   "source": [
    "Via computation of  $ E\\left[I\\left(A_{i} =a_{t}, \\tilde{A}_{i} =a_{t'}\\right)\\left\\{ ... \\right\\}\\right]=0$ and $E\\left[\\left(X_{i}-\\mu_{X}\\right)\\left\\{ ... \\right\\}\\right]=0 $ and their counterparts, we immediately see \n",
    "\n",
    "> 1. $C_1 \\theta_t + C_3 \\tilde{\\theta}_{t'} = E\\left(Y_{i}^{(t,t')}\\right)$.\n",
    ">\n",
    "> 2. If $X_i \\perp \\tilde{X}_i$, then $C_2 \\beta = \\sum_{t=1}^{k} \\pi_{t} \\beta_{t}$ and $C_4 \\tilde{\\beta} = \\sum_{t=1}^{k} \\pi_{t} \\tilde{\\beta}_{t}$ where $ \\tilde{\\beta}_{t} = \\Sigma_{\\tilde{X}}^{-1} \\sum_{t=1}^{k} \\operatorname{cov}\\left(\\tilde{X}_{i}, Y_{i}^{(t)}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b9621-b2be-47f3-ae78-35886f45a1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c12ee-1f00-450c-93de-7b4ca97b0f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdba6b-ceb6-4a1d-b823-42a92815abea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d60f3a-7993-4348-b68b-723d1359ba0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134175b2-b3c5-4b08-975c-895cf51a3280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01546a-8d1a-49da-8824-0720d90ed9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463dd6e-770a-439e-82d0-50dde6dbe4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732800b9-4746-4464-a93b-1367e0d10d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4433c2-cbd0-4571-99ea-bc3f6b29c8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b349cb-2941-4ea0-a171-e96981221b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f28c99-02ff-45bf-ac85-baba30c40ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reference\n",
    "\n",
    "1. Shuxiao Chen. Minimax Rates and Adaptivity in Combining Experimental and Observational Data.\n",
    "2. Qingyuan Zhao. Lecture Notes on Causal Inference. \n",
    "3. Joaquin Quiñonero-Candela. Dataset Shift In Machine Learning.\n",
    "4. Geoff K. Nicholls. Bayes Methods.\n",
    "5. Patrick J. Laub. Hawkes Processes.\n",
    "6. Tomas Björk. An Introduction to Point Processes from a Martingale Point of View.\n",
    "7. Jonas Peters. Elements of Causal Inference.\n",
    "8. Alessio Zanga. A Survey on Causal Discovery.\n",
    "9. Qing Zhou. Directed Mixed Graphs for Latent Variables.\n",
    "10. Peter Spirtes. Causation, Prediction, and Search. \n",
    "11. Thomas Richardson. Ancestral Graph Markov Models.\n",
    "12. Jonas Peters. Causal Inference Using Invariant Prediction.\n",
    "13. Jonathan Baxter. A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
